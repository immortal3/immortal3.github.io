<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Speeding up the GPT - KV cache | Becoming The Unbeatable against AGI</title><meta name=keywords content="transformer,nlp,gpt,speedup"><meta name=description content="The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it."><meta name=author content="Me"><link rel=canonical href=https://immortal3.github.io/posts/gpt-kvcache/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://immortal3.github.io/small_icon.jpg><link rel=icon type=image/png sizes=16x16 href=https://immortal3.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://immortal3.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://immortal3.github.io/apple-touch-icon.png><link rel=mask-icon href=https://immortal3.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-PCR7JSXYVY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PCR7JSXYVY",{anonymize_ip:!1})}</script><meta property="og:title" content="Speeding up the GPT - KV cache"><meta property="og:description" content="The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it."><meta property="og:type" content="article"><meta property="og:url" content="https://immortal3.github.io/posts/gpt-kvcache/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-12T12:02:55+05:30"><meta property="article:modified_time" content="2023-02-12T12:02:55+05:30"><meta property="og:site_name" content="Becoming The Unbeatable against AGI"><meta name=twitter:card content="summary"><meta name=twitter:title content="Speeding up the GPT - KV cache"><meta name=twitter:description content="The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://immortal3.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Speeding up the GPT - KV cache","item":"https://immortal3.github.io/posts/gpt-kvcache/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Speeding up the GPT - KV cache","name":"Speeding up the GPT - KV cache","description":"The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it.","keywords":["transformer","nlp","gpt","speedup"],"articleBody":"The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it. Before you read further, the blog assumes you have background on transformers; if you don’t, then read this blog post. It’s awesome, and you will learn a lot from it.\nFirst, let’s understand a few things about GPT code.\ndef gpt(inputs: list[int]) -\u003e list[list[float]]: # inputs has shape [n_seq] # output has shape [n_seq, n_vocab] output = # beep boop neural network magic return output We can deduce from the input-output signature that we can provide arbitrary long input and receive output of the same length, with each element of the output indicating the probability of the next token. So, I can just give a single token as input and get the probability of next token. It should just work, right ?\nModifying the code of picoGPT to just give the input of the last single token and get the probability of the next token.\nfor _ in tqdm(range(n_tokens_to_generate), \"generating\"): # auto-regressive decode loop logits = gpt2(inputs[-1:], **params, n_head=n_head) # model forward pass next_id = np.argmax(logits[-1]) # greedy sampling inputs = np.append(inputs, [next_id]) # append prediction to input We are providing inputs[-1:] as input (single token) to the model. So, we are just passing a single token as input. Let’s see what happens.\nthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the I didn’t work. Because the main magic is in the attention, in order to have good prediction of next tokens we need to provide all previous tokens. Although in practice, we do have limited memory and compute which forces us to provide context upto last N tokens. for example, chagpt has context upto 4096. In summary, We can’t just pass a single token and get very good prediction of next token. This makes attention have quadratic complexity.\nBut, if we look at the architecture of GPT, we can see that we only interact with previous tokens in the attention block, all other layers, such as the embedding layer, the feed forward layer, the layer norm, etc., don’t care about previous tokens. So, what if we can cache the input of the attention block for all previous tokens and pass it during inference? We don’t have to pass all these tokens again and again. We can just pass the last token and get the probability of the next token.\nThe input of the attention block is q, k, v and mask. We can try to cache q, k, and v for all previous tokens. But, let’s think about what really matters for us. We only need k and v of the previous tokens to perform attention on the current input token because we are only passing one token as input. See the image below for a visual representation of what I mean.\ndef attention(q, k, v, mask): # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -\u003e [n_q, d_v] return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v So, we need to calculate new_k and new_v for current input token. Append it to the existing cache and pass it to attention block for further processing.\ndef mha(x, c_attn, c_proj, n_head, kvcache=None): # [n_seq, n_embd] -\u003e [n_seq, n_embd] # qkv projection # when we pass kvcache, n_seq = 1. so we will compute new_q, new_k and new_v x = linear(x, **c_attn) # [n_seq, n_embd] -\u003e [n_seq, 3*n_embd] # split into qkv qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -\u003e [3, n_seq, n_embd] if kvcache: # qkv new_q, new_k, new_v = qkv # new_q, new_k, new_v = [1, n_embd] old_k, old_v = kvcache k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1 v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1 qkv = [new_q, k, v] There is one more thing we need to take care of is causal mask. When we pass single token we would like it to attend to all previous tokens.\n# causal mask to hide future inputs from being attended to if kvcache: # when we pass kvcache, we are passing single token as input which need to attend to all previous tokens, so we create mask with all 0s causal_mask = np.zeros((1, k.shape[0])) else: # create triangular causal mask causal_mask = (1 - np.tri(x.shape[0])) * -1e10 # [n_seq, n_seq] Combining all the things together, we get the following code.\ndef mha(x, c_attn, c_proj, n_head, kvcache=None): # [n_seq, n_embd] -\u003e [n_seq, n_embd] # qkv projection # n_seq = 1 when we pass kvcache, so we will compute new_q, new_k and new_v x = linear(x, **c_attn) # [n_seq, n_embd] -\u003e [n_seq, 3*n_embd] # split into qkv qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -\u003e [3, n_seq, n_embd] if kvcache: # qkv new_q, new_k, new_v = qkv # new_q, new_k, new_v = [1, n_embd] old_k, old_v = kvcache k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1 v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1 qkv = [new_q, k, v] current_cache = [qkv[1], qkv[2]] # split into heads qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv)) # [3, n_seq, n_embd] -\u003e [n_head, 3, n_seq, n_embd/n_head] # causal mask to hide future inputs from being attended to if kvcache: causal_mask = np.zeros((1, k.shape[0])) else: causal_mask = (1 - np.tri(x.shape[0])) * -1e10 # [n_seq, n_seq] # perform attention over each head out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)] # [n_head, 3, n_seq, n_embd/n_head] -\u003e [n_head, n_seq, n_embd/n_head] # merge heads x = np.hstack(out_heads) # [n_head, n_seq, n_embd/n_head] -\u003e [n_seq, n_embd] # out projection x = linear(x, **c_proj) # [n_seq, n_embd] -\u003e [n_seq, n_embd] return x, current_cache We introduced minor breaking changes in output as well. We are introducing current_cache alongside our normal output. This is because we can use an updated cache for the next run.\nWe also need to change a few functions to make it work.\ndef transformer_block(x, mlp, attn, ln_1, ln_2, n_head, kvcache=None): # [n_seq, n_embd] -\u003e [n_seq, n_embd] # multi-head causal self attention attn_out, kvcache_updated = mha(layer_norm(x, **ln_1), **attn, n_head=n_head, kvcache=kvcache) x = x + attn_out # [n_seq, n_embd] -\u003e [n_seq, n_embd] # position-wise feed forward network x = x + ffn(layer_norm(x, **ln_2), **mlp) # [n_seq, n_embd] -\u003e [n_seq, n_embd] return x, kvcache_updated We added kvcache as an input to the function and returned kvcache_updated as an output for each transformer block. We also need to change transformer function.\ndef gpt2(inputs, wte, wpe, blocks, ln_f, n_head, kvcache = None): # [n_seq] -\u003e [n_seq, n_vocab] if not kvcache: kvcache = [None]*len(blocks) wpe_out = wpe[range(len(inputs))] else: # cache already available, only send last token as input for predicting next token wpe_out = wpe[[len(inputs)-1]] inputs = [inputs[-1]] # token + positional embeddings x = wte[inputs] + wpe_out # [n_seq] -\u003e [n_seq, n_embd] # forward pass through n_layer transformer blocks new_kvcache = [] for block, kvcache_block in zip(blocks, kvcache): x, updated_cache = transformer_block(x, **block, n_head=n_head, kvcache=kvcache_block) # [n_seq, n_embd] -\u003e [n_seq, n_embd] new_kvcache.append(updated_cache) # TODO: inplace extend new cache instead of re-saving whole # projection to vocab x = layer_norm(x, **ln_f) # [n_seq, n_embd] -\u003e [n_seq, n_embd] return x @ wte.T, new_kvcache # [n_seq, n_embd] -\u003e [n_seq, n_vocab] Notice, When we have already compute kvcache, we only return input last token to GPT2 alongside with kvcache. You can also see len(kvcache) == # number of transformer blocks. This is because we need to update kvcache for attention and we have single attention in each transformer block.\nAnd, finally, it’s time to change our generate function to use cache. In the first iteration, we will not have kvcache and we will pass kvcache=None to gpt2 function. In subsequent iterations, we will utilise the previously generated kvcache.\nkvcache = None for _ in tqdm(range(n_tokens_to_generate), \"generating\"): # auto-regressive decode loop logits, kvcache = gpt2(inputs, **params, n_head=n_head, kvcache=kvcache) # model forward pass next_id = np.argmax(logits[-1]) # greedy sampling inputs = np.append(inputs, [next_id]) # append prediction to input This cache helps us to reduce computation for each iteration. We can see that, in first iteration, we are computing attention for all tokens in input. But, in subsequent iterations, we are only computing attention for last token. Reducing time complexity from O(n^2) to O(n).\nFinally, we can verify generate text with our previous code which didn’t have caching and compare two output. Both output should be same.\nIn terminal\n\u003e\u003e\u003e python gpt2_kvcache.py \"Alan Turing theorized that computers would one day become\" Output: the most powerful machines on the planet. The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain. You can see the all the code in this pull request. You can also see the code in this repository.\nYou can see more details of calculation related to kv cache memory footprint calculation and computation time in this blog post.\nReferences:\nhttps://kipp.ly/blog/transformer-inference-arithmetic/ https://lilianweng.github.io/posts/2023-01-10-inference-optimization/ https://jaykmody.com/blog/gpt-from-scratch/ ","wordCount":"1580","inLanguage":"en","datePublished":"2023-02-12T12:02:55+05:30","dateModified":"2023-02-12T12:02:55+05:30","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://immortal3.github.io/posts/gpt-kvcache/"},"publisher":{"@type":"Organization","name":"Becoming The Unbeatable against AGI","logo":{"@type":"ImageObject","url":"https://immortal3.github.io/small_icon.jpg"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://immortal3.github.io accesskey=h title="Home (Alt + H)"><img src=https://immortal3.github.io/small_icon.jpg alt aria-label=logo height=25>Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://immortal3.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://immortal3.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://immortal3.github.io/ title=home><span>home</span></a></li><li><a href=https://immortal3.github.io/notes/ title=notes><span>notes</span></a></li><li><a href=https://immortal3.github.io/posts/ title=posts><span>posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://immortal3.github.io>Home</a>&nbsp;»&nbsp;<a href=https://immortal3.github.io/posts/>Posts</a></div><h1 class=post-title>Speeding up the GPT - KV cache</h1><div class=post-meta><span title='2023-02-12 12:02:55 +0530 IST'>February 12, 2023</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Me</div></header><div class=post-content><p>The common optimization trick for speeding up transformer inference is KV caching <a href=https://kipp.ly/blog/transformer-inference-arithmetic/>1</a> <a href=https://lilianweng.github.io/posts/2023-01-10-inference-optimization/>2</a>. This technique is so prominent that huggingface library has <code>use_cache</code> flag is enabled by default <a href="https://huggingface.co/transformers/v3.0.2/model_doc/gpt2.html?highlight=use_cache#transformers.GPT2Model.forward">6</a>. A few days ago, I read an awesome blog post on <a href=https://jaykmody.com/blog/gpt-from-scratch/>GPT in 60 Lines of NumPy</a>. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it. Before you read further, the blog assumes you have background on transformers; if you don&rsquo;t, then read <a href=https://jaykmody.com/blog/gpt-from-scratch/>this blog post</a>. It’s awesome, and you will learn a lot from it.</p><p>First, let’s understand a few things about GPT code.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>gpt</span>(inputs: list[int]) <span style=color:#f92672>-&gt;</span> list[list[float]]:
</span></span><span style=display:flex><span>    <span style=color:#75715e># inputs has shape [n_seq]</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># output has shape [n_seq, n_vocab]</span>
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> <span style=color:#75715e># beep boop neural network magic</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> output
</span></span></code></pre></div><p>We can deduce from the input-output signature that we can provide arbitrary long input and receive output of the same length, with each element of the output indicating the probability of the next token. So, I can just give a single token as input and get the probability of next token. It should just work, right ?</p><p>Modifying the code of picoGPT to just give the input of the last single token and get the probability of the next token.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> tqdm(range(n_tokens_to_generate), <span style=color:#e6db74>&#34;generating&#34;</span>):  <span style=color:#75715e># auto-regressive decode loop</span>
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> gpt2(inputs[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>:], <span style=color:#f92672>**</span>params, n_head<span style=color:#f92672>=</span>n_head)  <span style=color:#75715e># model forward pass</span>
</span></span><span style=display:flex><span>        next_id <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(logits[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])  <span style=color:#75715e># greedy sampling</span>
</span></span><span style=display:flex><span>        inputs <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>append(inputs, [next_id])  <span style=color:#75715e># append prediction to input</span>
</span></span></code></pre></div><p>We are providing <code>inputs[-1:]</code> as input (single token) to the model. So, we are just passing a single token as input. Let&rsquo;s see what happens.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
</span></span></code></pre></div><p>I didn’t work. Because the main magic is in the attention, in order to have good prediction of next tokens we need to provide all previous tokens. Although in practice, we do have limited memory and compute which forces us to provide context upto last N tokens. for example, chagpt has context upto 4096. In summary, We can’t just pass a single token and get very good prediction of next token. This makes attention have quadratic complexity.</p><p>But, if we look at the architecture of GPT, we can see that we only interact with previous tokens in the attention block, all other layers, such as the embedding layer, the feed forward layer, the layer norm, etc., don’t care about previous tokens. So, what if we can cache the input of the attention block for all previous tokens and pass it during inference? We don’t have to pass all these tokens again and again. We can just pass the last token and get the probability of the next token.</p><p>The input of the attention block is q, k, v and mask. We can try to cache q, k, and v for all previous tokens. But, let’s think about what really matters for us. We only need k and v of the previous tokens to perform attention on the current input token because we are only passing one token as input. See the image below for a visual representation of what I mean.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>attention</span>(q, k, v, mask):  <span style=color:#75715e># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> softmax(q <span style=color:#f92672>@</span> k<span style=color:#f92672>.</span>T <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sqrt(q<span style=color:#f92672>.</span>shape[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]) <span style=color:#f92672>+</span> mask) <span style=color:#f92672>@</span> v
</span></span></code></pre></div><p><img loading=lazy src=/blog_photos/kvcache.jpg alt="attention with kvcache"></p><p>So, we need to calculate new_k and new_v for current input token. Append it to the existing cache and pass it to attention block for further processing.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mha</span>(x, c_attn, c_proj, n_head, kvcache<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># qkv projection</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># when we pass kvcache, n_seq = 1. so we will compute new_q, new_k and new_v</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> linear(x, <span style=color:#f92672>**</span>c_attn)  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># split into qkv</span>
</span></span><span style=display:flex><span>    qkv <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>split(x, <span style=color:#ae81ff>3</span>, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)  <span style=color:#75715e># [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> kvcache:
</span></span><span style=display:flex><span>        <span style=color:#75715e># qkv</span>
</span></span><span style=display:flex><span>        new_q, new_k, new_v <span style=color:#f92672>=</span> qkv  <span style=color:#75715e># new_q, new_k, new_v = [1, n_embd]</span>
</span></span><span style=display:flex><span>        old_k, old_v <span style=color:#f92672>=</span> kvcache
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>vstack([old_k, new_k]) <span style=color:#75715e># k = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span>
</span></span><span style=display:flex><span>        v <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>vstack([old_v, new_v]) <span style=color:#75715e># v = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span>
</span></span><span style=display:flex><span>        qkv <span style=color:#f92672>=</span> [new_q, k, v]
</span></span></code></pre></div><p>There is one more thing we need to take care of is causal mask. When we pass single token we would like it to attend to all previous tokens.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#75715e># causal mask to hide future inputs from being attended to</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> kvcache: 
</span></span><span style=display:flex><span>        <span style=color:#75715e># when we pass kvcache, we are passing single token as input which need to attend to all previous tokens, so we create mask with all 0s</span>
</span></span><span style=display:flex><span>        causal_mask <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, k<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># create triangular causal mask</span>
</span></span><span style=display:flex><span>        causal_mask <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>tri(x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>])) <span style=color:#f92672>*</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1e10</span>  <span style=color:#75715e># [n_seq, n_seq]</span>
</span></span></code></pre></div><p>Combining all the things together, we get the following code.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mha</span>(x, c_attn, c_proj, n_head, kvcache<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># qkv projection</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># n_seq = 1 when we pass kvcache, so we will compute new_q, new_k and new_v</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> linear(x, <span style=color:#f92672>**</span>c_attn)  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># split into qkv</span>
</span></span><span style=display:flex><span>    qkv <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>split(x, <span style=color:#ae81ff>3</span>, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)  <span style=color:#75715e># [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> kvcache:
</span></span><span style=display:flex><span>        <span style=color:#75715e># qkv</span>
</span></span><span style=display:flex><span>        new_q, new_k, new_v <span style=color:#f92672>=</span> qkv  <span style=color:#75715e># new_q, new_k, new_v = [1, n_embd]</span>
</span></span><span style=display:flex><span>        old_k, old_v <span style=color:#f92672>=</span> kvcache
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>vstack([old_k, new_k]) <span style=color:#75715e># k = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span>
</span></span><span style=display:flex><span>        v <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>vstack([old_v, new_v]) <span style=color:#75715e># v = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span>
</span></span><span style=display:flex><span>        qkv <span style=color:#f92672>=</span> [new_q, k, v]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    current_cache <span style=color:#f92672>=</span> [qkv[<span style=color:#ae81ff>1</span>], qkv[<span style=color:#ae81ff>2</span>]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># split into heads</span>
</span></span><span style=display:flex><span>    qkv_heads <span style=color:#f92672>=</span> list(map(<span style=color:#66d9ef>lambda</span> x: np<span style=color:#f92672>.</span>split(x, n_head, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>), qkv))  <span style=color:#75715e># [3, n_seq, n_embd] -&gt; [n_head, 3, n_seq, n_embd/n_head]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># causal mask to hide future inputs from being attended to</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> kvcache:
</span></span><span style=display:flex><span>        causal_mask <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, k<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        causal_mask <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>tri(x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>])) <span style=color:#f92672>*</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1e10</span>  <span style=color:#75715e># [n_seq, n_seq]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># perform attention over each head</span>
</span></span><span style=display:flex><span>    out_heads <span style=color:#f92672>=</span> [attention(q, k, v, causal_mask) <span style=color:#66d9ef>for</span> q, k, v <span style=color:#f92672>in</span> zip(<span style=color:#f92672>*</span>qkv_heads)]  <span style=color:#75715e># [n_head, 3, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># merge heads</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>hstack(out_heads)  <span style=color:#75715e># [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># out projection</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> linear(x, <span style=color:#f92672>**</span>c_proj)  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x, current_cache
</span></span></code></pre></div><p>We introduced minor breaking changes in output as well. We are introducing <code>current_cache</code> alongside our normal output. This is because we can use an updated cache for the next run.</p><p>We also need to change a few functions to make it work.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transformer_block</span>(x, mlp, attn, ln_1, ln_2, n_head, kvcache<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># multi-head causal self attention</span>
</span></span><span style=display:flex><span>    attn_out, kvcache_updated <span style=color:#f92672>=</span> mha(layer_norm(x, <span style=color:#f92672>**</span>ln_1), <span style=color:#f92672>**</span>attn, n_head<span style=color:#f92672>=</span>n_head, kvcache<span style=color:#f92672>=</span>kvcache)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> attn_out  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># position-wise feed forward network</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> ffn(layer_norm(x, <span style=color:#f92672>**</span>ln_2), <span style=color:#f92672>**</span>mlp)  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x, kvcache_updated
</span></span></code></pre></div><p>We added <code>kvcache</code> as an input to the function and returned <code>kvcache_updated</code> as an output for each transformer block. We also need to change <code>transformer</code> function.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>gpt2</span>(inputs, wte, wpe, blocks, ln_f, n_head, kvcache <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>):  <span style=color:#75715e># [n_seq] -&gt; [n_seq, n_vocab]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> kvcache:
</span></span><span style=display:flex><span>        kvcache <span style=color:#f92672>=</span> [<span style=color:#66d9ef>None</span>]<span style=color:#f92672>*</span>len(blocks)
</span></span><span style=display:flex><span>        wpe_out <span style=color:#f92672>=</span> wpe[range(len(inputs))]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>: <span style=color:#75715e># cache already available, only send last token as input for predicting next token</span>
</span></span><span style=display:flex><span>        wpe_out <span style=color:#f92672>=</span> wpe[[len(inputs)<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]]
</span></span><span style=display:flex><span>        inputs <span style=color:#f92672>=</span> [inputs[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># token + positional embeddings</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> wte[inputs] <span style=color:#f92672>+</span> wpe_out  <span style=color:#75715e># [n_seq] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># forward pass through n_layer transformer blocks</span>
</span></span><span style=display:flex><span>    new_kvcache <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> block, kvcache_block <span style=color:#f92672>in</span> zip(blocks, kvcache):
</span></span><span style=display:flex><span>        x, updated_cache <span style=color:#f92672>=</span> transformer_block(x, <span style=color:#f92672>**</span>block, n_head<span style=color:#f92672>=</span>n_head, kvcache<span style=color:#f92672>=</span>kvcache_block)  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>        new_kvcache<span style=color:#f92672>.</span>append(updated_cache)  <span style=color:#75715e># TODO: inplace extend new cache instead of re-saving whole</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># projection to vocab</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> layer_norm(x, <span style=color:#f92672>**</span>ln_f)  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x <span style=color:#f92672>@</span> wte<span style=color:#f92672>.</span>T, new_kvcache  <span style=color:#75715e># [n_seq, n_embd] -&gt; [n_seq, n_vocab]</span>
</span></span></code></pre></div><p>Notice, When we have already compute <code>kvcache</code>, we only return input last token to GPT2 alongside with <code>kvcache</code>. You can also see <code>len(kvcache) == # number of transformer blocks</code>. This is because we need to update <code>kvcache</code> for attention and we have single attention in each transformer block.</p><p>And, finally, it&rsquo;s time to change our <code>generate</code> function to use cache. In the first iteration, we will not have <code>kvcache</code> and we will pass <code>kvcache=None</code> to <code>gpt2</code> function. In subsequent iterations, we will utilise the previously generated <code>kvcache</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>kvcache <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> tqdm(range(n_tokens_to_generate), <span style=color:#e6db74>&#34;generating&#34;</span>):  <span style=color:#75715e># auto-regressive decode loop</span>
</span></span><span style=display:flex><span>    logits, kvcache <span style=color:#f92672>=</span> gpt2(inputs, <span style=color:#f92672>**</span>params, n_head<span style=color:#f92672>=</span>n_head, kvcache<span style=color:#f92672>=</span>kvcache)  <span style=color:#75715e># model forward pass</span>
</span></span><span style=display:flex><span>    next_id <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(logits[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])  <span style=color:#75715e># greedy sampling</span>
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>append(inputs, [next_id])  <span style=color:#75715e># append prediction to input</span>
</span></span></code></pre></div><p>This cache helps us to reduce computation for each iteration. We can see that, in first iteration, we are computing attention for all tokens in input. But, in subsequent iterations, we are only computing attention for last token. Reducing time complexity from <code>O(n^2)</code> to <code>O(n)</code>.</p><p>Finally, we can verify generate text with our previous code which didn&rsquo;t have caching and compare two output. Both output should be same.</p><p>In terminal</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> python gpt2_kvcache<span style=color:#f92672>.</span>py <span style=color:#e6db74>&#34;Alan Turing theorized that computers would one day become&#34;</span>
</span></span><span style=display:flex><span>Output:
</span></span><span style=display:flex><span> the most powerful machines on the planet<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The computer <span style=color:#f92672>is</span> a machine that can perform complex calculations, <span style=color:#f92672>and</span> it can perform these calculations <span style=color:#f92672>in</span> a way that <span style=color:#f92672>is</span> very similar to the human brain<span style=color:#f92672>.</span>
</span></span></code></pre></div><p>You can see the all the code in this <a href=https://github.com/jaymody/picoGPT/pull/7/files>pull request</a>. You can also see the code in this <a href=https://github.com/immortal3/picoGPT>repository</a>.</p><p>You can see more details of calculation related to kv cache memory footprint calculation and computation time in this <a href=https://kipp.ly/blog/transformer-inference-arithmetic/>blog post</a>.</p><p>References:</p><ol><li><a href=https://kipp.ly/blog/transformer-inference-arithmetic/>https://kipp.ly/blog/transformer-inference-arithmetic/</a></li><li><a href=https://lilianweng.github.io/posts/2023-01-10-inference-optimization/>https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</a></li><li><a href=https://jaykmody.com/blog/gpt-from-scratch/>https://jaykmody.com/blog/gpt-from-scratch/</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://immortal3.github.io/tags/transformer/>transformer</a></li><li><a href=https://immortal3.github.io/tags/nlp/>nlp</a></li><li><a href=https://immortal3.github.io/tags/gpt/>gpt</a></li><li><a href=https://immortal3.github.io/tags/speedup/>speedup</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://immortal3.github.io>Becoming The Unbeatable against AGI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>