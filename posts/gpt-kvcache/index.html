<!DOCTYPE html>
<html lang="en">

<head>
    
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-SHT0HV5ELQ"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());

            gtag('config', 'G-SHT0HV5ELQ');
        </script>
        


            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>
                Speeding up the GPT - KV cache | Dip&#39;s Blog
            </title>
            <meta name="description"
                content="Thoughts on Machine Learning, AI, and Software Engineering by Dipkumar Patel">
            <meta property="og:title" content="Speeding up the GPT - KV cache" />
            <meta property="og:description"
                content="Thoughts on Machine Learning, AI, and Software Engineering by Dipkumar Patel" />
            <meta property="og:type" content="article" />
            <meta property="og:image" content="https://dipkumar.dev/static/social-share.jpg" />
            <meta name="twitter:card" content="summary_large_image" />
            <meta name="twitter:image" content="https://dipkumar.dev/static/social-share.jpg" />

            <link rel="icon" type="image/jpeg" href="/static/icon.jpeg">
            <link rel="stylesheet" href="/static/style.css">
            <link rel="stylesheet" href="/static/fish.css">

            <!-- Prism.js for Syntax Highlighting -->
            <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css"
                rel="stylesheet" />

            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=STIX+Two+Text:wght@400;500;600;700&display=swap"
                rel="stylesheet">
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"
                integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA=="
                crossorigin="anonymous" referrerpolicy="no-referrer" />
</head>

<body>
    
    <div class="reading-progress" id="reading-progress"></div>
    
    <div class="container">
        
                            <nav class="post-nav">
                                <a href="/" class="back-link">Home</a>
                                <a href="/blogs" class="all-posts-link">All Posts</a>
                            </nav>
                            <article class="post-content">
                                <header>
                                    <h1>
                                        Speeding up the GPT - KV cache
                                    </h1>
                                    <div class="meta-row">
                                        <div class="author-meta">
                                            <span class="social-mini">
                                                <a href="https://x.com/immortaldip" target="_blank">ùïè</a>
                                                <a href="https://github.com/immortal3" target="_blank">github</a>
                                                <a href="mailto:patel@dipkumar.dev">email</a>
                                            </span>
                                            By <a href="/" class="author-link">Dipkumar Patel</a>
                                        </div>
                                        <div class="date-meta">
                                            February 12, 2023
                                            
                                            <span class="reading-time">9 min read</span>
                                            
                                        </div>
                                    </div>
                                </header>

                                

                                        <div class="markdown-body">
                                            <p>The common optimization trick for speeding up transformer inference is KV caching  <a href="https://kipp.ly/blog/transformer-inference-arithmetic/">1</a> <a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">2</a>. This technique is so prominent that huggingface library has <code>use_cache</code> flag is enabled by default <a href="https://huggingface.co/transformers/v3.0.2/model_doc/gpt2.html?highlight=use_cache#transformers.GPT2Model.forward">6</a>. A few days ago, I read an awesome blog post on <a href="https://jaykmody.com/blog/gpt-from-scratch/">GPT in 60 Lines of NumPy</a>. So, i thought, why not extend it to use the KV cache technique? So, let‚Äôs roll up our sleeves and start working on it. Before you read further, the blog assumes you have background on transformers; if you don&#39;t, then read <a href="https://jaykmody.com/blog/gpt-from-scratch/">this blog post</a>. It‚Äôs awesome, and you will learn a lot from it.</p>
<p>First, let‚Äôs understand a few things about GPT code.</p>
<pre><code class="language-python">def gpt(inputs: list[int]) -&gt; list[list[float]]:
    # inputs has shape [n_seq]
    # output has shape [n_seq, n_vocab]
    output = # beep boop neural network magic
    return output
</code></pre>
<p>We can deduce from the input-output signature that we can provide arbitrary long input and receive output of the same length, with each element of the output indicating the probability of the next token. So, I can just give a single token as input and get the probability of next token. It should just work, right ?</p>
<p>Modifying the code of picoGPT to just give the input of the last single token and get the probability of the next token.</p>
<pre><code class="language-python">for _ in tqdm(range(n_tokens_to_generate), &quot;generating&quot;):  # auto-regressive decode loop
        logits = gpt2(inputs[-1:], **params, n_head=n_head)  # model forward pass
        next_id = np.argmax(logits[-1])  # greedy sampling
        inputs = np.append(inputs, [next_id])  # append prediction to input
</code></pre>
<p>We are providing <code>inputs[-1:]</code> as input (single token) to the model. So, we are just passing a single token as input. Let&#39;s see what happens.</p>
<pre><code class="language-markdown"> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
</code></pre>
<p>I didn‚Äôt work. Because the main magic is in the attention, in order to have good prediction of next tokens we need to provide all previous tokens. Although in practice, we do have limited memory and compute which forces us to provide context upto last N tokens. for example, chagpt has context upto 4096. In summary, We can‚Äôt just pass a single token and get very good prediction of next token. This makes attention have quadratic complexity.</p>
<p>But, if we look at the architecture of GPT, we can see that we only interact with previous tokens in the attention block, all other layers, such as the embedding layer, the feed forward layer, the layer norm, etc., don‚Äôt care about previous tokens. So, what if we can cache the input of the attention block for all previous tokens and pass it during inference? We don‚Äôt have to pass all these tokens again and again. We can just pass the last token and get the probability of the next token.</p>
<p>The input of the attention block is q, k, v¬†and¬†mask. We can try to cache q, k, and v for all previous tokens. But, let‚Äôs think about what really matters for us. We only need k and v of the previous tokens to perform attention on the current input token because we are only passing one token as input. See the image below for a visual representation of what I mean. </p>
<pre><code class="language-python">def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]
    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v
</code></pre>
<p><img src="/static/blog_photos/kvcache.jpg" alt="attention with kvcache"></p>
<p>So, we need to calculate new_k and new_v for current input token. Append it to the existing cache and pass it to attention block for further processing. </p>
<pre><code class="language-python">def mha(x, c_attn, c_proj, n_head, kvcache=None):  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # qkv projection
    # when we pass kvcache, n_seq = 1. so we will compute new_q, new_k and new_v
    x = linear(x, **c_attn)  # [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]

    # split into qkv
    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]

    if kvcache:
        # qkv
        new_q, new_k, new_v = qkv  # new_q, new_k, new_v = [1, n_embd]
        old_k, old_v = kvcache
        k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1
        v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1
        qkv = [new_q, k, v]
</code></pre>
<p>There is one more thing we need to take care of is causal mask. When we pass single token we would like it to attend to all previous tokens.</p>
<pre><code class="language-python">    # causal mask to hide future inputs from being attended to
    if kvcache: 
        # when we pass kvcache, we are passing single token as input which need to attend to all previous tokens, so we create mask with all 0s
        causal_mask = np.zeros((1, k.shape[0]))
    else:
        # create triangular causal mask
        causal_mask = (1 - np.tri(x.shape[0])) * -1e10  # [n_seq, n_seq]
</code></pre>
<p>Combining all the things together, we get the following code.</p>
<pre><code class="language-python">def mha(x, c_attn, c_proj, n_head, kvcache=None):  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # qkv projection
    # n_seq = 1 when we pass kvcache, so we will compute new_q, new_k and new_v
    x = linear(x, **c_attn)  # [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]

    # split into qkv
    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]

    if kvcache:
        # qkv
        new_q, new_k, new_v = qkv  # new_q, new_k, new_v = [1, n_embd]
        old_k, old_v = kvcache
        k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1
        v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1
        qkv = [new_q, k, v]

    current_cache = [qkv[1], qkv[2]]

    # split into heads
    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -&gt; [n_head, 3, n_seq, n_embd/n_head]

    # causal mask to hide future inputs from being attended to
    if kvcache:
        causal_mask = np.zeros((1, k.shape[0]))
    else:
        causal_mask = (1 - np.tri(x.shape[0])) * -1e10  # [n_seq, n_seq]

    # perform attention over each head
    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [n_head, 3, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]

    
    # merge heads
    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]

    # out projection
    x = linear(x, **c_proj)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    return x, current_cache
</code></pre>
<p>We introduced minor breaking changes in output as well. We are introducing <code>current_cache</code> alongside our normal output. This is because we can use an updated cache for the next run.</p>
<p>We also need to change a few functions to make it work.</p>
<pre><code class="language-python">def transformer_block(x, mlp, attn, ln_1, ln_2, n_head, kvcache=None):  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # multi-head causal self attention
    attn_out, kvcache_updated = mha(layer_norm(x, **ln_1), **attn, n_head=n_head, kvcache=kvcache)
    x = x + attn_out  # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    # position-wise feed forward network
    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    return x, kvcache_updated
</code></pre>
<p>We added <code>kvcache</code> as an input to the function and returned <code>kvcache_updated</code> as an output for each transformer block. We also need to change <code>transformer</code> function.</p>
<pre><code class="language-python"> def gpt2(inputs, wte, wpe, blocks, ln_f, n_head, kvcache = None):  # [n_seq] -&gt; [n_seq, n_vocab]
    if not kvcache:
        kvcache = [None]*len(blocks)
        wpe_out = wpe[range(len(inputs))]
    else: # cache already available, only send last token as input for predicting next token
        wpe_out = wpe[[len(inputs)-1]]
        inputs = [inputs[-1]]

    # token + positional embeddings
    x = wte[inputs] + wpe_out  # [n_seq] -&gt; [n_seq, n_embd]

    
    # forward pass through n_layer transformer blocks
    new_kvcache = []
    for block, kvcache_block in zip(blocks, kvcache):
        x, updated_cache = transformer_block(x, **block, n_head=n_head, kvcache=kvcache_block)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
        new_kvcache.append(updated_cache)  # TODO: inplace extend new cache instead of re-saving whole

    # projection to vocab
    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    return x @ wte.T, new_kvcache  # [n_seq, n_embd] -&gt; [n_seq, n_vocab]
</code></pre>
<p>Notice, When we have already compute <code>kvcache</code>, we only return input last token to GPT2 alongside with <code>kvcache</code>. You can also see <code>len(kvcache) == # number of transformer blocks</code>. This is because we need to update <code>kvcache</code> for attention and we have single attention in each transformer block.</p>
<p>And, finally, it&#39;s time to change our <code>generate</code> function to use cache. In the first iteration, we will not have <code>kvcache</code> and we will pass <code>kvcache=None</code> to <code>gpt2</code> function. In subsequent iterations, we will utilise the previously generated <code>kvcache</code>.</p>
<pre><code class="language-python">kvcache = None
for _ in tqdm(range(n_tokens_to_generate), &quot;generating&quot;):  # auto-regressive decode loop
    logits, kvcache = gpt2(inputs, **params, n_head=n_head, kvcache=kvcache)  # model forward pass
    next_id = np.argmax(logits[-1])  # greedy sampling
    inputs = np.append(inputs, [next_id])  # append prediction to input
</code></pre>
<p>This cache helps us to reduce computation for each iteration. We can see that, in first iteration, we are computing attention for all tokens in input. But, in subsequent iterations, we are only computing attention for last token. Reducing time complexity from <code>O(n^2)</code> to <code>O(n)</code>.</p>
<p>Finally, we can verify generate text with our previous code which didn&#39;t have caching and compare two output. Both output should be same.</p>
<p>In terminal</p>
<pre><code class="language-python">&gt;&gt;&gt; python gpt2_kvcache.py &quot;Alan Turing theorized that computers would one day become&quot;
Output:
 the most powerful machines on the planet.

The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.
</code></pre>
<p>You can see the all the code in this <a href="https://github.com/jaymody/picoGPT/pull/7/files">pull request</a>. You can also see the code in this <a href="https://github.com/immortal3/picoGPT">repository</a>.</p>
<p>You can see more details of calculation related to kv cache memory footprint calculation and computation time in this <a href="https://kipp.ly/blog/transformer-inference-arithmetic/">blog post</a>.</p>
<p>References:</p>
<ol>
<li><a href="https://kipp.ly/blog/transformer-inference-arithmetic/">https://kipp.ly/blog/transformer-inference-arithmetic/</a></li>
<li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</a></li>
<li><a href="https://jaykmody.com/blog/gpt-from-scratch/">https://jaykmody.com/blog/gpt-from-scratch/</a></li>
</ol>

                                        </div>

                                        
                                            <div class="post-footer-tags">
                                                
                                                    <a href="/tags/transformer">#transformer</a>
                                                    
                                                    <a href="/tags/nlp">#nlp</a>
                                                    
                                                    <a href="/tags/gpt">#gpt</a>
                                                    
                                                    <a href="/tags/speedup">#speedup</a>
                                                    
                                            </div>
                                            
                            </article>
                            
    </div>

    <!-- Prism JS for Syntax Highlighting -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure autoloader path
        Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';

        // Highlight on load
        document.addEventListener('DOMContentLoaded', function () {
            Prism.highlightAll();
        });

        // Mermaid initialization
        if (document.querySelector('.language-mermaid')) {
            const script = document.createElement('script');
            script.src = 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js';
            script.onload = () => {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default',
                    flowchart: { useMaxWidth: true, htmlLabels: true, curve: 'basis' },
                    securityLevel: 'loose',
                });
                // Replace mermaid code blocks with div.mermaid
                document.querySelectorAll('pre code.language-mermaid').forEach(el => {
                    const pre = el.parentElement;
                    const div = document.createElement('div');
                    div.className = 'mermaid';
                    div.textContent = el.textContent;
                    pre.parentElement.replaceChild(div, pre);
                });
                mermaid.init(undefined, '.mermaid');
            };
            document.head.appendChild(script);
        }

        // Reading progress bar
        
        (function() {
            const progressBar = document.getElementById('reading-progress');
            const article = document.querySelector('.markdown-body');
            if (progressBar && article) {
                window.addEventListener('scroll', function() {
                    const articleRect = article.getBoundingClientRect();
                    const articleTop = articleRect.top + window.scrollY;
                    const articleHeight = article.offsetHeight;
                    const windowHeight = window.innerHeight;
                    const scrollTop = window.scrollY;

                    // Calculate progress based on how much of the article has been scrolled past
                    const progress = Math.min(100, Math.max(0,
                        ((scrollTop - articleTop + windowHeight * 0.3) / articleHeight) * 100
                    ));
                    progressBar.style.width = progress + '%';
                });
            }
        })();
        

        
    </script>
</body>

</html>