<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Speeding up the GPT - KV cache | Dip&#39;Blog</title>
<meta name="keywords" content="transformer, nlp, gpt, speedup">
<meta name="description" content="The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it.">
<meta name="author" content="
Author:
Dipkumar Patel">
<link rel="canonical" href="https://dipkumar.dev/posts/gpt-kvcache/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.9a3a9b6cc85cc3f62d3346fcc6d5b9344945ceca05719846bfd9d1f7a39acc84.css" integrity="sha256-mjqbbMhcw/YtM0b8xtW5NElFzsoFcZhGv9nR96OazIQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://dipkumar.dev/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://dipkumar.dev/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://dipkumar.dev/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://dipkumar.dev/apple-touch-icon.png">
<link rel="mask-icon" href="https://dipkumar.dev/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://dipkumar.dev/posts/gpt-kvcache/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="Speeding up the GPT - KV cache" />
<meta property="og:description" content="The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dipkumar.dev/posts/gpt-kvcache/" />
<meta property="og:image" content="https://dipkumar.dev/images/papermod-cover.png" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-12T12:02:55+05:30" />
<meta property="article:modified_time" content="2023-02-12T12:02:55+05:30" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://dipkumar.dev/images/papermod-cover.png" />
<meta name="twitter:title" content="Speeding up the GPT - KV cache"/>
<meta name="twitter:description" content="The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://dipkumar.dev/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Speeding up the GPT - KV cache",
      "item": "https://dipkumar.dev/posts/gpt-kvcache/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Speeding up the GPT - KV cache",
  "name": "Speeding up the GPT - KV cache",
  "description": "The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it.",
  "keywords": [
    "transformer", "nlp", "gpt", "speedup"
  ],
  "articleBody": "The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it. Before you read further, the blog assumes you have background on transformers; if you don’t, then read this blog post. It’s awesome, and you will learn a lot from it.\nFirst, let’s understand a few things about GPT code.\ndef gpt(inputs: list[int]) -\u003e list[list[float]]: # inputs has shape [n_seq] # output has shape [n_seq, n_vocab] output = # beep boop neural network magic return output We can deduce from the input-output signature that we can provide arbitrary long input and receive output of the same length, with each element of the output indicating the probability of the next token. So, I can just give a single token as input and get the probability of next token. It should just work, right ?\nModifying the code of picoGPT to just give the input of the last single token and get the probability of the next token.\nfor _ in tqdm(range(n_tokens_to_generate), \"generating\"): # auto-regressive decode loop logits = gpt2(inputs[-1:], **params, n_head=n_head) # model forward pass next_id = np.argmax(logits[-1]) # greedy sampling inputs = np.append(inputs, [next_id]) # append prediction to input We are providing inputs[-1:] as input (single token) to the model. So, we are just passing a single token as input. Let’s see what happens.\nthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the I didn’t work. Because the main magic is in the attention, in order to have good prediction of next tokens we need to provide all previous tokens. Although in practice, we do have limited memory and compute which forces us to provide context upto last N tokens. for example, chagpt has context upto 4096. In summary, We can’t just pass a single token and get very good prediction of next token. This makes attention have quadratic complexity.\nBut, if we look at the architecture of GPT, we can see that we only interact with previous tokens in the attention block, all other layers, such as the embedding layer, the feed forward layer, the layer norm, etc., don’t care about previous tokens. So, what if we can cache the input of the attention block for all previous tokens and pass it during inference? We don’t have to pass all these tokens again and again. We can just pass the last token and get the probability of the next token.\nThe input of the attention block is q, k, v and mask. We can try to cache q, k, and v for all previous tokens. But, let’s think about what really matters for us. We only need k and v of the previous tokens to perform attention on the current input token because we are only passing one token as input. See the image below for a visual representation of what I mean.\ndef attention(q, k, v, mask): # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -\u003e [n_q, d_v] return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v So, we need to calculate new_k and new_v for current input token. Append it to the existing cache and pass it to attention block for further processing.\ndef mha(x, c_attn, c_proj, n_head, kvcache=None): # [n_seq, n_embd] -\u003e [n_seq, n_embd] # qkv projection # when we pass kvcache, n_seq = 1. so we will compute new_q, new_k and new_v x = linear(x, **c_attn) # [n_seq, n_embd] -\u003e [n_seq, 3*n_embd] # split into qkv qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -\u003e [3, n_seq, n_embd] if kvcache: # qkv new_q, new_k, new_v = qkv # new_q, new_k, new_v = [1, n_embd] old_k, old_v = kvcache k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1 v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1 qkv = [new_q, k, v] There is one more thing we need to take care of is causal mask. When we pass single token we would like it to attend to all previous tokens.\n# causal mask to hide future inputs from being attended to if kvcache: # when we pass kvcache, we are passing single token as input which need to attend to all previous tokens, so we create mask with all 0s causal_mask = np.zeros((1, k.shape[0])) else: # create triangular causal mask causal_mask = (1 - np.tri(x.shape[0])) * -1e10 # [n_seq, n_seq] Combining all the things together, we get the following code.\ndef mha(x, c_attn, c_proj, n_head, kvcache=None): # [n_seq, n_embd] -\u003e [n_seq, n_embd] # qkv projection # n_seq = 1 when we pass kvcache, so we will compute new_q, new_k and new_v x = linear(x, **c_attn) # [n_seq, n_embd] -\u003e [n_seq, 3*n_embd] # split into qkv qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -\u003e [3, n_seq, n_embd] if kvcache: # qkv new_q, new_k, new_v = qkv # new_q, new_k, new_v = [1, n_embd] old_k, old_v = kvcache k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1 v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1 qkv = [new_q, k, v] current_cache = [qkv[1], qkv[2]] # split into heads qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv)) # [3, n_seq, n_embd] -\u003e [n_head, 3, n_seq, n_embd/n_head] # causal mask to hide future inputs from being attended to if kvcache: causal_mask = np.zeros((1, k.shape[0])) else: causal_mask = (1 - np.tri(x.shape[0])) * -1e10 # [n_seq, n_seq] # perform attention over each head out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)] # [n_head, 3, n_seq, n_embd/n_head] -\u003e [n_head, n_seq, n_embd/n_head] # merge heads x = np.hstack(out_heads) # [n_head, n_seq, n_embd/n_head] -\u003e [n_seq, n_embd] # out projection x = linear(x, **c_proj) # [n_seq, n_embd] -\u003e [n_seq, n_embd] return x, current_cache We introduced minor breaking changes in output as well. We are introducing current_cache alongside our normal output. This is because we can use an updated cache for the next run.\nWe also need to change a few functions to make it work.\ndef transformer_block(x, mlp, attn, ln_1, ln_2, n_head, kvcache=None): # [n_seq, n_embd] -\u003e [n_seq, n_embd] # multi-head causal self attention attn_out, kvcache_updated = mha(layer_norm(x, **ln_1), **attn, n_head=n_head, kvcache=kvcache) x = x + attn_out # [n_seq, n_embd] -\u003e [n_seq, n_embd] # position-wise feed forward network x = x + ffn(layer_norm(x, **ln_2), **mlp) # [n_seq, n_embd] -\u003e [n_seq, n_embd] return x, kvcache_updated We added kvcache as an input to the function and returned kvcache_updated as an output for each transformer block. We also need to change transformer function.\ndef gpt2(inputs, wte, wpe, blocks, ln_f, n_head, kvcache = None): # [n_seq] -\u003e [n_seq, n_vocab] if not kvcache: kvcache = [None]*len(blocks) wpe_out = wpe[range(len(inputs))] else: # cache already available, only send last token as input for predicting next token wpe_out = wpe[[len(inputs)-1]] inputs = [inputs[-1]] # token + positional embeddings x = wte[inputs] + wpe_out # [n_seq] -\u003e [n_seq, n_embd] # forward pass through n_layer transformer blocks new_kvcache = [] for block, kvcache_block in zip(blocks, kvcache): x, updated_cache = transformer_block(x, **block, n_head=n_head, kvcache=kvcache_block) # [n_seq, n_embd] -\u003e [n_seq, n_embd] new_kvcache.append(updated_cache) # TODO: inplace extend new cache instead of re-saving whole # projection to vocab x = layer_norm(x, **ln_f) # [n_seq, n_embd] -\u003e [n_seq, n_embd] return x @ wte.T, new_kvcache # [n_seq, n_embd] -\u003e [n_seq, n_vocab] Notice, When we have already compute kvcache, we only return input last token to GPT2 alongside with kvcache. You can also see len(kvcache) == # number of transformer blocks. This is because we need to update kvcache for attention and we have single attention in each transformer block.\nAnd, finally, it’s time to change our generate function to use cache. In the first iteration, we will not have kvcache and we will pass kvcache=None to gpt2 function. In subsequent iterations, we will utilise the previously generated kvcache.\nkvcache = None for _ in tqdm(range(n_tokens_to_generate), \"generating\"): # auto-regressive decode loop logits, kvcache = gpt2(inputs, **params, n_head=n_head, kvcache=kvcache) # model forward pass next_id = np.argmax(logits[-1]) # greedy sampling inputs = np.append(inputs, [next_id]) # append prediction to input This cache helps us to reduce computation for each iteration. We can see that, in first iteration, we are computing attention for all tokens in input. But, in subsequent iterations, we are only computing attention for last token. Reducing time complexity from O(n^2) to O(n).\nFinally, we can verify generate text with our previous code which didn’t have caching and compare two output. Both output should be same.\nIn terminal\n\u003e\u003e\u003e python gpt2_kvcache.py \"Alan Turing theorized that computers would one day become\" Output: the most powerful machines on the planet. The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain. You can see the all the code in this pull request. You can also see the code in this repository.\nYou can see more details of calculation related to kv cache memory footprint calculation and computation time in this blog post.\nReferences:\nhttps://kipp.ly/blog/transformer-inference-arithmetic/ https://lilianweng.github.io/posts/2023-01-10-inference-optimization/ https://jaykmody.com/blog/gpt-from-scratch/ ",
  "wordCount" : "1580",
  "inLanguage": "en",
  "image": "https://dipkumar.dev/images/papermod-cover.png","datePublished": "2023-02-12T12:02:55+05:30",
  "dateModified": "2023-02-12T12:02:55+05:30",
  "author":{
    "@type": "Person",
    "name": "Dipkumar Patel"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dipkumar.dev/posts/gpt-kvcache/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Dip'Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dipkumar.dev/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://dipkumar.dev/" accesskey="h" title="Dip&#39;Blog (Alt + H)">Dip&#39;Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://dipkumar.dev/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Speeding up the GPT - KV cache
    </h1>
    <div class="post-meta"><span title='2023-02-12 12:02:55 +0530 IST'> February 12, 2023</span>&nbsp;|&nbsp;Estimated Reading Time: 8 min&nbsp;|&nbsp;
Author:
Dipkumar Patel&nbsp;|&nbsp;<a href="https://github.com/adityatelange/hugo-PaperMod/tree/exampleSite/content/posts/gpt-kvcache.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
  <div class="post-content"><p>The common optimization trick for speeding up transformer inference is KV caching  <a href="https://kipp.ly/blog/transformer-inference-arithmetic/">1</a> <a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">2</a>. This technique is so prominent that huggingface library has <code>use_cache</code> flag is enabled by default <a href="https://huggingface.co/transformers/v3.0.2/model_doc/gpt2.html?highlight=use_cache#transformers.GPT2Model.forward">6</a>. A few days ago, I read an awesome blog post on <a href="https://jaykmody.com/blog/gpt-from-scratch/">GPT in 60 Lines of NumPy</a>. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it. Before you read further, the blog assumes you have background on transformers; if you don&rsquo;t, then read <a href="https://jaykmody.com/blog/gpt-from-scratch/">this blog post</a>. It’s awesome, and you will learn a lot from it.</p>
<p>First, let’s understand a few things about GPT code.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">gpt</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># inputs has shape [n_seq]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># output has shape [n_seq, n_vocab]</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="c1"># beep boop neural network magic</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></div><p>We can deduce from the input-output signature that we can provide arbitrary long input and receive output of the same length, with each element of the output indicating the probability of the next token. So, I can just give a single token as input and get the probability of next token. It should just work, right ?</p>
<p>Modifying the code of picoGPT to just give the input of the last single token and get the probability of the next token.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_tokens_to_generate</span><span class="p">),</span> <span class="s2">&#34;generating&#34;</span><span class="p">):</span>  <span class="c1"># auto-regressive decode loop</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">gpt2</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span> <span class="o">**</span><span class="n">params</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">)</span>  <span class="c1"># model forward pass</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_id</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># greedy sampling</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="n">next_id</span><span class="p">])</span>  <span class="c1"># append prediction to input</span>
</span></span></code></pre></div><p>We are providing <code>inputs[-1:]</code> as input (single token) to the model. So, we are just passing a single token as input. Let&rsquo;s see what happens.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-markdown" data-lang="markdown"><span class="line"><span class="cl"> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
</span></span></code></pre></div><p>I didn’t work. Because the main magic is in the attention, in order to have good prediction of next tokens we need to provide all previous tokens. Although in practice, we do have limited memory and compute which forces us to provide context upto last N tokens. for example, chagpt has context upto 4096. In summary, We can’t just pass a single token and get very good prediction of next token. This makes attention have quadratic complexity.</p>
<p>But, if we look at the architecture of GPT, we can see that we only interact with previous tokens in the attention block, all other layers, such as the embedding layer, the feed forward layer, the layer norm, etc., don’t care about previous tokens. So, what if we can cache the input of the attention block for all previous tokens and pass it during inference? We don’t have to pass all these tokens again and again. We can just pass the last token and get the probability of the next token.</p>
<p>The input of the attention block is q, k, v and mask. We can try to cache q, k, and v for all previous tokens. But, let’s think about what really matters for us. We only need k and v of the previous tokens to perform attention on the current input token because we are only passing one token as input. See the image below for a visual representation of what I mean.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>  <span class="c1"># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">mask</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>
</span></span></code></pre></div><p><img loading="lazy" src="/blog_photos/kvcache.jpg" alt="attention with kvcache"  />
</p>
<p>So, we need to calculate new_k and new_v for current input token. Append it to the existing cache and pass it to attention block for further processing.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c_attn</span><span class="p">,</span> <span class="n">c_proj</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">kvcache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># qkv projection</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># when we pass kvcache, n_seq = 1. so we will compute new_q, new_k and new_v</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_attn</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># split into qkv</span>
</span></span><span class="line"><span class="cl">    <span class="n">qkv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">kvcache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># qkv</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_q</span><span class="p">,</span> <span class="n">new_k</span><span class="p">,</span> <span class="n">new_v</span> <span class="o">=</span> <span class="n">qkv</span>  <span class="c1"># new_q, new_k, new_v = [1, n_embd]</span>
</span></span><span class="line"><span class="cl">        <span class="n">old_k</span><span class="p">,</span> <span class="n">old_v</span> <span class="o">=</span> <span class="n">kvcache</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">old_k</span><span class="p">,</span> <span class="n">new_k</span><span class="p">])</span> <span class="c1"># k = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">old_v</span><span class="p">,</span> <span class="n">new_v</span><span class="p">])</span> <span class="c1"># v = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span>
</span></span><span class="line"><span class="cl">        <span class="n">qkv</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">]</span>
</span></span></code></pre></div><p>There is one more thing we need to take care of is causal mask. When we pass single token we would like it to attend to all previous tokens.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="c1"># causal mask to hide future inputs from being attended to</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">kvcache</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">        <span class="c1"># when we pass kvcache, we are passing single token as input which need to attend to all previous tokens, so we create mask with all 0s</span>
</span></span><span class="line"><span class="cl">        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># create triangular causal mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">causal_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tri</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e10</span>  <span class="c1"># [n_seq, n_seq]</span>
</span></span></code></pre></div><p>Combining all the things together, we get the following code.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c_attn</span><span class="p">,</span> <span class="n">c_proj</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">kvcache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># qkv projection</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># n_seq = 1 when we pass kvcache, so we will compute new_q, new_k and new_v</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_attn</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># split into qkv</span>
</span></span><span class="line"><span class="cl">    <span class="n">qkv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">kvcache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># qkv</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_q</span><span class="p">,</span> <span class="n">new_k</span><span class="p">,</span> <span class="n">new_v</span> <span class="o">=</span> <span class="n">qkv</span>  <span class="c1"># new_q, new_k, new_v = [1, n_embd]</span>
</span></span><span class="line"><span class="cl">        <span class="n">old_k</span><span class="p">,</span> <span class="n">old_v</span> <span class="o">=</span> <span class="n">kvcache</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">old_k</span><span class="p">,</span> <span class="n">new_k</span><span class="p">])</span> <span class="c1"># k = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">old_v</span><span class="p">,</span> <span class="n">new_v</span><span class="p">])</span> <span class="c1"># v = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span>
</span></span><span class="line"><span class="cl">        <span class="n">qkv</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">current_cache</span> <span class="o">=</span> <span class="p">[</span><span class="n">qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># split into heads</span>
</span></span><span class="line"><span class="cl">    <span class="n">qkv_heads</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">qkv</span><span class="p">))</span>  <span class="c1"># [3, n_seq, n_embd] -&gt; [n_head, 3, n_seq, n_embd/n_head]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># causal mask to hide future inputs from being attended to</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">kvcache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">causal_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tri</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e10</span>  <span class="c1"># [n_seq, n_seq]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># perform attention over each head</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_heads</span> <span class="o">=</span> <span class="p">[</span><span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span> <span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">qkv_heads</span><span class="p">)]</span>  <span class="c1"># [n_head, 3, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># merge heads</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">out_heads</span><span class="p">)</span>  <span class="c1"># [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># out projection</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_proj</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">current_cache</span>
</span></span></code></pre></div><p>We introduced minor breaking changes in output as well. We are introducing <code>current_cache</code> alongside our normal output. This is because we can use an updated cache for the next run.</p>
<p>We also need to change a few functions to make it work.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">transformer_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">ln_1</span><span class="p">,</span> <span class="n">ln_2</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">kvcache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># multi-head causal self attention</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_out</span><span class="p">,</span> <span class="n">kvcache_updated</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">ln_1</span><span class="p">),</span> <span class="o">**</span><span class="n">attn</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span> <span class="n">kvcache</span><span class="o">=</span><span class="n">kvcache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">attn_out</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># position-wise feed forward network</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">ffn</span><span class="p">(</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">ln_2</span><span class="p">),</span> <span class="o">**</span><span class="n">mlp</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">kvcache_updated</span>
</span></span></code></pre></div><p>We added <code>kvcache</code> as an input to the function and returned <code>kvcache_updated</code> as an output for each transformer block. We also need to change <code>transformer</code> function.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="k">def</span> <span class="nf">gpt2</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">wte</span><span class="p">,</span> <span class="n">wpe</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">ln_f</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">kvcache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>  <span class="c1"># [n_seq] -&gt; [n_seq, n_vocab]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">kvcache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">kvcache</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">blocks</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wpe_out</span> <span class="o">=</span> <span class="n">wpe</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span> <span class="c1"># cache already available, only send last token as input for predicting next token</span>
</span></span><span class="line"><span class="cl">        <span class="n">wpe_out</span> <span class="o">=</span> <span class="n">wpe</span><span class="p">[[</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># token + positional embeddings</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">wte</span><span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">wpe_out</span>  <span class="c1"># [n_seq] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># forward pass through n_layer transformer blocks</span>
</span></span><span class="line"><span class="cl">    <span class="n">new_kvcache</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">block</span><span class="p">,</span> <span class="n">kvcache_block</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">blocks</span><span class="p">,</span> <span class="n">kvcache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">,</span> <span class="n">updated_cache</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">block</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span> <span class="n">kvcache</span><span class="o">=</span><span class="n">kvcache_block</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_kvcache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">updated_cache</span><span class="p">)</span>  <span class="c1"># TODO: inplace extend new cache instead of re-saving whole</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># projection to vocab</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">ln_f</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">wte</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">new_kvcache</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_vocab]</span>
</span></span></code></pre></div><p>Notice, When we have already compute <code>kvcache</code>, we only return input last token to GPT2 alongside with <code>kvcache</code>. You can also see <code>len(kvcache) == # number of transformer blocks</code>. This is because we need to update <code>kvcache</code> for attention and we have single attention in each transformer block.</p>
<p>And, finally, it&rsquo;s time to change our <code>generate</code> function to use cache. In the first iteration, we will not have <code>kvcache</code> and we will pass <code>kvcache=None</code> to <code>gpt2</code> function. In subsequent iterations, we will utilise the previously generated <code>kvcache</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">kvcache</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_tokens_to_generate</span><span class="p">),</span> <span class="s2">&#34;generating&#34;</span><span class="p">):</span>  <span class="c1"># auto-regressive decode loop</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">,</span> <span class="n">kvcache</span> <span class="o">=</span> <span class="n">gpt2</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span> <span class="n">kvcache</span><span class="o">=</span><span class="n">kvcache</span><span class="p">)</span>  <span class="c1"># model forward pass</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_id</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># greedy sampling</span>
</span></span><span class="line"><span class="cl">    <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="n">next_id</span><span class="p">])</span>  <span class="c1"># append prediction to input</span>
</span></span></code></pre></div><p>This cache helps us to reduce computation for each iteration. We can see that, in first iteration, we are computing attention for all tokens in input. But, in subsequent iterations, we are only computing attention for last token. Reducing time complexity from <code>O(n^2)</code> to <code>O(n)</code>.</p>
<p>Finally, we can verify generate text with our previous code which didn&rsquo;t have caching and compare two output. Both output should be same.</p>
<p>In terminal</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">python</span> <span class="n">gpt2_kvcache</span><span class="o">.</span><span class="n">py</span> <span class="s2">&#34;Alan Turing theorized that computers would one day become&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">Output</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"> <span class="n">the</span> <span class="n">most</span> <span class="n">powerful</span> <span class="n">machines</span> <span class="n">on</span> <span class="n">the</span> <span class="n">planet</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">The</span> <span class="n">computer</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">machine</span> <span class="n">that</span> <span class="n">can</span> <span class="n">perform</span> <span class="nb">complex</span> <span class="n">calculations</span><span class="p">,</span> <span class="ow">and</span> <span class="n">it</span> <span class="n">can</span> <span class="n">perform</span> <span class="n">these</span> <span class="n">calculations</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">way</span> <span class="n">that</span> <span class="ow">is</span> <span class="n">very</span> <span class="n">similar</span> <span class="n">to</span> <span class="n">the</span> <span class="n">human</span> <span class="n">brain</span><span class="o">.</span>
</span></span></code></pre></div><p>You can see the all the code in this <a href="https://github.com/jaymody/picoGPT/pull/7/files">pull request</a>. You can also see the code in this <a href="https://github.com/immortal3/picoGPT">repository</a>.</p>
<p>You can see more details of calculation related to kv cache memory footprint calculation and computation time in this <a href="https://kipp.ly/blog/transformer-inference-arithmetic/">blog post</a>.</p>
<p>References:</p>
<ol>
<li><a href="https://kipp.ly/blog/transformer-inference-arithmetic/">https://kipp.ly/blog/transformer-inference-arithmetic/</a></li>
<li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</a></li>
<li><a href="https://jaykmody.com/blog/gpt-from-scratch/">https://jaykmody.com/blog/gpt-from-scratch/</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://dipkumar.dev/tags/transformer/">Transformer</a></li>
      <li><a href="https://dipkumar.dev/tags/nlp/">Nlp</a></li>
      <li><a href="https://dipkumar.dev/tags/gpt/">Gpt</a></li>
      <li><a href="https://dipkumar.dev/tags/speedup/">Speedup</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://dipkumar.dev/">Dip&#39;Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
