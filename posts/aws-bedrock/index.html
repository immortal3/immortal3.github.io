<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AWS BedRock - Converse API - A single endpoint for all models ? | Becoming The Unbeatable against AGI</title><meta name=keywords content="llm,ai"><meta name=description content="Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources."><meta name=author content="Me"><link rel=canonical href=https://immortal3.github.io/posts/aws-bedrock/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://immortal3.github.io/small_icon.jpg><link rel=icon type=image/png sizes=16x16 href=https://immortal3.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://immortal3.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://immortal3.github.io/apple-touch-icon.png><link rel=mask-icon href=https://immortal3.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-PCR7JSXYVY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PCR7JSXYVY",{anonymize_ip:!1})}</script><meta property="og:title" content="AWS BedRock - Converse API - A single endpoint for all models ?"><meta property="og:description" content="Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources."><meta property="og:type" content="article"><meta property="og:url" content="https://immortal3.github.io/posts/aws-bedrock/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-13T23:15:53+05:30"><meta property="article:modified_time" content="2024-06-13T23:15:53+05:30"><meta property="og:site_name" content="Becoming The Unbeatable against AGI"><meta name=twitter:card content="summary"><meta name=twitter:title content="AWS BedRock - Converse API - A single endpoint for all models ?"><meta name=twitter:description content="Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://immortal3.github.io/posts/"},{"@type":"ListItem","position":3,"name":"AWS BedRock - Converse API - A single endpoint for all models ?","item":"https://immortal3.github.io/posts/aws-bedrock/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AWS BedRock - Converse API - A single endpoint for all models ?","name":"AWS BedRock - Converse API - A single endpoint for all models ?","description":"Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.","keywords":["llm","ai"],"articleBody":"Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. [1]\nAWS BedRock’s Converse API is a single endpoint that allows you to chat with any model. Indeed, the single endpoint is, I believe, the best feature of AWS BedRock. Let’s visit this endpoint and see how it works.\nmodel_id = \"anthropic.claude-3-sonnet-20240229-v1:0\" inference_config = {\"temperature\": 0.5} additional_model_fields = {\"top_k\": 200} # Send the message. response = bedrock_client.converse( modelId=model_id, messages=messages, system=system_prompts, inferenceConfig=inference_config, additionalModelRequestFields=additional_model_fields ) By changing model_id, you can switch between different models.\nI think AWS BedRock should have used the same standards as OpenAI’s client rather than creating their own But, hey, it’s still a single endpoint. right ?… I should be just able to switch models by changing model_id. right ?….\nHidden Gotcha of Converse API Not every model is available AWS BedRock has LLama3, Anthropic Claude, Mistral and their own Titan. But, It doesn’t have OpenAI models like GPT-4/GPT-4o. This might not be a deal breaker, depending on what you are trying to achieve. You can check the availability of models in AWS Bedrock Models\nNot every model has system prompt, or multi-modality support If you check converse API parameters, you will see that there is a parameter called system. This parameter is used to provide system prompt to the model. However, not every model supports system prompts. (Because they were not trained with system prompts). If you’re switching between models via code using ENV/Flags/Config, you might need to handle edge cases where a system prompt is unavailable for the given modelId. Otherwise, It will throw an Exception. (Ideally, i think it should just give a warning) AWS has a nice table to check if given model has system prompt.\nThe same goes for multi-modality. If your messages include images, switching between models might not be straightforward.\nNot every model has same context window I mean this is on you, but again good reminder.\nAdvance Prompt technique like Prefilling Assistant Message # code copied from https://eugeneyan.com//writing/prompting/#prefill-claudes-responses input = \"\"\" The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99. At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice or app—no matter where you place it in your home. This affordable little hub brings convenient hands-free control to your smart devices. Extract the , , , and from this product . Return the extracted attributes within . \"\"\" messages=[ { \"role\": \"user\", \"content\": input, }, { \"role\": \"assistant\", \"content\": \"\" # Prefilled response } ] # raise error_class(parsed_response, operation_name) # botocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the Converse # operation: The model that you are using requires the last turn in the conversation to be a user message. Add a # user message to the conversation and try again. If you’re using advanced prompting techniques, such as Prefilling Assistant Messages [3], where you pre-populate the message with text designated as ‘assistant’, you need to be cautious when switching between models. Not all models are compatible with this technique and their is validation check which will raise exception.\nSo, Overall, We are still far away from having a unified API for all models. I will update this article if i find anything new.\nReferences [1] https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html\n[2] https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features\n[3] https://eugeneyan.com//writing/prompting/#prefill-claudes-responses\n","wordCount":"654","inLanguage":"en","datePublished":"2024-06-13T23:15:53+05:30","dateModified":"2024-06-13T23:15:53+05:30","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://immortal3.github.io/posts/aws-bedrock/"},"publisher":{"@type":"Organization","name":"Becoming The Unbeatable against AGI","logo":{"@type":"ImageObject","url":"https://immortal3.github.io/small_icon.jpg"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://immortal3.github.io accesskey=h title="Home (Alt + H)"><img src=https://immortal3.github.io/small_icon.jpg alt aria-label=logo height=25>Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://immortal3.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://immortal3.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://immortal3.github.io/ title=home><span>home</span></a></li><li><a href=https://immortal3.github.io/notes/ title=notes><span>notes</span></a></li><li><a href=https://immortal3.github.io/posts/ title=posts><span>posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://immortal3.github.io>Home</a>&nbsp;»&nbsp;<a href=https://immortal3.github.io/posts/>Posts</a></div><h1 class=post-title>AWS BedRock - Converse API - A single endpoint for all models ?</h1><div class=post-meta><span title='2024-06-13 23:15:53 +0530 IST'>June 13, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Me</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#hidden-gotcha-of-converse-api aria-label="Hidden Gotcha of Converse API">Hidden Gotcha of Converse API</a><ul><li><a href=#not-every-model-is-available aria-label="Not every model is available">Not every model is available</a></li><li><a href=#not-every-model-has-system-prompt-or-multi-modality-support aria-label="Not every model has system prompt, or multi-modality support">Not every model has system prompt, or multi-modality support</a></li><li><a href=#not-every-model-has-same-context-window aria-label="Not every model has same context window">Not every model has same context window</a></li><li><a href=#advance-prompt-technique-like-prefilling-assistant-message aria-label="Advance Prompt technique like Prefilling Assistant Message">Advance Prompt technique like Prefilling Assistant Message</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. [1]</p><p>AWS BedRock&rsquo;s Converse API is a single endpoint that allows you to chat with any model. Indeed, <strong>the single endpoint</strong> is, I believe, the best feature of AWS BedRock. Let&rsquo;s visit this endpoint and see how it works.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;anthropic.claude-3-sonnet-20240229-v1:0&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>inference_config <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;temperature&#34;</span>: <span style=color:#ae81ff>0.5</span>}
</span></span><span style=display:flex><span>additional_model_fields <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;top_k&#34;</span>: <span style=color:#ae81ff>200</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Send the message.</span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> bedrock_client<span style=color:#f92672>.</span>converse(
</span></span><span style=display:flex><span>    modelId<span style=color:#f92672>=</span>model_id,
</span></span><span style=display:flex><span>    messages<span style=color:#f92672>=</span>messages,
</span></span><span style=display:flex><span>    system<span style=color:#f92672>=</span>system_prompts,
</span></span><span style=display:flex><span>    inferenceConfig<span style=color:#f92672>=</span>inference_config,
</span></span><span style=display:flex><span>    additionalModelRequestFields<span style=color:#f92672>=</span>additional_model_fields
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>By changing <code>model_id</code>, you can switch between different models.</p><p>I think AWS BedRock should have used the same standards as OpenAI&rsquo;s client rather than creating their own
But, hey, it&rsquo;s still a single endpoint. right ?&mldr;
I should be just able to switch models by changing model_id. right ?&mldr;.</p><p><img loading=lazy src="https://i.imgflip.com/5bgun8.jpg?a477264=400x400" alt="AWS BedRock Endpoint"></p><h2 id=hidden-gotcha-of-converse-api>Hidden Gotcha of Converse API<a hidden class=anchor aria-hidden=true href=#hidden-gotcha-of-converse-api>#</a></h2><h3 id=not-every-model-is-available>Not every model is available<a hidden class=anchor aria-hidden=true href=#not-every-model-is-available>#</a></h3><p>AWS BedRock has LLama3, Anthropic Claude, Mistral and their own Titan. But, It doesn&rsquo;t have OpenAI models like GPT-4/GPT-4o. This might not be a deal breaker, depending on what you are trying to achieve.
You can check the availability of models in <a href=https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html>AWS Bedrock Models</a></p><h3 id=not-every-model-has-system-prompt-or-multi-modality-support>Not every model has system prompt, or multi-modality support<a hidden class=anchor aria-hidden=true href=#not-every-model-has-system-prompt-or-multi-modality-support>#</a></h3><p>If you check converse API parameters, you will see that there is a parameter called <code>system</code>. This parameter is used to provide system prompt to the model. However, not every model supports system prompts. (Because they were not trained with system prompts). If you&rsquo;re switching between models via code using ENV/Flags/Config, you might need to handle edge cases where a system prompt is unavailable for the given <code>modelId</code>. Otherwise, It will throw an Exception. (Ideally, i think it should just give a warning)
AWS has a <a href=https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features>nice table</a> to check if given model has system prompt.</p><p>The same goes for multi-modality. If your messages include images, switching between models might not be straightforward.</p><h3 id=not-every-model-has-same-context-window>Not every model has same context window<a hidden class=anchor aria-hidden=true href=#not-every-model-has-same-context-window>#</a></h3><p>I mean this is on you, but again good reminder.</p><h3 id=advance-prompt-technique-like-prefilling-assistant-message>Advance Prompt technique like Prefilling Assistant Message<a hidden class=anchor aria-hidden=true href=#advance-prompt-technique-like-prefilling-assistant-message>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># code copied from https://eugeneyan.com//writing/prompting/#prefill-claudes-responses</span>
</span></span><span style=display:flex><span>input <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;description&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>The SmartHome Mini is a compact smart home assistant available in black or white for 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>only $49.99. At just 5 inches wide, it lets you control lights, thermostats, and other 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>connected devices via voice or app—no matter where you place it in your home. This 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>affordable little hub brings convenient hands-free control to your smart devices.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;/description&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Extract the &lt;name&gt;, &lt;size&gt;, &lt;price&gt;, and &lt;color&gt; from this product &lt;description&gt;.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Return the extracted attributes within &lt;attributes&gt;.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>messages<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;content&#34;</span>: input,
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;assistant&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;&lt;attributes&gt;&lt;name&gt;&#34;</span>  <span style=color:#75715e># Prefilled response</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span><span style=color:#75715e># raise error_class(parsed_response, operation_name)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># botocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the Converse  </span>
</span></span><span style=display:flex><span><span style=color:#75715e># operation: The model that you are using requires the last turn in the conversation to be a user message. Add a </span>
</span></span><span style=display:flex><span><span style=color:#75715e># user message to the conversation and try again.</span>
</span></span></code></pre></div><p>If you&rsquo;re using advanced prompting techniques, such as Prefilling Assistant Messages [3], where you pre-populate the message with text designated as &lsquo;assistant&rsquo;, you need to be cautious when switching between models. Not all models are compatible with this technique and their is validation check which will raise exception.</p><p>So, Overall, We are still far away from having a unified API for all models. I will update this article if i find anything new.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <a href=https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html>https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html</a></p><p>[2] <a href=https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features>https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features</a></p><p>[3] <a href=https://eugeneyan.com//writing/prompting/#prefill-claudes-responses>https://eugeneyan.com//writing/prompting/#prefill-claudes-responses</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://immortal3.github.io/tags/llm/>llm</a></li><li><a href=https://immortal3.github.io/tags/ai/>ai</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://immortal3.github.io>Becoming The Unbeatable against AGI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>