<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
        Bits-per-Byte (BPB): a tokenizer-agnostic way to measure LLMs | Dip&#39;s Blog
    </title>
    <meta name="description"
        content="Thoughts on Machine Learning, AI, and Software Engineering by Dipkumar Patel">
    <meta property="og:title" content="Bits-per-Byte (BPB): a tokenizer-agnostic way to measure LLMs" />
    <meta property="og:description"
        content="Thoughts on Machine Learning, AI, and Software Engineering by Dipkumar Patel" />
    <meta property="og:type" content="article" />
    <meta property="og:image" content="https://dipkumar.dev/static/social-share.jpg" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:image" content="https://dipkumar.dev/static/social-share.jpg" />

    <link rel="icon" type="image/jpeg" href="/static/icon.jpeg">
    <link rel="stylesheet" href="/static/style.css">
    <link rel="stylesheet" href="/static/fish.css">

    <!-- Prism.js for Syntax Highlighting -->
    <link href="https://unpkg.com/prismjs@1.29.0/themes/prism.css" rel="stylesheet" />

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"
        integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
</head>

<body>
    <div class="container">
        
                        <nav class="post-nav">
                            <a href="/">‚Üê Home</a>
                            <a href="/blogs" class="all-posts-link">All Posts</a>
                        </nav>
                        <article class="post-content">
                            <header>
                                <h1>
                                    Bits-per-Byte (BPB): a tokenizer-agnostic way to measure LLMs
                                </h1>
                                <div class="meta-row">
                                    <div class="author-meta">
                                        <span class="social-mini">
                                            <a href="https://x.com/immortaldip" target="_blank">ùïè</a>
                                            <a href="https://github.com/immortal3" target="_blank">github</a>
                                            <a href="mailto:patel@dipkumar.dev">email</a>
                                        </span>
                                        By <a href="/" class="author-link">Dipkumar Patel</a>
                                    </div>
                                    <div class="date-meta">
                                        October 15, 2025
                                    </div>
                                </div>
                            </header>
                            <div class="markdown-body">
                                <blockquote>
<p>Karpathy recently released <a href="https://github.com/karpathy/nanochat/">nanochat repo</a> which cotains code for <strong>training the best ChatGPT under $100</strong>. While skimming the high level code, I noticed across <code>bits per bytes</code> instead of typical <code>cross entropy</code> loss. And, i found it interesting, so i decided to dig in. </p>
</blockquote>
<h3>TL;DR</h3>
<ul>
<li>Bit per byte (BPB) is just cross-entropy measured per byte. We divide cross-entropy by bytes and log(2) to convert to bits.</li>
<li>Because it‚Äôs per byte, BPB is tokenizer-agnostic and lets you compare models fairly even when they use different vocabularies and rules.</li>
<li>Perplexity and token-level loss change when you change the tokenizer; BPB largely doesn‚Äôt.</li>
</ul>
<p>LLM doesn&#39;t predict the text, it predicts the (next) token. But token definitions depend on the tokenizer (BPE, Unigram, merges, special tokens, etc.). Swap tokenizers and the same sentence can become more or fewer tokens. So <code>per-token</code> metrics (avg CE, perplexity) change even if the underlying modeling quality didn‚Äôt.</p>
<p>Some popular tokenizer choices are:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Tokenizer</th>
<th>Vocab Size</th>
</tr>
</thead>
<tbody><tr>
<td>GPT-4</td>
<td>cl100k_base (BPE)</td>
<td>100,256</td>
</tr>
<tr>
<td>LLaMA 3</td>
<td>TikToken (BPE)</td>
<td>128,000</td>
</tr>
<tr>
<td>Gemini 2.5</td>
<td>SentencePiece (Unigram)</td>
<td>256,000</td>
</tr>
<tr>
<td>Claude</td>
<td>closed-source</td>
<td>undisclosed</td>
</tr>
</tbody></table>
<p>Different tokenizers ‚â† comparable &quot;tokens&quot;. So a model that uses a coarser tokenizer (fewer, longer tokens) can appear to have a lower per-token loss or perplexity, simply because the denominator changed.</p>
<p>Instead of normalizing loss per token, normalize per byte of UTF-8 text that those tokens represent. Then, no matter how you split words into tokens, you&#39;re still asking: how many bits, on average, does the model need to encode each byte of text?</p>
<h3>Example: Why Per-Token Metrics Mislead</h3>
<p>Consider two models predicting &quot;The Capital of India&quot; -&gt; &quot; is Delhi&quot; (8 bytes in UTF-8, including the space):</p>
<p><strong>Model A</strong> (coarse tokenizer):</p>
<ul>
<li>Tokens: <code>[&quot; is&quot;, &quot; Delhi&quot;]</code> (2 tokens)</li>
<li>Per-token loss: <code>[1.5, 4.5]</code> nats</li>
<li>Total loss: 6.0 nats</li>
</ul>
<p><strong>Model B</strong> (fine-grained tokenizer):</p>
<ul>
<li>Tokens: <code>[&quot; is&quot;, &quot; Del&quot;, &quot;hi&quot;]</code> (3 tokens)  </li>
<li>Per-token loss: <code>[1.5, 2.0, 2.5]</code> nats</li>
<li>Total loss: 6.0 nats</li>
</ul>
<p><strong>Per-token metrics (misleading):</strong></p>
<pre><code class="language-bash">Model A avg loss:  6.0 / 2 = 3.0 nats/token
Model B avg loss:  6.0 / 3 = 2.0 nats/token  ‚Üê appears better!

Model A perplexity:  exp(3.0) = 20.09
Model B perplexity:  exp(2.0) = 7.39        ‚Üê appears better!
</code></pre>
<p>Model B looks significantly better, but it&#39;s the <strong>same 6.0 nats</strong> spread over more tokens.</p>
<p><strong>Bits-per-byte (fair comparison):</strong></p>
<pre><code class="language-bash">Model A BPB:  6.0 / (ln(2) √ó 8) = 1.08 bits/byte
Model B BPB:  6.0 / (ln(2) √ó 8) = 1.08 bits/byte  ‚Üê identical!
</code></pre>
<p>BPB correctly shows both models have the same predictive quality. The apparent &quot;improvement&quot; in Model B&#39;s per-token metrics was purely an artifact of tokenization granularity.</p>
<h3>Implementation</h3>
<p>Below is the simplified and more readable version of the <a href="https://github.com/karpathy/nanochat/blob/master/nanochat/loss_eval.py">original code</a>.</p>
<pre><code class="language-python">import math
import torch
import torch.distributed as dist

@torch.no_grad()
def evaluate_bpb(model, batches, steps: int, token_bytes: torch.Tensor) -&gt; float:
    &quot;&quot;&quot;
    Compute Bits-Per-Byte (BPB) over `steps` batches.

    Shapes (your mental model):
      B  = batch size
      Seq = sequence length
      V  = vocab size

    Inputs:
      - model: callable like model(x, y, loss_reduction=&#39;none&#39;) -&gt; loss per token.
               Expects:
                 x: (B, Seq) token ids (int64)
                 y: (B, Seq) target token ids (int64), may contain ignore_index (&lt;0)
               Returns:
                 loss2d: (B, Seq) per-token loss in NATs (float32/float16)
      - batches: iterable yielding (x, y) as above.
      - steps: number of batches to evaluate.
      - token_bytes: (V,) int64 ‚Äî byte length of each token id; 0 for special tokens
                     (those should not count toward BPB).

    Notes:
      - BPB = (sum of losses in NATs over *counted* tokens) / (ln(2) * total_counted_bytes)
      - Tokens contribute to the denominator by their byte length; tokens with 0 bytes
        (specials) and ignored targets (&lt;0) are excluded from both numerator &amp; denominator.
    &quot;&quot;&quot;
    device = model.get_device() if hasattr(model, &quot;get_device&quot;) else next(model.parameters()).device

    # Accumulators across steps (and later across ranks)
    sum_nats  = torch.tensor(0.0, dtype=torch.float32, device=device)  # scalar
    sum_bytes = torch.tensor(0,   dtype=torch.int64,   device=device)  # scalar

    token_bytes = token_bytes.to(device=device, dtype=torch.int64)     # (V,)

    batch_iter = iter(batches)
    for _ in range(steps):
        x, y = next(batch_iter)                  # x: (B, Seq), y: (B, Seq)
        x = x.to(device)
        y = y.to(device)

        loss2d = model(x, y, loss_reduction=&#39;none&#39;)  # (B, Seq) NATs
        loss1d = loss2d.reshape(-1)                  # (B*Seq,)
        y1d    = y.reshape(-1)                       # (B*Seq,)

        if (y1d &lt; 0).any():
            # Mask out ignore_index (&lt;0) before indexing into token_bytes
            valid  = (y1d &gt;= 0)                                      # (B*Seq,)
            ysafe  = torch.where(valid, y1d, torch.zeros_like(y1d))  # (B*Seq,)
            nb     = torch.where(valid, token_bytes[ysafe], torch.zeros_like(y1d))  # (B*Seq,) int64
        else:
            nb = token_bytes[y1d]  # (B*Seq,) int64

        # Count only tokens with positive byte length
        counted = (nb &gt; 0)                             # (B*Seq,) bool
        sum_nats  += (loss1d[counted]).sum()           # scalar
        sum_bytes += nb[counted].sum()                 # scalar int64

    # Distributed sum over all ranks, if initialized
    if dist.is_initialized() and dist.get_world_size() &gt; 1:
        dist.all_reduce(sum_nats,  op=dist.ReduceOp.SUM)
        dist.all_reduce(sum_bytes, op=dist.ReduceOp.SUM)

    total_nats  = float(sum_nats.item())
    total_bytes = int(sum_bytes.item())

    # Guard against division by zero (e.g., all tokens were special/ignored)
    if total_bytes == 0:
        return float(&quot;nan&quot;)

    bpb = total_nats / (math.log(2.0) * total_bytes)
    return bpb
</code></pre>

                            </div>

                            
                                <div class="post-footer-tags">
                                    
                                        <a href="/tags/llm">#llm</a>
                                        
                                        <a href="/tags/tokenizer">#tokenizer</a>
                                        
                                </div>
                                
                        </article>
                        
    </div>

    <!-- Prism JS for Syntax Highlighting -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure autoloader path
        Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';

        // Highlight on load
        document.addEventListener('DOMContentLoaded', function () {
            Prism.highlightAll();
        });

        
    </script>
</body>

</html>