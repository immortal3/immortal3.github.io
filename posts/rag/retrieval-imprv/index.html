<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>No Bullshit Guide to Improving Retrieval in RAG | Dip&#39;Blog</title>
<meta name="keywords" content="">
<meta name="description" content="A practical guide to improving retrieval in RAG systems by optimizing recall, precision, and NDCG">
<meta name="author" content="
Author:
Dipkumar Patel">
<link rel="canonical" href="https://dipkumar.dev/posts/rag/retrieval-imprv/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.9a3a9b6cc85cc3f62d3346fcc6d5b9344945ceca05719846bfd9d1f7a39acc84.css" integrity="sha256-mjqbbMhcw/YtM0b8xtW5NElFzsoFcZhGv9nR96OazIQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://dipkumar.dev/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://dipkumar.dev/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://dipkumar.dev/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://dipkumar.dev/apple-touch-icon.png">
<link rel="mask-icon" href="https://dipkumar.dev/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://dipkumar.dev/posts/rag/retrieval-imprv/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="No Bullshit Guide to Improving Retrieval in RAG" />
<meta property="og:description" content="A practical guide to improving retrieval in RAG systems by optimizing recall, precision, and NDCG" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dipkumar.dev/posts/rag/retrieval-imprv/" />
<meta property="og:image" content="https://dipkumar.dev/images/papermod-cover.png" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-03-08T12:23:53+05:30" />
<meta property="article:modified_time" content="2025-03-08T12:23:53+05:30" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://dipkumar.dev/images/papermod-cover.png" />
<meta name="twitter:title" content="No Bullshit Guide to Improving Retrieval in RAG"/>
<meta name="twitter:description" content="A practical guide to improving retrieval in RAG systems by optimizing recall, precision, and NDCG"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://dipkumar.dev/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "No Bullshit Guide to Improving Retrieval in RAG",
      "item": "https://dipkumar.dev/posts/rag/retrieval-imprv/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "No Bullshit Guide to Improving Retrieval in RAG",
  "name": "No Bullshit Guide to Improving Retrieval in RAG",
  "description": "A practical guide to improving retrieval in RAG systems by optimizing recall, precision, and NDCG",
  "keywords": [
    
  ],
  "articleBody": "No Bullshit Guide to Improving Retrieval in RAG (via Recall, Precision, and NDCG) Introduction Retrieval-Augmented Generation (RAG) is the superhero sidekick that grounds your Large Language Model (LLM) in cold, hard facts. But here’s the dirty secret: if your retrieval sucks, your RAG system is just a fancy chatbot with a broken brain. Weak retrieval = missed documents, irrelevant results, and rankings that make no sense.\nThis guide cuts through the noise. You’ll learn how to turbocharge your RAG retrieval with a no-fluff, step-by-step approach to maximize recall, sharpen precision, and nail NDCG. Whether you’re a data scientist, developer, or AI enthusiast, this is your playbook to stop screwing around and start getting results. Let’s roll.\nThe Basics of Retrieval Vector Search vs. Full-Text Search Retrieval is the backbone of RAG, and it’s a tug-of-war between two heavyweights: vector search and full-text search. Here’s the breakdown:\nVector Search: Turns words into numbers (embeddings) to capture meaning. Think of it as a genius librarian who gets that “machine learning frameworks” is related to “neural network libraries” even if the exact words don’t match.\nExample: Query = “machine learning frameworks.” Vector search grabs articles about “PyTorch vs TensorFlow comparison” because it understands semantic similarity.\nFull-Text Search: The old-school keyword matcher. It’s like a librarian who only cares about exact titles—if “machine learning frameworks” isn’t in the text, you’re out of luck.\nExample: Same query, “machine learning frameworks.” Full-text search might miss that PyTorch article unless the phrase matches perfectly, but it’ll snag anything with “frameworks” lightning-fast.\nHere’s a quick comparison:\nFeature Vector Search Full-Text Search Strengths Semantic understanding Speed, exact matches Weaknesses Slower, resource-hungry Misses context Best For Complex queries Simple lookups Why Both Matter: Hybrid search (vector + keywords) is the cheat code. Combine them, and you get the best of both worlds—broad coverage with pinpoint accuracy.\nMetrics 101 – What to Optimize For You can’t fix what you don’t measure. Here’s your retrieval holy trinity:\nRecall: Are you finding all the good stuff?\nExample: Imagine 100 blog posts about “transformer architecture” exist. Your retriever grabs 85 of them. That’s 85% recall. Miss too many, and your LLM is flying blind.\nPrecision: Are you dodging the junk?\nExample: You retrieve 100 documents for “transformer architecture,” but only 70 are relevant (the rest are about “electrical transformers”). That’s 70% precision. Too much noise, and your RAG drowns in garbage.\nNDCG (Normalized Discounted Cumulative Gain): Are the best hits at the top?\nExample: Picture the perfect ranking: top 5 results about transformer models are gold, next 5 are decent. If your retriever puts electrical engineering papers at #1 and buries the good ML content at #10, your NDCG tanks. High NDCG = happy users.\nThe Hierarchy of Needs Recall First: Cast a wide net—don’t miss the critical docs. Precision Next: Trim the fat—keep only what’s relevant. NDCG Last: Polish the rankings—put the best up top. Step 1 – Maximizing Recall Why Recall First? If your retriever misses key documents, your generator’s toast. It’s like cooking a steak dinner with no steak. Recall is step one—get everything on the table.\nTactics to Boost Recall Query Expansion: Make your query a beast by adding synonyms or related terms.\nExample: Query = “transformer models.” Expand it to “attention mechanisms,” “BERT architecture,” “language model design.”\nWhat to do:\nCheck out WordNet for traditional expansion Use an LLM for contextual expansion or even re-writing to multiple different queries. In production, run all these expansions in parallel and merge results. Hybrid Search: Merge vector and keyword results like a DJ mixing tracks. Use reciprocal rank fusion (1/rank) to blend the scores.\nExample: Query = “transformer models.” Vector search finds “attention mechanism design,” while full-text grabs “BERT model implementations.” Fusion ranks them smartly.\nWhat to do:\nUse a hybrid search engine like Pinecone, Qdrant, or TurboPuffer Fine-Tune Embeddings: Generic embeddings suck for niche domains. Train on your data—say, medical literature or financial reports—for better matches.\nExample: Fine-tune on a dataset of ML research papers. Now “transformer architecture” queries snag “multi-head attention mechanism” docs too.\nWhat to do:\nDo it yourself: fine-tune BAAI/bge-small on your own data and benchmark it against current embeddings Follow LlamaIndex’s guide on embedding fine-tuning Take inspiration from Glean, which fine-tunes embeddings for each customer (Video) Chunking Strategy: Break documents into bite-sized pieces. Smaller chunks (e.g., 256 tokens) catch more, but overlap them (e.g., 50 tokens) to keep context.\nExample: An ML research paper on “transformer models” split into 500-token chunks might miss a key implementation detail. Shrink to 250 tokens with overlap, and you nab it.\nPro Tip:\nDepending on your embedding model and domain, benchmark chunk size and overlap to find the best balance. Step 2 – Precision Tuning Why Precision Matters You’ve got a pile of docs—now ditch the trash. Precision ensures your RAG isn’t wading through irrelevant sludge.\nPrecision-Boosting Strategies Re-Rankers: Run a heavy-hitter model (e.g., BERT cross-encoder) on your top 50-100 results to rescore them.\nExample: Query = “transformer architecture.” Initial retrieval grabs 100 docs, including some about “electrical power transformers.” A re-ranker kicks out the electrical engineering stuff, keeping ML architecture gold.\nWhat to do:\nUse Cohere’s Rerank API, it’s dead simple to integrate For brave souls, try open-source options such as ColBERT and BAAI/bge-reranker-base Metadata Filtering: Use tags like date, category, or source to slice the fat.\nExample: Query = “transformer models.” Filter out docs older than 2020 or from non-ML domains—bam, instant precision boost.\nWhat to do:\nImplement with vector databases like Pinecone, TurboPuffer, or Qdrant that support metadata filtering Thresholding: Set a similarity cutoff (e.g., cosine \u003e 0.5) to trash low-confidence matches.\nExample: Query = “transformer architecture.” Docs below 0.5 might be random electrical engineering content—drop ’em and keep the signal.\nWhat to do:\nConfigure similarity score thresholds in your vector database query APIs Step 3 – NDCG Optimization Why Ranking Matters You’ve maximized recall and precision—now make sure the gold is at the top. With LLMs having finite token limits, the order of retrieval can make or break your RAG system. If your best content is buried at position #30, your LLM might never see it.\nRanking Improvement Strategies Reranking: Use re-rankers to filter and re-rank your results. This helps to improve both precision and NDCG.\nUser Feedback Integration: Capture what users actually find valuable and use it to improve your rankings.\nExample: Users consistently reference information from the third document in your RAG answers for “transformer applications.” Your system learns to boost similar documents higher for that query, dramatically improving NDCG.\nWhat to do:\nTrack interactions: Implement explicit feedback (thumbs up/down) and implicit signals (time spent, follow-up questions) Build feedback loops: Create a simple database that stores query-document pairs with user ratings Implement active learning: Prioritize collecting feedback on borderline documents where the system is uncertain Curate your corpus: Ruthlessly remove consistently low-rated documents from your vector database—this is a game-changer that most teams overlook Apply immediate boosts: For frequent queries, manually boost documents with positive feedback by 1.2-1.5x in your ranking algorithm Pro Tip: Don’t wait for perfect data—start with a simple “Was this helpful?” button after each RAG response, and you’ll be shocked how quickly you can improve rankings with even sparse feedback.\nContext is King: Leverage conversation history to supercharge your retrieval relevance.\nExample: A user asks “What are the best frameworks?” after discussing PyTorch for 10 minutes. Without context, you might return generic framework docs. With context, you nail it with PyTorch-specific framework comparisons.\nWhat to do:\nStore conversation history: Keep the last 3-5 exchanges in a context window Question rewriting: Use the history to expand ambiguous queries Context-aware filtering: Use topics from previous exchanges to filter metadata Pro Tip: Don’t just append history blindly—it creates noise. Instead, extract key entities and concepts from previous exchanges and use them to enrich your current query. For example, if discussing “transformer models for NLP tasks,” extract “transformer” + “NLP” as context boosters.\nMeasuring NDCG Improvement Don’t fly blind—benchmark your changes:\nCreate a test set with queries and human-judged relevance scores Calculate NDCG@k (typically k=5 or k=10) before and after changes Aim for at least 5-10% lift in NDCG to justify implementation costs Pro Tip: Let’s do some LLM math that won’t make your brain explode! Focus on NDCG@k based on your document size, because your poor LLM can only eat so many tokens before it gets a tummy ache.\nHere’s a real-world example with numbers so simple even your coffee-deprived morning brain can handle them:\nYour average document: 10,000 tokens (that’s a chatty document!) Your fancy GPT-4o: 128,000 token capacity (big brain energy!) Your context + prompt: ~3,000 tokens (the appetizer) Now for the main course calculation: 10,000 tokens × 10 documents = 100,000 tokens 100,000 tokens + 3,000 tokens = 103,000 tokens\n103,000 \u003c 128,000… We’re good! 🎉\nConclusion: Build a Retrieval Flywheel Here’s the game plan:\nHybrid Search: Max out recall—grab everything. Re-Rankers: Sharpen precision—ditch the junk. Contextual Ranking: Make sure the gold is at the top. This isn’t a one-and-done deal. It’s a flywheel—every tweak spins it faster. Experiment with chunk sizes, thresholds, and models. Small wins stack up to massive gains.\nFinal Tip: Don’t guess—test. Try a 0.7 threshold vs. 0.9. Swap 256-token chunks for 512. Data beats dogma.\nRetrieval Cheat Sheet Step Goal Tactics 1. Recall Grab everything Query Expansion, Hybrid Search, Fine-Tuning, Chunking 2. Precision Ditch the junk Re-Rankers, Metadata Filters, Thresholds 3. NDCG Perfect rankings Reranking, User Feedback, Context That’s it—your RAG retrieval is now a lean, mean, result-spitting machine. Go forth and dominate!\n",
  "wordCount" : "1587",
  "inLanguage": "en",
  "image": "https://dipkumar.dev/images/papermod-cover.png","datePublished": "2025-03-08T12:23:53+05:30",
  "dateModified": "2025-03-08T12:23:53+05:30",
  "author":{
    "@type": "Person",
    "name": "Dipkumar Patel"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dipkumar.dev/posts/rag/retrieval-imprv/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Dip'Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dipkumar.dev/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://dipkumar.dev/" accesskey="h" title="Dip&#39;Blog (Alt + H)">Dip&#39;Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://dipkumar.dev/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://dipkumar.dev/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://dipkumar.dev/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      No Bullshit Guide to Improving Retrieval in RAG
    </h1>
    <div class="post-description">
      A practical guide to improving retrieval in RAG systems by optimizing recall, precision, and NDCG
    </div>
    <div class="post-meta"><span title='2025-03-08 12:23:53 +0530 IST'> March 8, 2025</span>&nbsp;|&nbsp;Estimated Reading Time: 8 min&nbsp;|&nbsp;
Author:
Dipkumar Patel&nbsp;|&nbsp;<a href="https://github.com/adityatelange/hugo-PaperMod/tree/exampleSite/content/posts/rag/retrieval-imprv.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#no-bullshit-guide-to-improving-retrieval-in-rag-via-recall-precision-and-ndcg" aria-label="No Bullshit Guide to Improving Retrieval in RAG (via Recall, Precision, and NDCG)">No Bullshit Guide to Improving Retrieval in RAG (via Recall, Precision, and NDCG)</a><ul>
                        
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#the-basics-of-retrieval" aria-label="The Basics of Retrieval">The Basics of Retrieval</a><ul>
                        
                <li>
                    <a href="#vector-search-vs-full-text-search" aria-label="Vector Search vs. Full-Text Search">Vector Search vs. Full-Text Search</a></li></ul>
                </li>
                <li>
                    <a href="#metrics-101--what-to-optimize-for" aria-label="Metrics 101 – What to Optimize For">Metrics 101 – What to Optimize For</a><ul>
                        
                <li>
                    <a href="#the-hierarchy-of-needs" aria-label="The Hierarchy of Needs">The Hierarchy of Needs</a></li></ul>
                </li>
                <li>
                    <a href="#step-1--maximizing-recall" aria-label="Step 1 – Maximizing Recall">Step 1 – Maximizing Recall</a><ul>
                        
                <li>
                    <a href="#why-recall-first" aria-label="Why Recall First?">Why Recall First?</a></li>
                <li>
                    <a href="#tactics-to-boost-recall" aria-label="Tactics to Boost Recall">Tactics to Boost Recall</a></li></ul>
                </li>
                <li>
                    <a href="#step-2--precision-tuning" aria-label="Step 2 – Precision Tuning">Step 2 – Precision Tuning</a><ul>
                        
                <li>
                    <a href="#why-precision-matters" aria-label="Why Precision Matters">Why Precision Matters</a></li>
                <li>
                    <a href="#precision-boosting-strategies" aria-label="Precision-Boosting Strategies">Precision-Boosting Strategies</a></li></ul>
                </li>
                <li>
                    <a href="#step-3--ndcg-optimization" aria-label="Step 3 – NDCG Optimization">Step 3 – NDCG Optimization</a><ul>
                        
                <li>
                    <a href="#why-ranking-matters" aria-label="Why Ranking Matters">Why Ranking Matters</a></li>
                <li>
                    <a href="#ranking-improvement-strategies" aria-label="Ranking Improvement Strategies">Ranking Improvement Strategies</a></li>
                <li>
                    <a href="#measuring-ndcg-improvement" aria-label="Measuring NDCG Improvement">Measuring NDCG Improvement</a></li></ul>
                </li>
                <li>
                    <a href="#conclusion-build-a-retrieval-flywheel" aria-label="Conclusion: Build a Retrieval Flywheel">Conclusion: Build a Retrieval Flywheel</a></li>
                <li>
                    <a href="#retrieval-cheat-sheet" aria-label="Retrieval Cheat Sheet">Retrieval Cheat Sheet</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="no-bullshit-guide-to-improving-retrieval-in-rag-via-recall-precision-and-ndcg">No Bullshit Guide to Improving Retrieval in RAG (via Recall, Precision, and NDCG)<a hidden class="anchor" aria-hidden="true" href="#no-bullshit-guide-to-improving-retrieval-in-rag-via-recall-precision-and-ndcg">#</a></h1>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Retrieval-Augmented Generation (RAG) is the superhero sidekick that grounds your Large Language Model (LLM) in cold, hard facts. But here&rsquo;s the dirty secret: if your retrieval sucks, your RAG system is just a fancy chatbot with a broken brain. Weak retrieval = missed documents, irrelevant results, and rankings that make no sense.</p>
<p>This guide cuts through the noise. You&rsquo;ll learn how to turbocharge your RAG retrieval with a no-fluff, step-by-step approach to maximize recall, sharpen precision, and nail NDCG. Whether you&rsquo;re a data scientist, developer, or AI enthusiast, this is your playbook to stop screwing around and start getting results. Let&rsquo;s roll.</p>
<h2 id="the-basics-of-retrieval">The Basics of Retrieval<a hidden class="anchor" aria-hidden="true" href="#the-basics-of-retrieval">#</a></h2>
<h3 id="vector-search-vs-full-text-search">Vector Search vs. Full-Text Search<a hidden class="anchor" aria-hidden="true" href="#vector-search-vs-full-text-search">#</a></h3>
<p>Retrieval is the backbone of RAG, and it&rsquo;s a tug-of-war between two heavyweights: vector search and full-text search. Here&rsquo;s the breakdown:</p>
<p><strong>Vector Search</strong>: Turns words into numbers (embeddings) to capture meaning. Think of it as a genius librarian who gets that &ldquo;machine learning frameworks&rdquo; is related to &ldquo;neural network libraries&rdquo; even if the exact words don&rsquo;t match.</p>
<p><em>Example</em>: Query = &ldquo;machine learning frameworks.&rdquo; Vector search grabs articles about &ldquo;PyTorch vs TensorFlow comparison&rdquo; because it understands semantic similarity.</p>
<p><strong>Full-Text Search</strong>: The old-school keyword matcher. It&rsquo;s like a librarian who only cares about exact titles—if &ldquo;machine learning frameworks&rdquo; isn&rsquo;t in the text, you&rsquo;re out of luck.</p>
<p><em>Example</em>: Same query, &ldquo;machine learning frameworks.&rdquo; Full-text search might miss that PyTorch article unless the phrase matches perfectly, but it&rsquo;ll snag anything with &ldquo;frameworks&rdquo; lightning-fast.</p>
<p>Here&rsquo;s a quick comparison:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Vector Search</th>
<th>Full-Text Search</th>
</tr>
</thead>
<tbody>
<tr>
<td>Strengths</td>
<td>Semantic understanding</td>
<td>Speed, exact matches</td>
</tr>
<tr>
<td>Weaknesses</td>
<td>Slower, resource-hungry</td>
<td>Misses context</td>
</tr>
<tr>
<td>Best For</td>
<td>Complex queries</td>
<td>Simple lookups</td>
</tr>
</tbody>
</table>
<p><strong>Why Both Matter</strong>: Hybrid search (vector + keywords) is the cheat code. Combine them, and you get the best of both worlds—broad coverage with pinpoint accuracy.</p>
<h2 id="metrics-101--what-to-optimize-for">Metrics 101 – What to Optimize For<a hidden class="anchor" aria-hidden="true" href="#metrics-101--what-to-optimize-for">#</a></h2>
<p>You can&rsquo;t fix what you don&rsquo;t measure. Here&rsquo;s your retrieval holy trinity:</p>
<p><strong>Recall</strong>: Are you finding all the good stuff?</p>
<p><em>Example</em>: Imagine 100 blog posts about &ldquo;transformer architecture&rdquo; exist. Your retriever grabs 85 of them. That&rsquo;s 85% recall. Miss too many, and your LLM is flying blind.</p>
<p><strong>Precision</strong>: Are you dodging the junk?</p>
<p><em>Example</em>: You retrieve 100 documents for &ldquo;transformer architecture,&rdquo; but only 70 are relevant (the rest are about &ldquo;electrical transformers&rdquo;). That&rsquo;s 70% precision. Too much noise, and your RAG drowns in garbage.</p>
<p><strong>NDCG</strong> (Normalized Discounted Cumulative Gain): Are the best hits at the top?</p>
<p><em>Example</em>: Picture the perfect ranking: top 5 results about transformer models are gold, next 5 are decent. If your retriever puts electrical engineering papers at #1 and buries the good ML content at #10, your NDCG tanks. High NDCG = happy users.</p>
<h3 id="the-hierarchy-of-needs">The Hierarchy of Needs<a hidden class="anchor" aria-hidden="true" href="#the-hierarchy-of-needs">#</a></h3>
<ol>
<li><strong>Recall First</strong>: Cast a wide net—don&rsquo;t miss the critical docs.</li>
<li><strong>Precision Next</strong>: Trim the fat—keep only what&rsquo;s relevant.</li>
<li><strong>NDCG Last</strong>: Polish the rankings—put the best up top.</li>
</ol>
<h2 id="step-1--maximizing-recall">Step 1 – Maximizing Recall<a hidden class="anchor" aria-hidden="true" href="#step-1--maximizing-recall">#</a></h2>
<h3 id="why-recall-first">Why Recall First?<a hidden class="anchor" aria-hidden="true" href="#why-recall-first">#</a></h3>
<p>If your retriever misses key documents, your generator&rsquo;s toast. It&rsquo;s like cooking a steak dinner with no steak. Recall is step one—get everything on the table.</p>
<h3 id="tactics-to-boost-recall">Tactics to Boost Recall<a hidden class="anchor" aria-hidden="true" href="#tactics-to-boost-recall">#</a></h3>
<ol>
<li>
<p><strong>Query Expansion</strong>: Make your query a beast by adding synonyms or related terms.</p>
<p><em>Example</em>: Query = &ldquo;transformer models.&rdquo; Expand it to &ldquo;attention mechanisms,&rdquo; &ldquo;BERT architecture,&rdquo; &ldquo;language model design.&rdquo;</p>
<p><em>What to do</em>:</p>
<ul>
<li>Check out WordNet for traditional expansion</li>
<li>Use an LLM for contextual expansion or even re-writing to multiple different queries. In production, run all these expansions in parallel and merge results.</li>
</ul>
</li>
<li>
<p><strong>Hybrid Search</strong>: Merge vector and keyword results like a DJ mixing tracks. Use reciprocal rank fusion (1/rank) to blend the scores.</p>
<p><em>Example</em>: Query = &ldquo;transformer models.&rdquo; Vector search finds &ldquo;attention mechanism design,&rdquo; while full-text grabs &ldquo;BERT model implementations.&rdquo; Fusion ranks them smartly.</p>
<p><em>What to do</em>:</p>
<ul>
<li>Use a hybrid search engine like <a href="https://www.pinecone.io/learn/hybrid-search/">Pinecone</a>, <a href="https://qdrant.tech/articles/hybrid-search/">Qdrant</a>, or <a href="https://turbopuffer.com/docs/hybrid">TurboPuffer</a></li>
</ul>
</li>
<li>
<p><strong>Fine-Tune Embeddings</strong>: Generic embeddings suck for niche domains. Train on your data—say, medical literature or financial reports—for better matches.</p>
<p><em>Example</em>: Fine-tune on a dataset of ML research papers. Now &ldquo;transformer architecture&rdquo; queries snag &ldquo;multi-head attention mechanism&rdquo; docs too.</p>
<p><em>What to do</em>:</p>
<ul>
<li>Do it yourself: fine-tune <a href="https://huggingface.co/BAAI/bge-small-en">BAAI/bge-small</a> on your own data and benchmark it against current embeddings</li>
<li>Follow LlamaIndex&rsquo;s <a href="https://docs.llamaindex.ai/en/latest/examples/finetuning/embeddings/finetune_embedding/">guide on embedding fine-tuning</a></li>
<li>Take inspiration from Glean, which fine-tunes embeddings for each customer (<a href="https://www.youtube.com/watch?v=jTBsWJ2TKy8">Video</a>)</li>
</ul>
</li>
<li>
<p><strong>Chunking Strategy</strong>: Break documents into bite-sized pieces. Smaller chunks (e.g., 256 tokens) catch more, but overlap them (e.g., 50 tokens) to keep context.</p>
<p><em>Example</em>: An ML research paper on &ldquo;transformer models&rdquo; split into 500-token chunks might miss a key implementation detail. Shrink to 250 tokens with overlap, and you nab it.</p>
<p><em>Pro Tip</em>:</p>
<ul>
<li>Depending on your embedding model and domain, benchmark chunk size and overlap to find the best balance.</li>
</ul>
</li>
</ol>
<h2 id="step-2--precision-tuning">Step 2 – Precision Tuning<a hidden class="anchor" aria-hidden="true" href="#step-2--precision-tuning">#</a></h2>
<h3 id="why-precision-matters">Why Precision Matters<a hidden class="anchor" aria-hidden="true" href="#why-precision-matters">#</a></h3>
<p>You&rsquo;ve got a pile of docs—now ditch the trash. Precision ensures your RAG isn&rsquo;t wading through irrelevant sludge.</p>
<h3 id="precision-boosting-strategies">Precision-Boosting Strategies<a hidden class="anchor" aria-hidden="true" href="#precision-boosting-strategies">#</a></h3>
<ol>
<li>
<p><strong>Re-Rankers</strong>: Run a heavy-hitter model (e.g., BERT cross-encoder) on your top 50-100 results to rescore them.</p>
<p><em>Example</em>: Query = &ldquo;transformer architecture.&rdquo; Initial retrieval grabs 100 docs, including some about &ldquo;electrical power transformers.&rdquo; A re-ranker kicks out the electrical engineering stuff, keeping ML architecture gold.</p>
<p><em>What to do</em>:</p>
<ul>
<li>Use Cohere&rsquo;s Rerank API, it&rsquo;s dead simple to integrate</li>
<li>For brave souls, try open-source options such as <a href="https://github.com/stanford-futuredata/ColBERT">ColBERT</a> and <a href="https://huggingface.co/BAAI/bge-reranker-base">BAAI/bge-reranker-base</a></li>
</ul>
</li>
<li>
<p><strong>Metadata Filtering</strong>: Use tags like date, category, or source to slice the fat.</p>
<p><em>Example</em>: Query = &ldquo;transformer models.&rdquo; Filter out docs older than 2020 or from non-ML domains—bam, instant precision boost.</p>
<p><em>What to do</em>:</p>
<ul>
<li>Implement with vector databases like Pinecone, TurboPuffer, or Qdrant that support metadata filtering</li>
</ul>
</li>
<li>
<p><strong>Thresholding</strong>: Set a similarity cutoff (e.g., cosine &gt; 0.5) to trash low-confidence matches.</p>
<p><em>Example</em>: Query = &ldquo;transformer architecture.&rdquo; Docs below 0.5 might be random electrical engineering content—drop &rsquo;em and keep the signal.</p>
<p><em>What to do</em>:</p>
<ul>
<li>Configure similarity score thresholds in your vector database query APIs</li>
</ul>
</li>
</ol>
<h2 id="step-3--ndcg-optimization">Step 3 – NDCG Optimization<a hidden class="anchor" aria-hidden="true" href="#step-3--ndcg-optimization">#</a></h2>
<h3 id="why-ranking-matters">Why Ranking Matters<a hidden class="anchor" aria-hidden="true" href="#why-ranking-matters">#</a></h3>
<p>You&rsquo;ve maximized recall and precision—now make sure the gold is at the top. With LLMs having finite token limits, the order of retrieval can make or break your RAG system. If your best content is buried at position #30, your LLM might never see it.</p>
<h3 id="ranking-improvement-strategies">Ranking Improvement Strategies<a hidden class="anchor" aria-hidden="true" href="#ranking-improvement-strategies">#</a></h3>
<ol>
<li>
<p><strong>Reranking</strong>: Use re-rankers to filter and re-rank your results. This helps to improve both precision and NDCG.</p>
</li>
<li>
<p><strong>User Feedback Integration</strong>: Capture what users actually find valuable and use it to improve your rankings.</p>
<p><em>Example</em>: Users consistently reference information from the third document in your RAG answers for &ldquo;transformer applications.&rdquo; Your system learns to boost similar documents higher for that query, dramatically improving NDCG.</p>
<p><em>What to do</em>:</p>
<ul>
<li><strong>Track interactions</strong>: Implement explicit feedback (thumbs up/down) and implicit signals (time spent, follow-up questions)</li>
<li><strong>Build feedback loops</strong>: Create a simple database that stores query-document pairs with user ratings</li>
<li><strong>Implement active learning</strong>: Prioritize collecting feedback on borderline documents where the system is uncertain</li>
<li><strong>Curate your corpus</strong>: Ruthlessly remove consistently low-rated documents from your vector database—this is a game-changer that most teams overlook</li>
<li><strong>Apply immediate boosts</strong>: For frequent queries, manually boost documents with positive feedback by 1.2-1.5x in your ranking algorithm</li>
</ul>
<p><em>Pro Tip</em>: Don&rsquo;t wait for perfect data—start with a simple &ldquo;Was this helpful?&rdquo; button after each RAG response, and you&rsquo;ll be shocked how quickly you can improve rankings with even sparse feedback.</p>
</li>
<li>
<p><strong>Context is King</strong>: Leverage conversation history to supercharge your retrieval relevance.</p>
<p><em>Example</em>: A user asks &ldquo;What are the best frameworks?&rdquo; after discussing PyTorch for 10 minutes. Without context, you might return generic framework docs. With context, you nail it with PyTorch-specific framework comparisons.</p>
<p><em>What to do</em>:</p>
<ul>
<li><strong>Store conversation history</strong>: Keep the last 3-5 exchanges in a context window</li>
<li><strong>Question rewriting</strong>: Use the history to expand ambiguous queries</li>
<li><strong>Context-aware filtering</strong>: Use topics from previous exchanges to filter metadata</li>
</ul>
<p><em>Pro Tip</em>: Don&rsquo;t just append history blindly—it creates noise. Instead, extract key entities and concepts from previous exchanges and use them to enrich your current query. For example, if discussing &ldquo;transformer models for NLP tasks,&rdquo; extract &ldquo;transformer&rdquo; + &ldquo;NLP&rdquo; as context boosters.</p>
</li>
</ol>
<h3 id="measuring-ndcg-improvement">Measuring NDCG Improvement<a hidden class="anchor" aria-hidden="true" href="#measuring-ndcg-improvement">#</a></h3>
<p>Don&rsquo;t fly blind—benchmark your changes:</p>
<ol>
<li>Create a test set with queries and human-judged relevance scores</li>
<li>Calculate NDCG@k (typically k=5 or k=10) before and after changes</li>
<li>Aim for at least 5-10% lift in NDCG to justify implementation costs</li>
</ol>
<p><em>Pro Tip</em>: Let&rsquo;s do some LLM math that won&rsquo;t make your brain explode! Focus on NDCG@k based on your document size, because your poor LLM can only eat so many tokens before it gets a tummy ache.</p>
<p>Here&rsquo;s a real-world example with numbers so simple even your coffee-deprived morning brain can handle them:</p>
<ul>
<li>Your average document: 10,000 tokens (that&rsquo;s a chatty document!)</li>
<li>Your fancy GPT-4o: 128,000 token capacity (big brain energy!)</li>
<li>Your context + prompt: ~3,000 tokens (the appetizer)</li>
</ul>
<p>Now for the main course calculation:
10,000 tokens × 10 documents = 100,000 tokens
100,000 tokens + 3,000 tokens = 103,000 tokens</p>
<p>103,000 &lt; 128,000&hellip; We&rsquo;re good! 🎉</p>
<h2 id="conclusion-build-a-retrieval-flywheel">Conclusion: Build a Retrieval Flywheel<a hidden class="anchor" aria-hidden="true" href="#conclusion-build-a-retrieval-flywheel">#</a></h2>
<p>Here&rsquo;s the game plan:</p>
<ol>
<li><strong>Hybrid Search</strong>: Max out recall—grab everything.</li>
<li><strong>Re-Rankers</strong>: Sharpen precision—ditch the junk.</li>
<li><strong>Contextual Ranking</strong>: Make sure the gold is at the top.</li>
</ol>
<p>This isn&rsquo;t a one-and-done deal. It&rsquo;s a flywheel—every tweak spins it faster. Experiment with chunk sizes, thresholds, and models. Small wins stack up to massive gains.</p>
<p><strong>Final Tip</strong>: Don&rsquo;t guess—test. Try a 0.7 threshold vs. 0.9. Swap 256-token chunks for 512. Data beats dogma.</p>
<h2 id="retrieval-cheat-sheet">Retrieval Cheat Sheet<a hidden class="anchor" aria-hidden="true" href="#retrieval-cheat-sheet">#</a></h2>
<table>
<thead>
<tr>
<th>Step</th>
<th>Goal</th>
<th>Tactics</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. Recall</td>
<td>Grab everything</td>
<td>Query Expansion, Hybrid Search, Fine-Tuning, Chunking</td>
</tr>
<tr>
<td>2. Precision</td>
<td>Ditch the junk</td>
<td>Re-Rankers, Metadata Filters, Thresholds</td>
</tr>
<tr>
<td>3. NDCG</td>
<td>Perfect rankings</td>
<td>Reranking, User Feedback, Context</td>
</tr>
</tbody>
</table>
<p>That&rsquo;s it—your RAG retrieval is now a lean, mean, result-spitting machine. Go forth and dominate!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://dipkumar.dev/">Dip&#39;Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
