<!DOCTYPE html>
<html lang="en">

<head>
    
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-SHT0HV5ELQ"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());

            gtag('config', 'G-SHT0HV5ELQ');
        </script>
        


            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>
                Improving Retrieval in RAG (via Recall, Precision, and NDCG) | Dip&#39;s Blog
            </title>
            <meta name="description"
                content="A practical guide to improving retrieval in RAG systems by optimizing recall, precision, and NDCG">
            <meta property="og:title" content="Improving Retrieval in RAG (via Recall, Precision, and NDCG)" />
            <meta property="og:description"
                content="A practical guide to improving retrieval in RAG systems by optimizing recall, precision, and NDCG" />
            <meta property="og:type" content="article" />
            <meta property="og:image" content="https://dipkumar.dev/static/social-share.jpg" />
            <meta name="twitter:card" content="summary_large_image" />
            <meta name="twitter:image" content="https://dipkumar.dev/static/social-share.jpg" />

            <link rel="icon" type="image/jpeg" href="/static/icon.jpeg">
            <link rel="stylesheet" href="/static/style.css">
            <link rel="stylesheet" href="/static/fish.css">

            <!-- Prism.js for Syntax Highlighting -->
            <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css"
                rel="stylesheet" />

            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=STIX+Two+Text:wght@400;500;600;700&display=swap"
                rel="stylesheet">
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"
                integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA=="
                crossorigin="anonymous" referrerpolicy="no-referrer" />
</head>

<body>
    
    <div class="reading-progress" id="reading-progress"></div>
    
    <div class="container">
        
                            <nav class="post-nav">
                                <a href="/" class="back-link">Home</a>
                                <a href="/blogs" class="all-posts-link">All Posts</a>
                            </nav>
                            <article class="post-content">
                                <header>
                                    <h1>
                                        Improving Retrieval in RAG (via Recall, Precision, and NDCG)
                                    </h1>
                                    <div class="meta-row">
                                        <div class="author-meta">
                                            <span class="social-mini">
                                                <a href="https://x.com/immortaldip" target="_blank">ùïè</a>
                                                <a href="https://github.com/immortal3" target="_blank">github</a>
                                                <a href="mailto:patel@dipkumar.dev">email</a>
                                            </span>
                                            By <a href="/" class="author-link">Dipkumar Patel</a>
                                        </div>
                                        <div class="date-meta">
                                            March 8, 2025
                                            
                                            <span class="reading-time">9 min read</span>
                                            
                                        </div>
                                    </div>
                                </header>

                                
                                    <nav class="toc-container">
                                        <div class="toc-title">On this page</div>
                                        <ul class="toc-list">
                                            
                                                <li class="toc-item level-1">
                                                    <a href="#improving-retrieval-in-rag-via-recall-precision-and-ndcg">
                                                        Improving Retrieval in RAG (via Recall, Precision, and NDCG)
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-2">
                                                    <a href="#introduction">
                                                        Introduction
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-2">
                                                    <a href="#the-basics-of-retrieval">
                                                        The Basics of Retrieval
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-3">
                                                    <a href="#vector-search-vs-full-text-search">
                                                        Vector Search vs. Full-Text Search
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-2">
                                                    <a href="#metrics-101-what-to-optimize-for">
                                                        Metrics 101 ‚Äì What to Optimize For
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-3">
                                                    <a href="#the-hierarchy-of-needs">
                                                        The Hierarchy of Needs
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-2">
                                                    <a href="#step-1-maximizing-recall">
                                                        Step 1 ‚Äì Maximizing Recall
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-3">
                                                    <a href="#why-recall-first">
                                                        Why Recall First?
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-3">
                                                    <a href="#tactics-to-boost-recall">
                                                        Tactics to Boost Recall
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-2">
                                                    <a href="#step-2-precision-tuning">
                                                        Step 2 ‚Äì Precision Tuning
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-3">
                                                    <a href="#why-precision-matters">
                                                        Why Precision Matters
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-3">
                                                    <a href="#precision-boosting-strategies">
                                                        Precision-Boosting Strategies
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-2">
                                                    <a href="#step-3-ndcg-optimization">
                                                        Step 3 ‚Äì NDCG Optimization
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-3">
                                                    <a href="#why-ranking-matters">
                                                        Why Ranking Matters
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-3">
                                                    <a href="#ranking-improvement-strategies">
                                                        Ranking Improvement Strategies
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-3">
                                                    <a href="#measuring-ndcg-improvement">
                                                        Measuring NDCG Improvement
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-2">
                                                    <a href="#conclusion-build-a-retrieval-flywheel">
                                                        Conclusion: Build a Retrieval Flywheel
                                                    </a>
                                                </li>
                                                
                                                <li class="toc-item level-2">
                                                    <a href="#retrieval-cheat-sheet">
                                                        Retrieval Cheat Sheet
                                                    </a>
                                                </li>
                                                
                                        </ul>
                                    </nav>
                                    

                                        <div class="markdown-body">
                                            <h1 id="improving-retrieval-in-rag-via-recall-precision-and-ndcg">Improving Retrieval in RAG (via Recall, Precision, and NDCG)</h1><h2 id="introduction">Introduction</h2><p>Retrieval-Augmented Generation (RAG) is the superhero sidekick that grounds your Large Language Model (LLM) in cold, hard facts. But here&#39;s the dirty secret: if your retrieval sucks, your RAG system is just a fancy chatbot with a broken brain. Weak retrieval = missed documents, irrelevant results, and rankings that make no sense.</p>
<p>This guide cuts through the noise. You&#39;ll learn how to turbocharge your RAG retrieval with a no-fluff, step-by-step approach to maximize recall, sharpen precision, and nail NDCG. Whether you&#39;re a data scientist, developer, or AI enthusiast, this is your playbook to stop screwing around and start getting results. Let&#39;s roll.</p>
<h2 id="the-basics-of-retrieval">The Basics of Retrieval</h2><h3 id="vector-search-vs-full-text-search">Vector Search vs. Full-Text Search</h3><p>Retrieval is the backbone of RAG, and it&#39;s a tug-of-war between two heavyweights: vector search and full-text search. Here&#39;s the breakdown:</p>
<p><strong>Vector Search</strong>: Turns words into numbers (embeddings) to capture meaning. Think of it as a genius librarian who gets that &quot;machine learning frameworks&quot; is related to &quot;neural network libraries&quot; even if the exact words don&#39;t match.</p>
<p><em>Example</em>: Query = &quot;machine learning frameworks.&quot; Vector search grabs articles about &quot;PyTorch vs TensorFlow comparison&quot; because it understands semantic similarity.</p>
<p><strong>Full-Text Search</strong>: The old-school keyword matcher. It&#39;s like a librarian who only cares about exact titles‚Äîif &quot;machine learning frameworks&quot; isn&#39;t in the text, you&#39;re out of luck.</p>
<p><em>Example</em>: Same query, &quot;machine learning frameworks.&quot; Full-text search might miss that PyTorch article unless the phrase matches perfectly, but it&#39;ll snag anything with &quot;frameworks&quot; lightning-fast.</p>
<p>Here&#39;s a quick comparison:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Vector Search</th>
<th>Full-Text Search</th>
</tr>
</thead>
<tbody><tr>
<td>Strengths</td>
<td>Semantic understanding</td>
<td>Speed, exact matches</td>
</tr>
<tr>
<td>Weaknesses</td>
<td>Slower, resource-hungry</td>
<td>Misses context</td>
</tr>
<tr>
<td>Best For</td>
<td>Complex queries</td>
<td>Simple lookups</td>
</tr>
</tbody></table>
<p><strong>Why Both Matter</strong>: Hybrid search (vector + keywords) is the cheat code. Combine them, and you get the best of both worlds‚Äîbroad coverage with pinpoint accuracy.</p>
<h2 id="metrics-101-what-to-optimize-for">Metrics 101 ‚Äì What to Optimize For</h2><p>You can&#39;t fix what you don&#39;t measure. Here&#39;s your retrieval holy trinity:</p>
<p><strong>Recall</strong>: Are you finding all the good stuff?</p>
<p><em>Example</em>: Imagine 100 blog posts about &quot;transformer architecture&quot; exist. Your retriever grabs 85 of them. That&#39;s 85% recall. Miss too many, and your LLM is flying blind.</p>
<p><strong>Precision</strong>: Are you dodging the junk?</p>
<p><em>Example</em>: You retrieve 100 documents for &quot;transformer architecture,&quot; but only 70 are relevant (the rest are about &quot;electrical transformers&quot;). That&#39;s 70% precision. Too much noise, and your RAG drowns in garbage.</p>
<p><strong>NDCG</strong> (Normalized Discounted Cumulative Gain): Are the best hits at the top?</p>
<p><em>Example</em>: Picture the perfect ranking: top 5 results about transformer models are gold, next 5 are decent. If your retriever puts electrical engineering papers at #1 and buries the good ML content at #10, your NDCG tanks. High NDCG = happy users.</p>
<h3 id="the-hierarchy-of-needs">The Hierarchy of Needs</h3><ol>
<li><strong>Recall First</strong>: Cast a wide net‚Äîdon&#39;t miss the critical docs.</li>
<li><strong>Precision Next</strong>: Trim the fat‚Äîkeep only what&#39;s relevant.</li>
<li><strong>NDCG Last</strong>: Polish the rankings‚Äîput the best up top.</li>
</ol>
<h2 id="step-1-maximizing-recall">Step 1 ‚Äì Maximizing Recall</h2><h3 id="why-recall-first">Why Recall First?</h3><p>If your retriever misses key documents, your generator&#39;s toast. It&#39;s like cooking a steak dinner with no steak. Recall is step one‚Äîget everything on the table.</p>
<h3 id="tactics-to-boost-recall">Tactics to Boost Recall</h3><ol>
<li><p><strong>Query Expansion</strong>: Make your query a beast by adding synonyms or related terms.</p>
<p><em>Example</em>: Query = &quot;transformer models.&quot; Expand it to &quot;attention mechanisms,&quot; &quot;BERT architecture,&quot; &quot;language model design.&quot; </p>
<p><em>What to do</em>: </p>
<ul>
<li>Check out WordNet for traditional expansion</li>
<li>Use an LLM for contextual expansion or even re-writing to multiple different queries. In production, run all these expansions in parallel and merge results.</li>
</ul>
</li>
<li><p><strong>Hybrid Search</strong>: Merge vector and keyword results like a DJ mixing tracks. Use reciprocal rank fusion (1/rank) to blend the scores.</p>
<p><em>Example</em>: Query = &quot;transformer models.&quot; Vector search finds &quot;attention mechanism design,&quot; while full-text grabs &quot;BERT model implementations.&quot; Fusion ranks them smartly.</p>
<p><em>What to do</em>: </p>
<ul>
<li>Use a hybrid search engine like <a href="https://www.pinecone.io/learn/hybrid-search/">Pinecone</a>, <a href="https://qdrant.tech/articles/hybrid-search/">Qdrant</a>, or <a href="https://turbopuffer.com/docs/hybrid">TurboPuffer</a></li>
</ul>
</li>
<li><p><strong>Fine-Tune Embeddings</strong>: Generic embeddings suck for niche domains. Train on your data‚Äîsay, medical literature or financial reports‚Äîfor better matches.</p>
<p><em>Example</em>: Fine-tune on a dataset of ML research papers. Now &quot;transformer architecture&quot; queries snag &quot;multi-head attention mechanism&quot; docs too.</p>
<p><em>What to do</em>: </p>
<ul>
<li>Do it yourself: fine-tune <a href="https://huggingface.co/BAAI/bge-small-en">BAAI/bge-small</a> on your own data and benchmark it against current embeddings</li>
<li>Follow LlamaIndex&#39;s <a href="https://docs.llamaindex.ai/en/latest/examples/finetuning/embeddings/finetune_embedding/">guide on embedding fine-tuning</a></li>
<li>Take inspiration from Glean, which fine-tunes embeddings for each customer (<a href="https://www.youtube.com/watch?v=jTBsWJ2TKy8">Video</a>)</li>
</ul>
</li>
<li><p><strong>Chunking Strategy</strong>: Break documents into bite-sized pieces. Smaller chunks (e.g., 256 tokens) catch more, but overlap them (e.g., 50 tokens) to keep context.</p>
<p><em>Example</em>: An ML research paper on &quot;transformer models&quot; split into 500-token chunks might miss a key implementation detail. Shrink to 250 tokens with overlap, and you nab it.</p>
<p><em>Pro Tip</em>: </p>
<ul>
<li>Depending on your embedding model and domain, benchmark chunk size and overlap to find the best balance.</li>
</ul>
</li>
</ol>
<h2 id="step-2-precision-tuning">Step 2 ‚Äì Precision Tuning</h2><h3 id="why-precision-matters">Why Precision Matters</h3><p>You&#39;ve got a pile of docs‚Äînow ditch the trash. Precision ensures your RAG isn&#39;t wading through irrelevant sludge.</p>
<h3 id="precision-boosting-strategies">Precision-Boosting Strategies</h3><ol>
<li><p><strong>Re-Rankers</strong>: Run a heavy-hitter model (e.g., BERT cross-encoder) on your top 50-100 results to rescore them.</p>
<p><em>Example</em>: Query = &quot;transformer architecture.&quot; Initial retrieval grabs 100 docs, including some about &quot;electrical power transformers.&quot; A re-ranker kicks out the electrical engineering stuff, keeping ML architecture gold.</p>
<p><em>What to do</em>: </p>
<ul>
<li>Use Cohere&#39;s Rerank API, it&#39;s dead simple to integrate</li>
<li>For brave souls, try open-source options such as <a href="https://github.com/stanford-futuredata/ColBERT">ColBERT</a> and <a href="https://huggingface.co/BAAI/bge-reranker-base">BAAI/bge-reranker-base</a></li>
</ul>
</li>
<li><p><strong>Metadata Filtering</strong>: Use tags like date, category, or source to slice the fat.</p>
<p><em>Example</em>: Query = &quot;transformer models.&quot; Filter out docs older than 2020 or from non-ML domains‚Äîbam, instant precision boost.</p>
<p><em>What to do</em>: </p>
<ul>
<li>Implement with vector databases like Pinecone, TurboPuffer, or Qdrant that support metadata filtering</li>
</ul>
</li>
<li><p><strong>Thresholding</strong>: Set a similarity cutoff (e.g., cosine &gt; 0.5) to trash low-confidence matches.</p>
<p><em>Example</em>: Query = &quot;transformer architecture.&quot; Docs below 0.5 might be random electrical engineering content‚Äîdrop &#39;em and keep the signal.</p>
<p><em>What to do</em>: </p>
<ul>
<li>Configure similarity score thresholds in your vector database query APIs</li>
</ul>
</li>
</ol>
<h2 id="step-3-ndcg-optimization">Step 3 ‚Äì NDCG Optimization</h2><h3 id="why-ranking-matters">Why Ranking Matters</h3><p>You&#39;ve maximized recall and precision‚Äînow make sure the gold is at the top. With LLMs having finite token limits, the order of retrieval can make or break your RAG system. If your best content is buried at position #30, your LLM might never see it.</p>
<h3 id="ranking-improvement-strategies">Ranking Improvement Strategies</h3><ol>
<li><p><strong>Reranking</strong>: Use re-rankers to filter and re-rank your results. This helps to improve both precision and NDCG.</p>
</li>
<li><p><strong>User Feedback Integration</strong>: Capture what users actually find valuable and use it to improve your rankings.</p>
<p><em>Example</em>: Users consistently reference information from the third document in your RAG answers for &quot;transformer applications.&quot; Your system learns to boost similar documents higher for that query, dramatically improving NDCG.</p>
<p><em>What to do</em>:</p>
<ul>
<li><strong>Track interactions</strong>: Implement explicit feedback (thumbs up/down) and implicit signals (time spent, follow-up questions)</li>
<li><strong>Build feedback loops</strong>: Create a simple database that stores query-document pairs with user ratings</li>
<li><strong>Implement active learning</strong>: Prioritize collecting feedback on borderline documents where the system is uncertain</li>
<li><strong>Curate your corpus</strong>: Ruthlessly remove consistently low-rated documents from your vector database‚Äîthis is a game-changer that most teams overlook</li>
<li><strong>Apply immediate boosts</strong>: For frequent queries, manually boost documents with positive feedback by 1.2-1.5x in your ranking algorithm</li>
</ul>
<p><em>Pro Tip</em>: Don&#39;t wait for perfect data‚Äîstart with a simple &quot;Was this helpful?&quot; button after each RAG response, and you&#39;ll be shocked how quickly you can improve rankings with even sparse feedback.</p>
</li>
<li><p><strong>Context is King</strong>: Leverage conversation history to supercharge your retrieval relevance.</p>
<p><em>Example</em>: A user asks &quot;What are the best frameworks?&quot; after discussing PyTorch for 10 minutes. Without context, you might return generic framework docs. With context, you nail it with PyTorch-specific framework comparisons.</p>
<p><em>What to do</em>:</p>
<ul>
<li><strong>Store conversation history</strong>: Keep the last 3-5 exchanges in a context window</li>
<li><strong>Question rewriting</strong>: Use the history to expand ambiguous queries</li>
<li><strong>Context-aware filtering</strong>: Use topics from previous exchanges to filter metadata</li>
</ul>
<p><em>Pro Tip</em>: Don&#39;t just append history blindly‚Äîit creates noise. Instead, extract key entities and concepts from previous exchanges and use them to enrich your current query. For example, if discussing &quot;transformer models for NLP tasks,&quot; extract &quot;transformer&quot; + &quot;NLP&quot; as context boosters.</p>
</li>
</ol>
<h3 id="measuring-ndcg-improvement">Measuring NDCG Improvement</h3><p>Don&#39;t fly blind‚Äîbenchmark your changes:</p>
<ol>
<li>Create a test set with queries and human-judged relevance scores</li>
<li>Calculate NDCG@k (typically k=5 or k=10) before and after changes</li>
<li>Aim for at least 5-10% lift in NDCG to justify implementation costs</li>
</ol>
<p><em>Pro Tip</em>: Let&#39;s do some LLM math that won&#39;t make your brain explode! Focus on NDCG@k based on your document size, because your poor LLM can only eat so many tokens before it gets a tummy ache.</p>
<p>Here&#39;s a real-world example with numbers so simple even your coffee-deprived morning brain can handle them:</p>
<ul>
<li>Your average document: 10,000 tokens (that&#39;s a chatty document!)</li>
<li>Your fancy GPT-4o: 128,000 token capacity (big brain energy!)</li>
<li>Your context + prompt: ~3,000 tokens (the appetizer)</li>
</ul>
<p>Now for the main course calculation:
10,000 tokens √ó 10 documents = 100,000 tokens
100,000 tokens + 3,000 tokens = 103,000 tokens</p>
<p>103,000 &lt; 128,000... We&#39;re good! üéâ</p>
<h2 id="conclusion-build-a-retrieval-flywheel">Conclusion: Build a Retrieval Flywheel</h2><p>Here&#39;s the game plan:</p>
<ol>
<li><strong>Hybrid Search</strong>: Max out recall‚Äîgrab everything.</li>
<li><strong>Re-Rankers</strong>: Sharpen precision‚Äîditch the junk.</li>
<li><strong>Contextual Ranking</strong>: Make sure the gold is at the top.</li>
</ol>
<p>This isn&#39;t a one-and-done deal. It&#39;s a flywheel‚Äîevery tweak spins it faster. Experiment with chunk sizes, thresholds, and models. Small wins stack up to massive gains.</p>
<p><strong>Final Tip</strong>: Don&#39;t guess‚Äîtest. Try a 0.7 threshold vs. 0.9. Swap 256-token chunks for 512. Data beats dogma.</p>
<h2 id="retrieval-cheat-sheet">Retrieval Cheat Sheet</h2><table>
<thead>
<tr>
<th>Step</th>
<th>Goal</th>
<th>Tactics</th>
</tr>
</thead>
<tbody><tr>
<td>1. Recall</td>
<td>Grab everything</td>
<td>Query Expansion, Hybrid Search, Fine-Tuning, Chunking</td>
</tr>
<tr>
<td>2. Precision</td>
<td>Ditch the junk</td>
<td>Re-Rankers, Metadata Filters, Thresholds</td>
</tr>
<tr>
<td>3. NDCG</td>
<td>Perfect rankings</td>
<td>Reranking, User Feedback, Context</td>
</tr>
</tbody></table>
<p>That&#39;s it‚Äîyour RAG retrieval is now a lean, mean, result-spitting machine. Go forth and dominate!</p>

                                        </div>

                                        
                                            <div class="post-footer-tags">
                                                
                                                    <a href="/tags/RAG">#RAG</a>
                                                    
                                            </div>
                                            
                            </article>
                            
    </div>

    <!-- Prism JS for Syntax Highlighting -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure autoloader path
        Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';

        // Highlight on load
        document.addEventListener('DOMContentLoaded', function () {
            Prism.highlightAll();
        });

        // Mermaid initialization
        if (document.querySelector('.language-mermaid')) {
            const script = document.createElement('script');
            script.src = 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js';
            script.onload = () => {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default',
                    flowchart: { useMaxWidth: true, htmlLabels: true, curve: 'basis' },
                    securityLevel: 'loose',
                });
                // Replace mermaid code blocks with div.mermaid
                document.querySelectorAll('pre code.language-mermaid').forEach(el => {
                    const pre = el.parentElement;
                    const div = document.createElement('div');
                    div.className = 'mermaid';
                    div.textContent = el.textContent;
                    pre.parentElement.replaceChild(div, pre);
                });
                mermaid.init(undefined, '.mermaid');
            };
            document.head.appendChild(script);
        }

        // Reading progress bar
        
        (function() {
            const progressBar = document.getElementById('reading-progress');
            const article = document.querySelector('.markdown-body');
            if (progressBar && article) {
                window.addEventListener('scroll', function() {
                    const articleRect = article.getBoundingClientRect();
                    const articleTop = articleRect.top + window.scrollY;
                    const articleHeight = article.offsetHeight;
                    const windowHeight = window.innerHeight;
                    const scrollTop = window.scrollY;

                    // Calculate progress based on how much of the article has been scrolled past
                    const progress = Math.min(100, Math.max(0,
                        ((scrollTop - articleTop + windowHeight * 0.3) / articleHeight) * 100
                    ));
                    progressBar.style.width = progress + '%';
                });
            }
        })();
        

        
    </script>
</body>

</html>