<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
        Instruction Aware Embeddings | Dip&#39;s Blog
    </title>
    <meta name="description"
        content="Why Your Retriever is Failing and How Context Can Save It - Instruction Aware Embeddings">
    <meta property="og:title" content="Instruction Aware Embeddings" />
    <meta property="og:description"
        content="Why Your Retriever is Failing and How Context Can Save It - Instruction Aware Embeddings" />
    <meta property="og:type" content="article" />
    <meta property="og:image" content="https://dipkumar.dev/static/social-share.jpg" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:image" content="https://dipkumar.dev/static/social-share.jpg" />

    <link rel="icon" type="image/jpeg" href="/static/icon.jpeg">
    <link rel="stylesheet" href="/static/style.css">
    <link rel="stylesheet" href="/static/fish.css">

    <!-- Prism.js for Syntax Highlighting -->
    <link href="https://unpkg.com/prismjs@1.29.0/themes/prism.css" rel="stylesheet" />

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"
        integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
</head>

<body>
    <div class="container">
        
                        <nav class="post-nav">
                            <a href="/">‚Üê Home</a>
                            <a href="/blogs" class="all-posts-link">All Posts</a>
                        </nav>
                        <article class="post-content">
                            <header>
                                <h1>
                                    Instruction Aware Embeddings
                                </h1>
                                <div class="meta-row">
                                    <div class="author-meta">
                                        <span class="social-mini">
                                            <a href="https://x.com/immortaldip" target="_blank">ùïè</a>
                                            <a href="https://github.com/immortal3" target="_blank">github</a>
                                            <a href="mailto:patel@dipkumar.dev">email</a>
                                        </span>
                                        By <a href="/" class="author-link">Dipkumar Patel</a>
                                    </div>
                                    <div class="date-meta">
                                        July 8, 2025
                                    </div>
                                </div>
                            </header>
                            <div class="markdown-body">
                                <h1>Why Your Retriever is Failing and How Context Can Save It</h1>
<p>Imagine asking &quot;I want to buy apple&quot; ‚Äì do you mean Apple Inc. stock, the latest iPhone, or simply fruit? Without context, your retriever may serve you the wrong results.</p>
<hr>
<h2>1. What Is the Problem in Your Retriever &amp; Embedding?</h2>
<p>Modern retrievers map queries and documents into high-dimensional vectors (embeddings) and rank by cosine similarity. But when a query is <strong>ambiguous</strong>, plain embeddings struggle:  </p>
<ul>
<li>They collapse multiple meanings of &quot;apple&quot; into one vector.  </li>
<li>The top results can mix stock guides, product pages, and nutrition articles.</li>
</ul>
<p>You might think this is a hypothetical scenario that rarely occurs in practice. However, here&#39;s a real-world example from Google Deep Research that illustrates the issue:</p>
<pre><code class="language-text">Query: &quot;We want to create a simple presentation on MCP server. We want to discuss why it&#39;s needed, current limitations, and potential use cases.

We also want to highlight its technical challenges.

Let&#39;s write a concise presentation for this.&quot;
</code></pre>
<p><img src="/blog_photos/context-gemini.png" alt="image"></p>
<p>It returned information about &quot;Unisys ClearPath MCP&quot; rather than the intended &quot;Model Control Protocol (MCP)&quot; proposed by Anthropic. This real-world misalignment underscores how context-less embeddings can derail retrieval.  </p>
<hr>
<h2>2. Missing Context in Embedding</h2>
<p>Embeddings encode <strong>semantic similarity</strong> but lack task or intent signals. Out of the box, they answer the question:  </p>
<blockquote>
<p>&quot;Which documents <em>sound</em> most like this query?&quot;  </p>
</blockquote>
<p>They don&#39;t know if &quot;apple&quot; refers to finance, technology, or groceries‚Äîso they return a blend of all.</p>
<hr>
<h2>3. How Does It Work Without Context?</h2>
<p>Using script&#39;s results (see <a href="https://gist.github.com/immortal3/6af71b0f9be87489d13a7e0f2cf68120">gist</a>), here&#39;s the <strong>plain embedding</strong> behavior for &quot;I want to buy apple&quot; with OpenAI and Qwen models:</p>
<pre><code class="language-text">üìù Query: &#39;I want to buy apple&#39;
üîç Using plain query (no instruction)

ü§ñ OpenAI Model Results:
1. How to Buy Apple Stock   (Score: 0.536)
2. Where to Buy Apples      (Score: 0.497)
3. iPhone 15 Pro Purchase   (Score: 0.455)

ü§ñ Qwen Model Results:
1. Where to Buy Apples       (Score: 0.604)
2. How to Buy Apple Stock    (Score: 0.594)
3. Health Benefits of Apples (Score: 0.501)
</code></pre>
<p>Both embeddings mix stock, fruit, and product topics. The <strong>Qwen</strong> model edges out OpenAI by a small margin, but neither is decisively focused.</p>
<h2>4. Introducing Qwen &amp; Replicating the Same Thing in OpenAI</h2>
<p>The <a href="https://qwenlm.github.io/blog/qwen3-embedding/">Qwen3-Embedding-8B model</a> is <strong>instruction-aware</strong>, trained to accept task descriptions alongside queries. When we add a &quot;grocery shopping&quot; instruction:</p>
<pre><code class="language-python"># Minimal instruction-aware query construction
instruction = &quot;Given a grocery shopping question, retrieve fruit purchase information&quot;
query = &quot;I want to buy apple&quot;
instructed_query = f&quot;Instruction: {instruction}\nQuery: {query}&quot;
</code></pre>
<p><img src="/blog_photos/context-qwen.png" alt="image"></p>
<hr>
<p><strong>Visualizing the Flow:</strong></p>
<pre><code class="language-text">User Query: &quot;I want to buy apple&quot;
        |
        v
[Plain Embedding Model]
        |
        v
Results: [Stock guides, iPhones, fruit articles]  &lt;-- Mixed, ambiguous

User Query + Instruction: &quot;Given a grocery shopping question, retrieve fruit purchase information\nI want to buy apple&quot;
        |
        v
[Instruction-Aware Embedding Model]
        |
        v
Results: [Fruit purchase guides, grocery info]  &lt;-- Focused, relevant
</code></pre>
<hr>
<h3>Focused Scenario Performance Gains</h3>
<p>Below is a comparison of similarity scores for the <strong>correct document</strong> in each use case, showing how instruction-aware embeddings shift the focus within the same model. 
Note, OpenAI does not support instruction-aware embeddings yet, but we tried to run the same instruction-aware query with OpenAI&#39;s embedding model. As you can see, it did not work very well and it&#39;s clear, instruction-aware embeddings need to be supported by the model and it&#39;s not just a matter of adding a prefix to the query.</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Model</th>
<th>Plain Score</th>
<th>Instruction Score</th>
<th>Œî Score</th>
</tr>
</thead>
<tbody><tr>
<td>Financial (Stock Purchase)</td>
<td>OpenAI</td>
<td>0.536</td>
<td>0.472</td>
<td>‚àí0.064</td>
</tr>
<tr>
<td>Financial (Stock Purchase)</td>
<td>Qwen</td>
<td>0.594</td>
<td>0.743</td>
<td>+0.149</td>
</tr>
<tr>
<td>Technology (iPhone Purchase)</td>
<td>OpenAI</td>
<td>0.455</td>
<td>0.393</td>
<td>‚àí0.062</td>
</tr>
<tr>
<td>Technology (iPhone Purchase)</td>
<td>Qwen</td>
<td><em>&lt;0.501</em></td>
<td>0.512</td>
<td><strong>‚Üë</strong></td>
</tr>
<tr>
<td>Grocery (Fruit Purchase)</td>
<td>OpenAI</td>
<td>0.497</td>
<td>0.502</td>
<td>+0.005</td>
</tr>
<tr>
<td>Grocery (Fruit Purchase)</td>
<td>Qwen</td>
<td>0.604</td>
<td>0.680</td>
<td>+0.076</td>
</tr>
</tbody></table>
<p><em>Note:</em> Qwen did not surface the iPhone doc in its top-3 plain results (score &lt;0.501), yet it rises to #2 (0.512) with instruction.</p>
<p><strong>What does this mean?</strong><br>Notice how Qwen&#39;s instruction-aware mode dramatically increases the relevance score for the correct document, while OpenAI&#39;s model barely changes or even drops. This demonstrates that simply adding instructions to the query only works if the model is trained to use them.</p>
<hr>
<h2>5. Alternative: Query Rewriting</h2>
<p>Embeddings also benefit when the query itself carries the necessary context. Instead of relying solely on instruction-aware models, you can rewrite the user&#39;s query using chat history or domain knowledge to inject focus. For example:</p>
<ul>
<li><strong>Original Query:</strong> &quot;I want to buy apple&quot;</li>
<li><strong>Rewritten Query:</strong> &quot;Where can I buy fresh apples at my local grocery store?&quot;</li>
</ul>
<p>Such rewrites embed context directly into the text, allowing plain embedding models to retrieve the correct documents (fruit vendors, grocery guides) without specialized instructions. This technique can be automated via:</p>
<ul>
<li>A chat interface that remembers previous messages and reformulates queries.</li>
<li>A domain-specific rewriter that maps generic queries to more precise, vocabulary-rich versions.</li>
</ul>
<p>By combining query rewriting with embeddings, you get the best of both worlds: minimal model changes and focused retrieval.</p>
<hr>
<h2>6. What You Can Do About It</h2>
<p>Facing ambiguous queries? You have four straightforward strategies:</p>
<ol>
<li><p><strong>Instruction-aware embeddings</strong></p>
<ul>
<li>Use models like Qwen3-Embedding-8B that accept contextual instructions.</li>
<li>Best for: New projects or high-priority use cases.</li>
<li>Trade-offs: Requires switching your embedding provider.</li>
</ul>
</li>
<li><p><strong>Query rewriting</strong></p>
<ul>
<li>Rewrite queries to inject context (e.g., &quot;Where can I buy fresh organic apples?&quot;).</li>
<li>Best for: Legacy systems or teams using plain embedding models.</li>
<li>Trade-offs: Requires building and maintaining rewriting logic.</li>
</ul>
</li>
<li><p><strong>Hybrid approach</strong></p>
<ul>
<li>Combine query rewriting for immediate gains with instruction-aware models for future migrations.</li>
<li>Best for: Teams seeking a phased adoption strategy.</li>
<li>Trade-offs: More complex workflow but balances risk and reward.</li>
</ul>
</li>
<li><p><strong>Ask clarifying questions</strong></p>
<ul>
<li>Detect vague or ambiguous queries and prompt the user for more details before retrieving.</li>
<li>Best for: Interactive search interfaces and chatbots.</li>
<li>Trade-offs: Requires a conversational UI and may add extra steps to user interactions.</li>
</ul>
</li>
</ol>
<p>Choose the strategy that fits your team&#39;s resources and goals, and start by tackling your most ambiguous queries first.</p>
<hr>
<h2>7. Closing Thoughts</h2>
<ul>
<li><strong>Missing context</strong> in embeddings is the core challenge for ambiguous queries.  </li>
<li><strong>Instruction-aware embeddings</strong> (like Qwen3-Embedding-8B) deliver <em>stronger</em> task focus, dramatically improving top-ranked results.  </li>
<li>You can mimic this in OpenAI by manually adding instructions, but specialized models yield bigger gains.</li>
</ul>
<p><strong>What should you do next?</strong>  </p>
<ul>
<li>Audit your current retrieval system for ambiguous queries.</li>
<li>Experiment with instruction-aware models if available.</li>
<li>Implement query rewriting where needed to improve retrieval focus.</li>
</ul>
<p>Embrace instruction-aware retrieval to resolve ambiguity and serve exactly what users intend‚Äîevery time.  </p>
<hr>
<p><em>References:</em>  </p>
<ul>
<li>Qwen3 Embedding model card: <a href="https://qwenlm.github.io/blog/qwen3-embedding/">Hugging Face</a>  </li>
<li>Code example and full script: <a href="https://gist.github.com/immortal3/6af71b0f9be87489d13a7e0f2cf68120">compare.py on GitHub Gist</a></li>
</ul>

                            </div>

                            
                                <div class="post-footer-tags">
                                    
                                        <a href="/tags/RAG">#RAG</a>
                                        
                                        <a href="/tags/embedding">#embedding</a>
                                        
                                </div>
                                
                        </article>
                        
    </div>

    <!-- Prism JS for Syntax Highlighting -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure autoloader path
        Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';

        // Highlight on load
        document.addEventListener('DOMContentLoaded', function () {
            Prism.highlightAll();
        });

        
    </script>
</body>

</html>