<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Llm on Dip&#39;Blog</title>
    <link>https://dipkumar.dev/tags/llm/</link>
    <description>Recent content in Llm on Dip&#39;Blog</description>
    <image>
      <title>Dip&#39;Blog</title>
      <url>https://dipkumar.dev/images/papermod-cover.png</url>
      <link>https://dipkumar.dev/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.129.0</generator>
    <language>en</language>
    <lastBuildDate>Wed, 15 Oct 2025 22:59:08 +0530</lastBuildDate>
    <atom:link href="https://dipkumar.dev/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bits-per-Byte (BPB): a tokenizer-agnostic way to measure LLMs</title>
      <link>https://dipkumar.dev/posts/llm/bits-per-byte/</link>
      <pubDate>Wed, 15 Oct 2025 22:59:08 +0530</pubDate>
      <guid>https://dipkumar.dev/posts/llm/bits-per-byte/</guid>
      <description>Karpathy recently released nanochat repo which cotains code for training the best ChatGPT under $100. While skimming the high level code, I noticed across bits per bytes instead of typical cross entropy loss. And, i found it interesting, so i decided to dig in.
TL;DR Bit per byte (BPB) is just cross-entropy measured per byte. We divide cross-entropy by bytes and log(2) to convert to bits. Because itâ€™s per byte, BPB is tokenizer-agnostic and lets you compare models fairly even when they use different vocabularies and rules.</description>
    </item>
    <item>
      <title>GPT-5 Router - Inevitable Future of Chat Interfaces</title>
      <link>https://dipkumar.dev/posts/llm/gpt5-router/</link>
      <pubDate>Wed, 13 Aug 2025 20:48:22 +0530</pubDate>
      <guid>https://dipkumar.dev/posts/llm/gpt5-router/</guid>
      <description>Why OpenAI&amp;#39;s GPT-5 router is inevitable: understanding the cost squeeze driving automatic model selection and what it means for users.</description>
    </item>
    <item>
      <title>AWS BedRock - Converse API - A single endpoint for all models ?</title>
      <link>https://dipkumar.dev/posts/aws-bedrock/</link>
      <pubDate>Thu, 13 Jun 2024 23:15:53 +0530</pubDate>
      <guid>https://dipkumar.dev/posts/aws-bedrock/</guid>
      <description>Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.</description>
    </item>
  </channel>
</rss>
