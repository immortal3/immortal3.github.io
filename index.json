[{"content":"Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. [1]\nAWS BedRock\u0026rsquo;s Converse API is a single endpoint that allows you to chat with any model. Indeed, the single endpoint is, I believe, the best feature of AWS BedRock. Let\u0026rsquo;s visit this endpoint and see how it works.\nmodel_id = \u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34; inference_config = {\u0026#34;temperature\u0026#34;: 0.5} additional_model_fields = {\u0026#34;top_k\u0026#34;: 200} # Send the message. response = bedrock_client.converse( modelId=model_id, messages=messages, system=system_prompts, inferenceConfig=inference_config, additionalModelRequestFields=additional_model_fields ) By changing model_id, you can switch between different models.\nI think AWS BedRock should have used the same standards as OpenAI\u0026rsquo;s client rather than creating their own But, hey, it\u0026rsquo;s still a single endpoint. right ?\u0026hellip; I should be just able to switch models by changing model_id. right ?\u0026hellip;.\nHidden Gotcha of Converse API Not every model is available AWS BedRock has LLama3, Anthropic Claude, Mistral and their own Titan. But, It doesn\u0026rsquo;t have OpenAI models like GPT-4/GPT-4o. This might not be a deal breaker, depending on what you are trying to achieve. You can check the availability of models in AWS Bedrock Models\nNot every model has system prompt, or multi-modality support If you check converse API parameters, you will see that there is a parameter called system. This parameter is used to provide system prompt to the model. However, not every model supports system prompts. (Because they were not trained with system prompts). If you\u0026rsquo;re switching between models via code using ENV/Flags/Config, you might need to handle edge cases where a system prompt is unavailable for the given modelId. Otherwise, It will throw an Exception. (Ideally, i think it should just give a warning) AWS has a nice table to check if given model has system prompt.\nThe same goes for multi-modality. If your messages include images, switching between models might not be straightforward.\nNot every model has same context window I mean this is on you, but again good reminder.\nAdvance Prompt technique like Prefilling Assistant Message # code copied from https://eugeneyan.com//writing/prompting/#prefill-claudes-responses input = \u0026#34;\u0026#34;\u0026#34; \u0026lt;description\u0026gt; The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99. At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice or app—no matter where you place it in your home. This affordable little hub brings convenient hands-free control to your smart devices. \u0026lt;/description\u0026gt; Extract the \u0026lt;name\u0026gt;, \u0026lt;size\u0026gt;, \u0026lt;price\u0026gt;, and \u0026lt;color\u0026gt; from this product \u0026lt;description\u0026gt;. Return the extracted attributes within \u0026lt;attributes\u0026gt;. \u0026#34;\u0026#34;\u0026#34; messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: input, }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;attributes\u0026gt;\u0026lt;name\u0026gt;\u0026#34; # Prefilled response } ] # raise error_class(parsed_response, operation_name) # botocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the Converse # operation: The model that you are using requires the last turn in the conversation to be a user message. Add a # user message to the conversation and try again. If you\u0026rsquo;re using advanced prompting techniques, such as Prefilling Assistant Messages [3], where you pre-populate the message with text designated as \u0026lsquo;assistant\u0026rsquo;, you need to be cautious when switching between models. Not all models are compatible with this technique and their is validation check which will raise exception.\nSo, Overall, We are still far away from having a unified API for all models. I will update this article if i find anything new.\nReferences [1] https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html\n[2] https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features\n[3] https://eugeneyan.com//writing/prompting/#prefill-claudes-responses\n","permalink":"https://dipkumar.dev/posts/aws-bedrock/","summary":"Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.","title":"AWS BedRock - Converse API - A single endpoint for all models ?"},{"content":"Essential Fields Be it relational or not, every table should have these 5 fields:\ncreated_at (default now()) updated_at (default now()) deleted_at (default null) created_by (not null) updated_by (not null) Just to be clear, every table should have these 5 fields and not must. Adding these fields have other side-effects such as bloat, performance and disk size. But, if you\u0026rsquo;re having these problems, i hope you\u0026rsquo;re profitable.\nWhy should you include this fields ? Auditability Incorporating these fields into every table significantly simplifies the auditing process. They enable you to track who created or modified an entry and when these actions occurred. It\u0026rsquo;s important to note that while this doesn\u0026rsquo;t provide a complete audit trail, not all tables require exhaustive audit trails. These fields deliver sufficient oversight for many applications.\nSoft Delete Capability Utilizing the deleted_at field for soft deletions boosts data recovery and error correction capabilities, enabling businesses to effortlessly restore mistakenly deleted data or perform historical data analysis without relying on intricate backup systems. Additionally, you can set up a cron job to transfer data to an archive table periodically. For instance, you might move all data marked as deleted over three months ago to cold storage. This strategy helps maintain manageable table sizes by systematically archiving older records.\nRow Level Security/Permissions (RLS) These fields might seem superfluous at first, but they are incredibly useful for controlling user access to specific rows within a table. For instance, you may want to prevent a user from updating a row that was created by someone else. By using these fields, you can define such permissions clearly and effectively. Furthermore, they enable more nuanced scenarios—for example, allowing a user to restore a deleted row only if they were the original creator, while still permitting any user to delete a row. This level of detailed control ensures both data integrity and adherence to specified access protocols.\nAvoiding Nightmares: A Cautionary Tale Imagine you\u0026rsquo;ve deployed a cron job in the background designed to update certain attributes in your table based on specific business logic. It ran flawlessly during the staging tests, so you pushed it to production without further validation. But then, disaster strikes: the script modifies incorrect data. Fortunately, the updated_at and updated_by fields can come to your rescue (though not always). To identify the affected data, you can execute a query like:\nSELECT * FROM items WHERE updated_at BETWEEN {script_begin} AND {script_end} AND updated_by = {script_user}; This allows you to pinpoint the exact entries altered during the time the script ran, providing a straightforward way to assess and rectify the unintended changes. This is a prime example of how such fields can help mitigate potential disasters, helping you manage crises more effectively.\nORM: Django if you\u0026rsquo;re using some framework for accessing db like ORM in your codebase, it becomes easier to add these fields to your tables and helper queries. For example, I am showcasing you how to add these fields in django (python).\n1. Create mixin class from django.db import models from django.utils import timezone from django.conf import settings class AuditFieldsMixin(models.Model): created_at = models.DateTimeField(auto_now_add=True) updated_at = models.DateTimeField(auto_now=True) deleted_at = models.DateTimeField(null=True, blank=True) created_by = models.ForeignKey(settings.AUTH_USER_MODEL, related_name=\u0026#34;%(class)s_created_by\u0026#34;, on_delete=models.SET_NULL, null=True) updated_by = models.ForeignKey(settings.AUTH_USER_MODEL, related_name=\u0026#34;%(class)s_updated_by\u0026#34;, on_delete=models.SET_NULL, null=True) class Meta: abstract = True def soft_delete(self): self.deleted_at = timezone.now() self.save() What’s going on here? We’re defining fields that automatically capture when and by whom a record was created or updated. Plus, we threw in a soft_delete method for good measure, so you can \u0026ldquo;delete\u0026rdquo; records without actually losing them.\nSlap the Mixin on a Model Using this mixin is as easy as pie. Just inherit from AuditFieldsMixin in your model:\nclass Item(AuditFieldsMixin): name = models.CharField(max_length=255) description = models.TextField() price = models.DecimalField(max_digits=5, decimal_places=2) # Imagine there are other fields here too! 2. QuerySets That Ignore Deleted Stuff You don\u0026rsquo;t want your default queries pulling up deleted records, right? Let’s fix that by tweaking the model’s manager to ignore anything that’s been soft-deleted:\nclass AuditQuerySet(models.QuerySet): def active(self): return self.filter(deleted_at__isnull=True) def deleted(self): return self.filter(deleted_at__isnull=False) class AuditManager(models.Manager): def get_queryset(self): return AuditQuerySet(self.model, using=self._db).active() class Item(AuditFieldsMixin): objects = AuditManager() all_objects = models.Manager() # This lets you access ALL records, even the \u0026#34;deleted\u0026#34; ones name = models.CharField(max_length=255) description = models.TextField() price = models.DecimalField(max_digits=5, decimal_places=2) # More fields, potentially Conclusion Why do you need conclusion ? This is ain\u0026rsquo;t generated by GPT. I am just a human being trying to help you.\nIf you have any past expirences of getting saved by some random fields, please let me know. I would be happy to learn.\nSend me an email at pate@ + dipkumar.dev\n","permalink":"https://dipkumar.dev/posts/essential-db-design-1/","summary":"Essential Fields Be it relational or not, every table should have these 5 fields:\ncreated_at (default now()) updated_at (default now()) deleted_at (default null) created_by (not null) updated_by (not null) Just to be clear, every table should have these 5 fields and not must. Adding these fields have other side-effects such as bloat, performance and disk size. But, if you\u0026rsquo;re having these problems, i hope you\u0026rsquo;re profitable.\nWhy should you include this fields ?","title":"Essential Database Design: Five Fields Every Table Must Have"},{"content":"Work-in Progress ","permalink":"https://dipkumar.dev/about/","summary":"Work-in Progress ","title":"About Me"},{"content":"The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it. Before you read further, the blog assumes you have background on transformers; if you don\u0026rsquo;t, then read this blog post. It’s awesome, and you will learn a lot from it.\nFirst, let’s understand a few things about GPT code.\ndef gpt(inputs: list[int]) -\u0026gt; list[list[float]]: # inputs has shape [n_seq] # output has shape [n_seq, n_vocab] output = # beep boop neural network magic return output We can deduce from the input-output signature that we can provide arbitrary long input and receive output of the same length, with each element of the output indicating the probability of the next token. So, I can just give a single token as input and get the probability of next token. It should just work, right ?\nModifying the code of picoGPT to just give the input of the last single token and get the probability of the next token.\nfor _ in tqdm(range(n_tokens_to_generate), \u0026#34;generating\u0026#34;): # auto-regressive decode loop logits = gpt2(inputs[-1:], **params, n_head=n_head) # model forward pass next_id = np.argmax(logits[-1]) # greedy sampling inputs = np.append(inputs, [next_id]) # append prediction to input We are providing inputs[-1:] as input (single token) to the model. So, we are just passing a single token as input. Let\u0026rsquo;s see what happens.\nthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the I didn’t work. Because the main magic is in the attention, in order to have good prediction of next tokens we need to provide all previous tokens. Although in practice, we do have limited memory and compute which forces us to provide context upto last N tokens. for example, chagpt has context upto 4096. In summary, We can’t just pass a single token and get very good prediction of next token. This makes attention have quadratic complexity.\nBut, if we look at the architecture of GPT, we can see that we only interact with previous tokens in the attention block, all other layers, such as the embedding layer, the feed forward layer, the layer norm, etc., don’t care about previous tokens. So, what if we can cache the input of the attention block for all previous tokens and pass it during inference? We don’t have to pass all these tokens again and again. We can just pass the last token and get the probability of the next token.\nThe input of the attention block is q, k, v and mask. We can try to cache q, k, and v for all previous tokens. But, let’s think about what really matters for us. We only need k and v of the previous tokens to perform attention on the current input token because we are only passing one token as input. See the image below for a visual representation of what I mean.\ndef attention(q, k, v, mask): # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -\u0026gt; [n_q, d_v] return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v So, we need to calculate new_k and new_v for current input token. Append it to the existing cache and pass it to attention block for further processing.\ndef mha(x, c_attn, c_proj, n_head, kvcache=None): # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] # qkv projection # when we pass kvcache, n_seq = 1. so we will compute new_q, new_k and new_v x = linear(x, **c_attn) # [n_seq, n_embd] -\u0026gt; [n_seq, 3*n_embd] # split into qkv qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -\u0026gt; [3, n_seq, n_embd] if kvcache: # qkv new_q, new_k, new_v = qkv # new_q, new_k, new_v = [1, n_embd] old_k, old_v = kvcache k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1 v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1 qkv = [new_q, k, v] There is one more thing we need to take care of is causal mask. When we pass single token we would like it to attend to all previous tokens.\n# causal mask to hide future inputs from being attended to if kvcache: # when we pass kvcache, we are passing single token as input which need to attend to all previous tokens, so we create mask with all 0s causal_mask = np.zeros((1, k.shape[0])) else: # create triangular causal mask causal_mask = (1 - np.tri(x.shape[0])) * -1e10 # [n_seq, n_seq] Combining all the things together, we get the following code.\ndef mha(x, c_attn, c_proj, n_head, kvcache=None): # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] # qkv projection # n_seq = 1 when we pass kvcache, so we will compute new_q, new_k and new_v x = linear(x, **c_attn) # [n_seq, n_embd] -\u0026gt; [n_seq, 3*n_embd] # split into qkv qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -\u0026gt; [3, n_seq, n_embd] if kvcache: # qkv new_q, new_k, new_v = qkv # new_q, new_k, new_v = [1, n_embd] old_k, old_v = kvcache k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1 v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1 qkv = [new_q, k, v] current_cache = [qkv[1], qkv[2]] # split into heads qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv)) # [3, n_seq, n_embd] -\u0026gt; [n_head, 3, n_seq, n_embd/n_head] # causal mask to hide future inputs from being attended to if kvcache: causal_mask = np.zeros((1, k.shape[0])) else: causal_mask = (1 - np.tri(x.shape[0])) * -1e10 # [n_seq, n_seq] # perform attention over each head out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)] # [n_head, 3, n_seq, n_embd/n_head] -\u0026gt; [n_head, n_seq, n_embd/n_head] # merge heads x = np.hstack(out_heads) # [n_head, n_seq, n_embd/n_head] -\u0026gt; [n_seq, n_embd] # out projection x = linear(x, **c_proj) # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] return x, current_cache We introduced minor breaking changes in output as well. We are introducing current_cache alongside our normal output. This is because we can use an updated cache for the next run.\nWe also need to change a few functions to make it work.\ndef transformer_block(x, mlp, attn, ln_1, ln_2, n_head, kvcache=None): # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] # multi-head causal self attention attn_out, kvcache_updated = mha(layer_norm(x, **ln_1), **attn, n_head=n_head, kvcache=kvcache) x = x + attn_out # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] # position-wise feed forward network x = x + ffn(layer_norm(x, **ln_2), **mlp) # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] return x, kvcache_updated We added kvcache as an input to the function and returned kvcache_updated as an output for each transformer block. We also need to change transformer function.\ndef gpt2(inputs, wte, wpe, blocks, ln_f, n_head, kvcache = None): # [n_seq] -\u0026gt; [n_seq, n_vocab] if not kvcache: kvcache = [None]*len(blocks) wpe_out = wpe[range(len(inputs))] else: # cache already available, only send last token as input for predicting next token wpe_out = wpe[[len(inputs)-1]] inputs = [inputs[-1]] # token + positional embeddings x = wte[inputs] + wpe_out # [n_seq] -\u0026gt; [n_seq, n_embd] # forward pass through n_layer transformer blocks new_kvcache = [] for block, kvcache_block in zip(blocks, kvcache): x, updated_cache = transformer_block(x, **block, n_head=n_head, kvcache=kvcache_block) # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] new_kvcache.append(updated_cache) # TODO: inplace extend new cache instead of re-saving whole # projection to vocab x = layer_norm(x, **ln_f) # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] return x @ wte.T, new_kvcache # [n_seq, n_embd] -\u0026gt; [n_seq, n_vocab] Notice, When we have already compute kvcache, we only return input last token to GPT2 alongside with kvcache. You can also see len(kvcache) == # number of transformer blocks. This is because we need to update kvcache for attention and we have single attention in each transformer block.\nAnd, finally, it\u0026rsquo;s time to change our generate function to use cache. In the first iteration, we will not have kvcache and we will pass kvcache=None to gpt2 function. In subsequent iterations, we will utilise the previously generated kvcache.\nkvcache = None for _ in tqdm(range(n_tokens_to_generate), \u0026#34;generating\u0026#34;): # auto-regressive decode loop logits, kvcache = gpt2(inputs, **params, n_head=n_head, kvcache=kvcache) # model forward pass next_id = np.argmax(logits[-1]) # greedy sampling inputs = np.append(inputs, [next_id]) # append prediction to input This cache helps us to reduce computation for each iteration. We can see that, in first iteration, we are computing attention for all tokens in input. But, in subsequent iterations, we are only computing attention for last token. Reducing time complexity from O(n^2) to O(n).\nFinally, we can verify generate text with our previous code which didn\u0026rsquo;t have caching and compare two output. Both output should be same.\nIn terminal\n\u0026gt;\u0026gt;\u0026gt; python gpt2_kvcache.py \u0026#34;Alan Turing theorized that computers would one day become\u0026#34; Output: the most powerful machines on the planet. The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain. You can see the all the code in this pull request. You can also see the code in this repository.\nYou can see more details of calculation related to kv cache memory footprint calculation and computation time in this blog post.\nReferences:\nhttps://kipp.ly/blog/transformer-inference-arithmetic/ https://lilianweng.github.io/posts/2023-01-10-inference-optimization/ https://jaykmody.com/blog/gpt-from-scratch/ ","permalink":"https://dipkumar.dev/posts/gpt-kvcache/","summary":"The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it.","title":"Speeding up the GPT - KV cache"},{"content":"Lec-2 spatial demension\n","permalink":"https://dipkumar.dev/notes/efficient-ml/","summary":"Lec-2 spatial demension","title":"Efficient ML"},{"content":"Biweekly-66 (27th Nov, 2021) 2085. Count Common Words With One Occurrence Hint 1\rUse hashmap (Counter)\r2086. Minimum Number of Buckets Required to Collect Rainwater from Houses\u0026quot; Hint 1\rFirst put the bucket at best place and the remove those covering home.\rHint 2\rAnswer is (best bucket cnt + remaining house).\rHint 3\rCorner case: check for each house is coverable\r2087. Minimum Cost Homecoming of a Robot in a Grid Hint\rdjikstra will fail. why ?\rHint\rToo many cells to cover (10**10). Think of something else\rHint\rTo reach home, which path you need to take ? (cost is non-negative)\rHint\rTo reach home, number of rows and number of cols changes are fixed.\r2088. Count Fertile Pyramids in a Land Hint\rDeconstruct pyramid into smaller part and then think to calculate how many pyramids are there\rHint\rwe can calculate left and right perpendiculars and then construct pyramids from them\rHint\rcalculate for normal and flipped version of grid\rWeekly-269 (28th Nov, 2021) 2089. Find Target Indices After Sorting Array Hint\rImplementation\r2090. K Radius Subarray Averages Hint\rPrefix sum\r2091. Removing Minimum and Maximum From Array Hint\rGreedy cases to minimize number of remove\rHint\rmin(r+1, n-l, l+1+(n-r)). here l and r are index of max and min elements (l \u0026lt; r).\r2092. Find All People With Secret Hint\rsort by time and try to share secret\rHint\rat current timestamp, find connected components and color all nodes if one of them have seen secret\r","permalink":"https://dipkumar.dev/posts/leetcode-contest/","summary":"Biweekly-66 (27th Nov, 2021) 2085. Count Common Words With One Occurrence Hint 1\rUse hashmap (Counter)\r2086. Minimum Number of Buckets Required to Collect Rainwater from Houses\u0026quot; Hint 1\rFirst put the bucket at best place and the remove those covering home.\rHint 2\rAnswer is (best bucket cnt + remaining house).\rHint 3\rCorner case: check for each house is coverable\r2087. Minimum Cost Homecoming of a Robot in a Grid Hint\rdjikstra will fail.","title":"LC contest problems summary"},{"content":"run local server hugo server -D Create New Post hugo new content/posts/{post-name}.md Hugo build/export the site hugo -d ../becoming-the-unbeatable relative imports example: static\\icons\\favicon.png relative imports: icons\\favicon.png\nfix for label image icon: small_icon.jpg instead of icon: small_icon.png\ngithub issue: https://github.com/adityatelange/hugo-PaperMod/issues/622\n","permalink":"https://dipkumar.dev/posts/hugo-cmds/","summary":"run local server hugo server -D Create New Post hugo new content/posts/{post-name}.md Hugo build/export the site hugo -d ../becoming-the-unbeatable relative imports example: static\\icons\\favicon.png relative imports: icons\\favicon.png\nfix for label image icon: small_icon.jpg instead of icon: small_icon.png\ngithub issue: https://github.com/adityatelange/hugo-PaperMod/issues/622","title":"Hugo commands"}]