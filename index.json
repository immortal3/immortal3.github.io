[{"content":" Karpathy recently released nanochat repo which cotains code for training the best ChatGPT under $100. While skimming the high level code, I noticed across bits per bytes instead of typical cross entropy loss. And, i found it interesting, so i decided to dig in.\nTL;DR Bit per byte (BPB) is just cross-entropy measured per byte. We divide cross-entropy by log(2) to convert to bits. Because it’s per byte, BPB is tokenizer-agnostic and lets you compare models fairly even when they use different vocabularies and rules. Perplexity and token-level loss change when you change the tokenizer; BPB largely doesn’t. LLM doesn\u0026rsquo;t predict the text, it predicts the (next) token. But token definitions depend on the tokenizer (BPE, Unigram, merges, special tokens, etc.). Swap tokenizers and the same sentence can become more or fewer tokens. So per-token metrics (avg CE, perplexity) change even if the underlying modeling quality didn’t.\nSome popular tokenizer choices are:\nModel Tokenizer Vocab Size GPT-4 cl100k_base (BPE) 100,256 LLaMA 3 TikToken (BPE) 128,000 Gemini 2.5 SentencePiece (Unigram) 256,000 Claude closed-source undisclosed Different tokenizers ≠ comparable \u0026ldquo;tokens\u0026rdquo;. So a model that uses a coarser tokenizer (fewer, longer tokens) can appear to have a lower per-token loss or perplexity, simply because the denominator changed.\nInstead of normalizing loss per token, normalize per byte of UTF-8 text that those tokens represent. Then, no matter how you split words into tokens, you\u0026rsquo;re still asking: how many bits, on average, does the model need to encode each byte of text?\nExample: Why Per-Token Metrics Mislead Consider two models predicting \u0026ldquo;The Capital of India\u0026rdquo; -\u0026gt; \u0026quot; is Delhi\u0026quot; (8 bytes in UTF-8, including the space):\nModel A (coarse tokenizer):\nTokens: [\u0026quot; is\u0026quot;, \u0026quot; Delhi\u0026quot;] (2 tokens) Per-token loss: [1.5, 4.5] nats Total loss: 6.0 nats Model B (fine-grained tokenizer):\nTokens: [\u0026quot; is\u0026quot;, \u0026quot; Del\u0026quot;, \u0026quot;hi\u0026quot;] (3 tokens) Per-token loss: [1.5, 2.0, 2.5] nats Total loss: 6.0 nats Per-token metrics (misleading):\nModel A avg loss: 6.0 / 2 = 3.0 nats/token\rModel B avg loss: 6.0 / 3 = 2.0 nats/token ← appears better!\rModel A perplexity: exp(3.0) = 20.09\rModel B perplexity: exp(2.0) = 7.39 ← appears better! Model B looks significantly better, but it\u0026rsquo;s the same 6.0 nats spread over more tokens.\nBits-per-byte (fair comparison):\nModel A BPB: 6.0 / (ln(2) × 8) = 1.08 bits/byte\rModel B BPB: 6.0 / (ln(2) × 8) = 1.08 bits/byte ← identical! BPB correctly shows both models have the same predictive quality. The apparent \u0026ldquo;improvement\u0026rdquo; in Model B\u0026rsquo;s per-token metrics was purely an artifact of tokenization granularity.\nImplementation Below is the simplified and more readable version of the original code.\nimport math import torch import torch.distributed as dist @torch.no_grad() def evaluate_bpb(model, batches, steps: int, token_bytes: torch.Tensor) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Compute Bits-Per-Byte (BPB) over `steps` batches. Shapes (your mental model): B = batch size Seq = sequence length V = vocab size Inputs: - model: callable like model(x, y, loss_reduction=\u0026#39;none\u0026#39;) -\u0026gt; loss per token. Expects: x: (B, Seq) token ids (int64) y: (B, Seq) target token ids (int64), may contain ignore_index (\u0026lt;0) Returns: loss2d: (B, Seq) per-token loss in NATs (float32/float16) - batches: iterable yielding (x, y) as above. - steps: number of batches to evaluate. - token_bytes: (V,) int64 — byte length of each token id; 0 for special tokens (those should not count toward BPB). Notes: - BPB = (sum of losses in NATs over *counted* tokens) / (ln(2) * total_counted_bytes) - Tokens contribute to the denominator by their byte length; tokens with 0 bytes (specials) and ignored targets (\u0026lt;0) are excluded from both numerator \u0026amp; denominator. \u0026#34;\u0026#34;\u0026#34; device = model.get_device() if hasattr(model, \u0026#34;get_device\u0026#34;) else next(model.parameters()).device # Accumulators across steps (and later across ranks) sum_nats = torch.tensor(0.0, dtype=torch.float32, device=device) # scalar sum_bytes = torch.tensor(0, dtype=torch.int64, device=device) # scalar token_bytes = token_bytes.to(device=device, dtype=torch.int64) # (V,) batch_iter = iter(batches) for _ in range(steps): x, y = next(batch_iter) # x: (B, Seq), y: (B, Seq) x = x.to(device) y = y.to(device) loss2d = model(x, y, loss_reduction=\u0026#39;none\u0026#39;) # (B, Seq) NATs loss1d = loss2d.reshape(-1) # (B*Seq,) y1d = y.reshape(-1) # (B*Seq,) if (y1d \u0026lt; 0).any(): # Mask out ignore_index (\u0026lt;0) before indexing into token_bytes valid = (y1d \u0026gt;= 0) # (B*Seq,) ysafe = torch.where(valid, y1d, torch.zeros_like(y1d)) # (B*Seq,) nb = torch.where(valid, token_bytes[ysafe], torch.zeros_like(y1d)) # (B*Seq,) int64 else: nb = token_bytes[y1d] # (B*Seq,) int64 # Count only tokens with positive byte length counted = (nb \u0026gt; 0) # (B*Seq,) bool sum_nats += (loss1d[counted]).sum() # scalar sum_bytes += nb[counted].sum() # scalar int64 # Distributed sum over all ranks, if initialized if dist.is_initialized() and dist.get_world_size() \u0026gt; 1: dist.all_reduce(sum_nats, op=dist.ReduceOp.SUM) dist.all_reduce(sum_bytes, op=dist.ReduceOp.SUM) total_nats = float(sum_nats.item()) total_bytes = int(sum_bytes.item()) # Guard against division by zero (e.g., all tokens were special/ignored) if total_bytes == 0: return float(\u0026#34;nan\u0026#34;) bpb = total_nats / (math.log(2.0) * total_bytes) return bpb ","permalink":"https://dipkumar.dev/posts/llm/bits-per-byte/","summary":"Karpathy recently released nanochat repo which cotains code for training the best ChatGPT under $100. While skimming the high level code, I noticed across bits per bytes instead of typical cross entropy loss. And, i found it interesting, so i decided to dig in.\nTL;DR Bit per byte (BPB) is just cross-entropy measured per byte. We divide cross-entropy by log(2) to convert to bits. Because it’s per byte, BPB is tokenizer-agnostic and lets you compare models fairly even when they use different vocabularies and rules.","title":"Bits-per-Byte (BPB): a tokenizer-agnostic way to measure LLMs"},{"content":"Creativity is a luxury.\nIt demands time, energy and space: things that feel scarce when rent, groceries, and the next shift loom larger than any poem or prototype. Most of us are caught in a slow-spinning loop of laundry, commutes, and alarms that reset before the dream has even ended.\nIt is also a luxury that needs literal room: a quiet corner, a desk that isn’t the dinner table, a door that closes. I have watched friends whose bedrooms double as storage closets carry old laptops to 24-hour cafés or office lobbies after hours, hunting for any pocket of stillness where code can compile without a toddler tugging at the charger.\nMuch of the talk about “why they don’t innovate” is aimed at developing nations, as if ingenuity were a switch we forgot to flip. The question forgets that for many, the next level of the game called life is simply surviving this week.\nCreativity requires time, the one thing handed out in identical seconds but lived in wildly unequal ways. A developer on bug-fix cycles measures the day in thirty-minute bites between Jira pings; a CTO can clear a whole afternoon to whiteboard a new microservice. Same 24 hours, but one calendar is packed with other people\u0026rsquo;s priorities, the other guarded so ideas can breathe.\nYet even in the squeeze, support engineers still refactor a memory leak during the last hour of their shift, and interns build open-source yet another caching library on hostel Wi-Fi at 2 a.m. Moral richness doesn\u0026rsquo;t wait for perfect conditions; it grows in cracks, stubborn and green, proving that the luxury of creativity is one humans keep insisting on, even when the price feels impossibly high.\nDisclaimer\rThis writing was assisted by an LLM.\n","permalink":"https://dipkumar.dev/posts/life/creativity-is-a-luxury/","summary":"Creativity is a luxury.\nIt demands time, energy and space: things that feel scarce when rent, groceries, and the next shift loom larger than any poem or prototype. Most of us are caught in a slow-spinning loop of laundry, commutes, and alarms that reset before the dream has even ended.\nIt is also a luxury that needs literal room: a quiet corner, a desk that isn’t the dinner table, a door that closes.","title":"Creativity Is a Luxury"},{"content":"OpenAI GPT-5 Router is like Apple removing headphone jack.\nIt sucks but everyone will follow it.\n\u0026mdash; immortal (@immortal_0698) August 14, 2025 What is GPT-5 Router The GPT-5 router picks the right model for each request in real time. In plain English: easy stuff goes to the small model; complex stuff goes to the big brain. The goal is simple, better answers per dollar and millisecond by mixing models instead of forcing a single static choice. I suspect router will be a key component in subscription pricing.\nHow It Works: Routing as Classification Problem Understanding the router means treating it like a classifier. For example, you have two models: a smaller, no-reasoning model and a larger, reasoning model. Given a user query, the router has to make a call:\nSmaller model: when the query is simple Larger model: when the query is complex In reality, we have more models, but for simplicity, we will stick to two models.\nThe Classification Matrix A compact way to reason about this: a confusion matrix. To keep score, call the positive class \u0026ldquo;complex\u0026rdquo; and the negative class \u0026ldquo;simple\u0026rdquo;. Rows are the router\u0026rsquo;s decision; columns are the true difficulty of user query.\nActual Difficulty: Simple Actual Difficulty: Complex Route: Smaller True Negative (TN) False Negative (FN) Route: Larger False Positive (FP) True Positive (TP) We don\u0026rsquo;t have to worry about the diagonal elements, as they are the cases where the router is correct. But we need to worry about the off-diagonal elements : False Positive and False Negative.\nError Analysis: Both Mistakes Cost Money False Negative (Complex → Smaller): The worst outcome\nBreaks user experience - they get a shallow answer to a deep question Damages trust and perceived quality Users complain, cancel subscriptions, bad reviews Cost: Customer churn and reputation damage False Positive (Simple → Larger): The expensive mistake\nUser gets a great answer but you burn unnecessary compute $0.05 query becomes a $0.60 query (12x cost) At scale, this adds up fast - 10,000 false positives = $5,500 in wasted compute Cost: Direct margin erosion So the strategy becomes: bias toward false positives (overspend on compute) rather than false negatives (lose customers). You can optimize compute costs later, but you can\u0026rsquo;t win back a user who thinks your AI is \u0026ldquo;dumber than your previous model.\u0026rdquo;\nThis is why OpenAI initially erred on the side of caution with the router, then faced backlash when the pendulum swung too far toward false negatives. The sweet spot is narrow and expensive to find.\nEconomic Motivation: The Subscription Squeeze This technical complexity of router exists because OpenAI faces a challenging economic reality: flat subscription pricing becomes difficult when usage explodes exponentially. As per Sam Altman, even $200/month struggles to maintain profitability.\ninsane thing: we are currently losing money on openai pro subscriptions!\npeople use it much more than we expected.\n\u0026mdash; Sam Altman (@sama) January 6, 2025 Math Behind the Subscription Pricing Here\u0026rsquo;s the math behind the subscription pricing:\nUsers pay $20/month for supposedly \u0026ldquo;unlimited\u0026rdquo; access (Nothing is unlimited) But Big models can burn upto $0.5+ per query in compute costs (Reasoning models) Deep research runs cost ~$1+ each and take 20+ minutes Other features such as memory, tools, etc. are not free. It\u0026rsquo;s not just OpenAI - other companies are facing similar challenges:\nAnthropic - Their $20/month subscription includes significant rate limiting. Cursor - They recently announced that after 250 Sonnet requests, they\u0026rsquo;ll meter usage and charge based on consumption Routers are going to get better Creating a good router is fundamentally a data problem, and OpenAI has a massive advantage here. Every query-response pair becomes training data for router improvement:\nData Collection at Scale:\nMillions of daily interactions across different complexity levels User feedback signals (thumbs up/down, follow-up questions) Engagement metrics (time spent reading, follow-up queries) Cost-per-query data for model optimization Iterative Improvement Loop:\nRouter misroutes a complex query → user complains or asks follow-up OpenAI labels this as \u0026ldquo;should have gone to reasoning model\u0026rdquo; Router learns: similar queries get routed to larger model next time Over time, accuracy improves from 80% → 90% → 95%+ The GPT-5 Launch Backlash When OpenAI launched GPT-5 with mandatory routing, users immediately complained about quality degradation. The router was routing too many complex queries to the smaller model, making GPT-5 seem \u0026ldquo;dumber\u0026rdquo; than GPT-4o.\nUser Backlash:\nUsers reported shallow answers to complex prompts Reddit filled with complaints about the perceived downgrade Loss of manual model selection frustrated paid subscribers OpenAI\u0026rsquo;s Response:\nBrought back GPT-4o access for Plus users Acknowledged router problems and began tuning improvements Added more transparency about which model responds Conclusion / Prediction The router will come back - but better trained. OpenAI learned that accuracy matters more than cost savings for user satisfaction. Expect:\nHigher-tier customers: Will likely get manual model selection options Free/basic tiers: Will live with the router, but a much-improved version Industry trend: Other AI companies will adopt similar routing strategies as costs mount The economics make routers inevitable, but OpenAI\u0026rsquo;s rough launch showed that execution quality determines success or failure.\n","permalink":"https://dipkumar.dev/posts/llm/gpt5-router/","summary":"OpenAI GPT-5 Router is like Apple removing headphone jack.\nIt sucks but everyone will follow it.\n\u0026mdash; immortal (@immortal_0698) August 14, 2025 What is GPT-5 Router The GPT-5 router picks the right model for each request in real time. In plain English: easy stuff goes to the small model; complex stuff goes to the big brain. The goal is simple, better answers per dollar and millisecond by mixing models instead of forcing a single static choice.","title":"GPT-5 Router - Inevitable Future of Chat Interfaces"},{"content":"Why Your Retriever is Failing and How Context Can Save It Imagine asking \u0026ldquo;I want to buy apple\u0026rdquo; – do you mean Apple Inc. stock, the latest iPhone, or simply fruit? Without context, your retriever may serve you the wrong results.\n1. What Is the Problem in Your Retriever \u0026amp; Embedding? Modern retrievers map queries and documents into high-dimensional vectors (embeddings) and rank by cosine similarity. But when a query is ambiguous, plain embeddings struggle:\nThey collapse multiple meanings of \u0026ldquo;apple\u0026rdquo; into one vector. The top results can mix stock guides, product pages, and nutrition articles. You might think this is a hypothetical scenario that rarely occurs in practice. However, here\u0026rsquo;s a real-world example from Google Deep Research that illustrates the issue:\nQuery: \u0026#34;We want to create a simple presentation on MCP server. We want to discuss why it\u0026#39;s needed, current limitations, and potential use cases. We also want to highlight its technical challenges. Let\u0026#39;s write a concise presentation for this.\u0026#34; It returned information about \u0026ldquo;Unisys ClearPath MCP\u0026rdquo; rather than the intended \u0026ldquo;Model Control Protocol (MCP)\u0026rdquo; proposed by Anthropic. This real-world misalignment underscores how context-less embeddings can derail retrieval.\n2. Missing Context in Embedding Embeddings encode semantic similarity but lack task or intent signals. Out of the box, they answer the question:\n\u0026ldquo;Which documents sound most like this query?\u0026rdquo;\nThey don\u0026rsquo;t know if \u0026ldquo;apple\u0026rdquo; refers to finance, technology, or groceries—so they return a blend of all.\n3. How Does It Work Without Context? Using script\u0026rsquo;s results (see gist), here\u0026rsquo;s the plain embedding behavior for \u0026ldquo;I want to buy apple\u0026rdquo; with OpenAI and Qwen models:\n📝 Query: \u0026#39;I want to buy apple\u0026#39; 🔍 Using plain query (no instruction) 🤖 OpenAI Model Results: 1. How to Buy Apple Stock (Score: 0.536) 2. Where to Buy Apples (Score: 0.497) 3. iPhone 15 Pro Purchase (Score: 0.455) 🤖 Qwen Model Results: 1. Where to Buy Apples (Score: 0.604) 2. How to Buy Apple Stock (Score: 0.594) 3. Health Benefits of Apples (Score: 0.501) Both embeddings mix stock, fruit, and product topics. The Qwen model edges out OpenAI by a small margin, but neither is decisively focused.\n4. Introducing Qwen \u0026amp; Replicating the Same Thing in OpenAI The Qwen3-Embedding-8B model is instruction-aware, trained to accept task descriptions alongside queries. When we add a \u0026ldquo;grocery shopping\u0026rdquo; instruction:\n# Minimal instruction-aware query construction instruction = \u0026#34;Given a grocery shopping question, retrieve fruit purchase information\u0026#34; query = \u0026#34;I want to buy apple\u0026#34; instructed_query = f\u0026#34;Instruction: {instruction}\\nQuery: {query}\u0026#34; Visualizing the Flow:\nUser Query: \u0026#34;I want to buy apple\u0026#34; | v [Plain Embedding Model] | v Results: [Stock guides, iPhones, fruit articles] \u0026lt;-- Mixed, ambiguous User Query + Instruction: \u0026#34;Given a grocery shopping question, retrieve fruit purchase information\\nI want to buy apple\u0026#34; | v [Instruction-Aware Embedding Model] | v Results: [Fruit purchase guides, grocery info] \u0026lt;-- Focused, relevant Focused Scenario Performance Gains Below is a comparison of similarity scores for the correct document in each use case, showing how instruction-aware embeddings shift the focus within the same model. Note, OpenAI does not support instruction-aware embeddings yet, but we tried to run the same instruction-aware query with OpenAI\u0026rsquo;s embedding model. As you can see, it did not work very well and it\u0026rsquo;s clear, instruction-aware embeddings need to be supported by the model and it\u0026rsquo;s not just a matter of adding a prefix to the query.\nScenario Model Plain Score Instruction Score Δ Score Financial (Stock Purchase) OpenAI 0.536 0.472 −0.064 Financial (Stock Purchase) Qwen 0.594 0.743 +0.149 Technology (iPhone Purchase) OpenAI 0.455 0.393 −0.062 Technology (iPhone Purchase) Qwen \u0026lt;0.501 0.512 ↑ Grocery (Fruit Purchase) OpenAI 0.497 0.502 +0.005 Grocery (Fruit Purchase) Qwen 0.604 0.680 +0.076 Note: Qwen did not surface the iPhone doc in its top-3 plain results (score \u0026lt;0.501), yet it rises to #2 (0.512) with instruction.\nWhat does this mean?\nNotice how Qwen\u0026rsquo;s instruction-aware mode dramatically increases the relevance score for the correct document, while OpenAI\u0026rsquo;s model barely changes or even drops. This demonstrates that simply adding instructions to the query only works if the model is trained to use them.\n5. Alternative: Query Rewriting Embeddings also benefit when the query itself carries the necessary context. Instead of relying solely on instruction-aware models, you can rewrite the user\u0026rsquo;s query using chat history or domain knowledge to inject focus. For example:\nOriginal Query: \u0026ldquo;I want to buy apple\u0026rdquo; Rewritten Query: \u0026ldquo;Where can I buy fresh apples at my local grocery store?\u0026rdquo; Such rewrites embed context directly into the text, allowing plain embedding models to retrieve the correct documents (fruit vendors, grocery guides) without specialized instructions. This technique can be automated via:\nA chat interface that remembers previous messages and reformulates queries. A domain-specific rewriter that maps generic queries to more precise, vocabulary-rich versions. By combining query rewriting with embeddings, you get the best of both worlds: minimal model changes and focused retrieval.\n6. What You Can Do About It Facing ambiguous queries? You have four straightforward strategies:\nInstruction-aware embeddings\nUse models like Qwen3-Embedding-8B that accept contextual instructions. Best for: New projects or high-priority use cases. Trade-offs: Requires switching your embedding provider. Query rewriting\nRewrite queries to inject context (e.g., \u0026ldquo;Where can I buy fresh organic apples?\u0026rdquo;). Best for: Legacy systems or teams using plain embedding models. Trade-offs: Requires building and maintaining rewriting logic. Hybrid approach\nCombine query rewriting for immediate gains with instruction-aware models for future migrations. Best for: Teams seeking a phased adoption strategy. Trade-offs: More complex workflow but balances risk and reward. Ask clarifying questions\nDetect vague or ambiguous queries and prompt the user for more details before retrieving. Best for: Interactive search interfaces and chatbots. Trade-offs: Requires a conversational UI and may add extra steps to user interactions. Choose the strategy that fits your team\u0026rsquo;s resources and goals, and start by tackling your most ambiguous queries first.\n7. Closing Thoughts Missing context in embeddings is the core challenge for ambiguous queries. Instruction-aware embeddings (like Qwen3-Embedding-8B) deliver stronger task focus, dramatically improving top-ranked results. You can mimic this in OpenAI by manually adding instructions, but specialized models yield bigger gains. What should you do next?\nAudit your current retrieval system for ambiguous queries. Experiment with instruction-aware models if available. Implement query rewriting where needed to improve retrieval focus. Embrace instruction-aware retrieval to resolve ambiguity and serve exactly what users intend—every time.\nReferences:\nQwen3 Embedding model card: Hugging Face Code example and full script: compare.py on GitHub Gist ","permalink":"https://dipkumar.dev/posts/rag/instruction-aware-embeddings/","summary":"Why Your Retriever is Failing and How Context Can Save It Imagine asking \u0026ldquo;I want to buy apple\u0026rdquo; – do you mean Apple Inc. stock, the latest iPhone, or simply fruit? Without context, your retriever may serve you the wrong results.\n1. What Is the Problem in Your Retriever \u0026amp; Embedding? Modern retrievers map queries and documents into high-dimensional vectors (embeddings) and rank by cosine similarity. But when a query is ambiguous, plain embeddings struggle:","title":"Instruction Aware Embeddings"},{"content":"Improving Retrieval in RAG (via Recall, Precision, and NDCG) Introduction Retrieval-Augmented Generation (RAG) is the superhero sidekick that grounds your Large Language Model (LLM) in cold, hard facts. But here\u0026rsquo;s the dirty secret: if your retrieval sucks, your RAG system is just a fancy chatbot with a broken brain. Weak retrieval = missed documents, irrelevant results, and rankings that make no sense.\nThis guide cuts through the noise. You\u0026rsquo;ll learn how to turbocharge your RAG retrieval with a no-fluff, step-by-step approach to maximize recall, sharpen precision, and nail NDCG. Whether you\u0026rsquo;re a data scientist, developer, or AI enthusiast, this is your playbook to stop screwing around and start getting results. Let\u0026rsquo;s roll.\nThe Basics of Retrieval Vector Search vs. Full-Text Search Retrieval is the backbone of RAG, and it\u0026rsquo;s a tug-of-war between two heavyweights: vector search and full-text search. Here\u0026rsquo;s the breakdown:\nVector Search: Turns words into numbers (embeddings) to capture meaning. Think of it as a genius librarian who gets that \u0026ldquo;machine learning frameworks\u0026rdquo; is related to \u0026ldquo;neural network libraries\u0026rdquo; even if the exact words don\u0026rsquo;t match.\nExample: Query = \u0026ldquo;machine learning frameworks.\u0026rdquo; Vector search grabs articles about \u0026ldquo;PyTorch vs TensorFlow comparison\u0026rdquo; because it understands semantic similarity.\nFull-Text Search: The old-school keyword matcher. It\u0026rsquo;s like a librarian who only cares about exact titles—if \u0026ldquo;machine learning frameworks\u0026rdquo; isn\u0026rsquo;t in the text, you\u0026rsquo;re out of luck.\nExample: Same query, \u0026ldquo;machine learning frameworks.\u0026rdquo; Full-text search might miss that PyTorch article unless the phrase matches perfectly, but it\u0026rsquo;ll snag anything with \u0026ldquo;frameworks\u0026rdquo; lightning-fast.\nHere\u0026rsquo;s a quick comparison:\nFeature Vector Search Full-Text Search Strengths Semantic understanding Speed, exact matches Weaknesses Slower, resource-hungry Misses context Best For Complex queries Simple lookups Why Both Matter: Hybrid search (vector + keywords) is the cheat code. Combine them, and you get the best of both worlds—broad coverage with pinpoint accuracy.\nMetrics 101 – What to Optimize For You can\u0026rsquo;t fix what you don\u0026rsquo;t measure. Here\u0026rsquo;s your retrieval holy trinity:\nRecall: Are you finding all the good stuff?\nExample: Imagine 100 blog posts about \u0026ldquo;transformer architecture\u0026rdquo; exist. Your retriever grabs 85 of them. That\u0026rsquo;s 85% recall. Miss too many, and your LLM is flying blind.\nPrecision: Are you dodging the junk?\nExample: You retrieve 100 documents for \u0026ldquo;transformer architecture,\u0026rdquo; but only 70 are relevant (the rest are about \u0026ldquo;electrical transformers\u0026rdquo;). That\u0026rsquo;s 70% precision. Too much noise, and your RAG drowns in garbage.\nNDCG (Normalized Discounted Cumulative Gain): Are the best hits at the top?\nExample: Picture the perfect ranking: top 5 results about transformer models are gold, next 5 are decent. If your retriever puts electrical engineering papers at #1 and buries the good ML content at #10, your NDCG tanks. High NDCG = happy users.\nThe Hierarchy of Needs Recall First: Cast a wide net—don\u0026rsquo;t miss the critical docs. Precision Next: Trim the fat—keep only what\u0026rsquo;s relevant. NDCG Last: Polish the rankings—put the best up top. Step 1 – Maximizing Recall Why Recall First? If your retriever misses key documents, your generator\u0026rsquo;s toast. It\u0026rsquo;s like cooking a steak dinner with no steak. Recall is step one—get everything on the table.\nTactics to Boost Recall Query Expansion: Make your query a beast by adding synonyms or related terms.\nExample: Query = \u0026ldquo;transformer models.\u0026rdquo; Expand it to \u0026ldquo;attention mechanisms,\u0026rdquo; \u0026ldquo;BERT architecture,\u0026rdquo; \u0026ldquo;language model design.\u0026rdquo;\nWhat to do:\nCheck out WordNet for traditional expansion Use an LLM for contextual expansion or even re-writing to multiple different queries. In production, run all these expansions in parallel and merge results. Hybrid Search: Merge vector and keyword results like a DJ mixing tracks. Use reciprocal rank fusion (1/rank) to blend the scores.\nExample: Query = \u0026ldquo;transformer models.\u0026rdquo; Vector search finds \u0026ldquo;attention mechanism design,\u0026rdquo; while full-text grabs \u0026ldquo;BERT model implementations.\u0026rdquo; Fusion ranks them smartly.\nWhat to do:\nUse a hybrid search engine like Pinecone, Qdrant, or TurboPuffer Fine-Tune Embeddings: Generic embeddings suck for niche domains. Train on your data—say, medical literature or financial reports—for better matches.\nExample: Fine-tune on a dataset of ML research papers. Now \u0026ldquo;transformer architecture\u0026rdquo; queries snag \u0026ldquo;multi-head attention mechanism\u0026rdquo; docs too.\nWhat to do:\nDo it yourself: fine-tune BAAI/bge-small on your own data and benchmark it against current embeddings Follow LlamaIndex\u0026rsquo;s guide on embedding fine-tuning Take inspiration from Glean, which fine-tunes embeddings for each customer (Video) Chunking Strategy: Break documents into bite-sized pieces. Smaller chunks (e.g., 256 tokens) catch more, but overlap them (e.g., 50 tokens) to keep context.\nExample: An ML research paper on \u0026ldquo;transformer models\u0026rdquo; split into 500-token chunks might miss a key implementation detail. Shrink to 250 tokens with overlap, and you nab it.\nPro Tip:\nDepending on your embedding model and domain, benchmark chunk size and overlap to find the best balance. Step 2 – Precision Tuning Why Precision Matters You\u0026rsquo;ve got a pile of docs—now ditch the trash. Precision ensures your RAG isn\u0026rsquo;t wading through irrelevant sludge.\nPrecision-Boosting Strategies Re-Rankers: Run a heavy-hitter model (e.g., BERT cross-encoder) on your top 50-100 results to rescore them.\nExample: Query = \u0026ldquo;transformer architecture.\u0026rdquo; Initial retrieval grabs 100 docs, including some about \u0026ldquo;electrical power transformers.\u0026rdquo; A re-ranker kicks out the electrical engineering stuff, keeping ML architecture gold.\nWhat to do:\nUse Cohere\u0026rsquo;s Rerank API, it\u0026rsquo;s dead simple to integrate For brave souls, try open-source options such as ColBERT and BAAI/bge-reranker-base Metadata Filtering: Use tags like date, category, or source to slice the fat.\nExample: Query = \u0026ldquo;transformer models.\u0026rdquo; Filter out docs older than 2020 or from non-ML domains—bam, instant precision boost.\nWhat to do:\nImplement with vector databases like Pinecone, TurboPuffer, or Qdrant that support metadata filtering Thresholding: Set a similarity cutoff (e.g., cosine \u0026gt; 0.5) to trash low-confidence matches.\nExample: Query = \u0026ldquo;transformer architecture.\u0026rdquo; Docs below 0.5 might be random electrical engineering content—drop \u0026rsquo;em and keep the signal.\nWhat to do:\nConfigure similarity score thresholds in your vector database query APIs Step 3 – NDCG Optimization Why Ranking Matters You\u0026rsquo;ve maximized recall and precision—now make sure the gold is at the top. With LLMs having finite token limits, the order of retrieval can make or break your RAG system. If your best content is buried at position #30, your LLM might never see it.\nRanking Improvement Strategies Reranking: Use re-rankers to filter and re-rank your results. This helps to improve both precision and NDCG.\nUser Feedback Integration: Capture what users actually find valuable and use it to improve your rankings.\nExample: Users consistently reference information from the third document in your RAG answers for \u0026ldquo;transformer applications.\u0026rdquo; Your system learns to boost similar documents higher for that query, dramatically improving NDCG.\nWhat to do:\nTrack interactions: Implement explicit feedback (thumbs up/down) and implicit signals (time spent, follow-up questions) Build feedback loops: Create a simple database that stores query-document pairs with user ratings Implement active learning: Prioritize collecting feedback on borderline documents where the system is uncertain Curate your corpus: Ruthlessly remove consistently low-rated documents from your vector database—this is a game-changer that most teams overlook Apply immediate boosts: For frequent queries, manually boost documents with positive feedback by 1.2-1.5x in your ranking algorithm Pro Tip: Don\u0026rsquo;t wait for perfect data—start with a simple \u0026ldquo;Was this helpful?\u0026rdquo; button after each RAG response, and you\u0026rsquo;ll be shocked how quickly you can improve rankings with even sparse feedback.\nContext is King: Leverage conversation history to supercharge your retrieval relevance.\nExample: A user asks \u0026ldquo;What are the best frameworks?\u0026rdquo; after discussing PyTorch for 10 minutes. Without context, you might return generic framework docs. With context, you nail it with PyTorch-specific framework comparisons.\nWhat to do:\nStore conversation history: Keep the last 3-5 exchanges in a context window Question rewriting: Use the history to expand ambiguous queries Context-aware filtering: Use topics from previous exchanges to filter metadata Pro Tip: Don\u0026rsquo;t just append history blindly—it creates noise. Instead, extract key entities and concepts from previous exchanges and use them to enrich your current query. For example, if discussing \u0026ldquo;transformer models for NLP tasks,\u0026rdquo; extract \u0026ldquo;transformer\u0026rdquo; + \u0026ldquo;NLP\u0026rdquo; as context boosters.\nMeasuring NDCG Improvement Don\u0026rsquo;t fly blind—benchmark your changes:\nCreate a test set with queries and human-judged relevance scores Calculate NDCG@k (typically k=5 or k=10) before and after changes Aim for at least 5-10% lift in NDCG to justify implementation costs Pro Tip: Let\u0026rsquo;s do some LLM math that won\u0026rsquo;t make your brain explode! Focus on NDCG@k based on your document size, because your poor LLM can only eat so many tokens before it gets a tummy ache.\nHere\u0026rsquo;s a real-world example with numbers so simple even your coffee-deprived morning brain can handle them:\nYour average document: 10,000 tokens (that\u0026rsquo;s a chatty document!) Your fancy GPT-4o: 128,000 token capacity (big brain energy!) Your context + prompt: ~3,000 tokens (the appetizer) Now for the main course calculation: 10,000 tokens × 10 documents = 100,000 tokens 100,000 tokens + 3,000 tokens = 103,000 tokens\n103,000 \u0026lt; 128,000\u0026hellip; We\u0026rsquo;re good! 🎉\nConclusion: Build a Retrieval Flywheel Here\u0026rsquo;s the game plan:\nHybrid Search: Max out recall—grab everything. Re-Rankers: Sharpen precision—ditch the junk. Contextual Ranking: Make sure the gold is at the top. This isn\u0026rsquo;t a one-and-done deal. It\u0026rsquo;s a flywheel—every tweak spins it faster. Experiment with chunk sizes, thresholds, and models. Small wins stack up to massive gains.\nFinal Tip: Don\u0026rsquo;t guess—test. Try a 0.7 threshold vs. 0.9. Swap 256-token chunks for 512. Data beats dogma.\nRetrieval Cheat Sheet Step Goal Tactics 1. Recall Grab everything Query Expansion, Hybrid Search, Fine-Tuning, Chunking 2. Precision Ditch the junk Re-Rankers, Metadata Filters, Thresholds 3. NDCG Perfect rankings Reranking, User Feedback, Context That\u0026rsquo;s it—your RAG retrieval is now a lean, mean, result-spitting machine. Go forth and dominate!\n","permalink":"https://dipkumar.dev/posts/rag/retrieval-imprv/","summary":"Improving Retrieval in RAG (via Recall, Precision, and NDCG) Introduction Retrieval-Augmented Generation (RAG) is the superhero sidekick that grounds your Large Language Model (LLM) in cold, hard facts. But here\u0026rsquo;s the dirty secret: if your retrieval sucks, your RAG system is just a fancy chatbot with a broken brain. Weak retrieval = missed documents, irrelevant results, and rankings that make no sense.\nThis guide cuts through the noise. You\u0026rsquo;ll learn how to turbocharge your RAG retrieval with a no-fluff, step-by-step approach to maximize recall, sharpen precision, and nail NDCG.","title":"Improving Retrieval in RAG (via Recall, Precision, and NDCG)"},{"content":"Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. [1]\nAWS BedRock\u0026rsquo;s Converse API is a single endpoint that allows you to chat with any model. Indeed, the single endpoint is, I believe, the best feature of AWS BedRock. Let\u0026rsquo;s visit this endpoint and see how it works.\nmodel_id = \u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34; inference_config = {\u0026#34;temperature\u0026#34;: 0.5} additional_model_fields = {\u0026#34;top_k\u0026#34;: 200} # Send the message. response = bedrock_client.converse( modelId=model_id, messages=messages, system=system_prompts, inferenceConfig=inference_config, additionalModelRequestFields=additional_model_fields ) By changing model_id, you can switch between different models.\nI think AWS BedRock should have used the same standards as OpenAI\u0026rsquo;s client rather than creating their own But, hey, it\u0026rsquo;s still a single endpoint. right ?\u0026hellip; I should be just able to switch models by changing model_id. right ?\u0026hellip;.\nHidden Gotcha of Converse API Not every model is available AWS BedRock has LLama3, Anthropic Claude, Mistral and their own Titan. But, It doesn\u0026rsquo;t have OpenAI models like GPT-4/GPT-4o. This might not be a deal breaker, depending on what you are trying to achieve. You can check the availability of models in AWS Bedrock Models\nNot every model has system prompt, or multi-modality support If you check converse API parameters, you will see that there is a parameter called system. This parameter is used to provide system prompt to the model. However, not every model supports system prompts. (Because they were not trained with system prompts). If you\u0026rsquo;re switching between models via code using ENV/Flags/Config, you might need to handle edge cases where a system prompt is unavailable for the given modelId. Otherwise, It will throw an Exception. (Ideally, i think it should just give a warning) AWS has a nice table to check if given model has system prompt.\nThe same goes for multi-modality. If your messages include images, switching between models might not be straightforward.\nNot every model has same context window I mean this is on you, but again good reminder.\nAdvance Prompt technique like Prefilling Assistant Message # code copied from https://eugeneyan.com//writing/prompting/#prefill-claudes-responses input = \u0026#34;\u0026#34;\u0026#34; \u0026lt;description\u0026gt; The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99. At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice or app—no matter where you place it in your home. This affordable little hub brings convenient hands-free control to your smart devices. \u0026lt;/description\u0026gt; Extract the \u0026lt;name\u0026gt;, \u0026lt;size\u0026gt;, \u0026lt;price\u0026gt;, and \u0026lt;color\u0026gt; from this product \u0026lt;description\u0026gt;. Return the extracted attributes within \u0026lt;attributes\u0026gt;. \u0026#34;\u0026#34;\u0026#34; messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: input, }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;attributes\u0026gt;\u0026lt;name\u0026gt;\u0026#34; # Prefilled response } ] # raise error_class(parsed_response, operation_name) # botocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the Converse # operation: The model that you are using requires the last turn in the conversation to be a user message. Add a # user message to the conversation and try again. If you\u0026rsquo;re using advanced prompting techniques, such as Prefilling Assistant Messages [3], where you pre-populate the message with text designated as \u0026lsquo;assistant\u0026rsquo;, you need to be cautious when switching between models. Not all models are compatible with this technique and their is validation check which will raise exception.\nSo, Overall, We are still far away from having a unified API for all models. I will update this article if i find anything new.\nReferences [1] https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html\n[2] https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features\n[3] https://eugeneyan.com//writing/prompting/#prefill-claudes-responses\n","permalink":"https://dipkumar.dev/posts/aws-bedrock/","summary":"Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI startups and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.","title":"AWS BedRock - Converse API - A single endpoint for all models ?"},{"content":"Essential Fields Be it relational or not, every table should have these 5 fields:\ncreated_at (default now()) updated_at (default now()) deleted_at (default null) created_by (not null) updated_by (not null) Just to be clear, every table should have these 5 fields and not must. Adding these fields have other side-effects such as bloat, performance and disk size. But, if you\u0026rsquo;re having these problems, i hope you\u0026rsquo;re profitable.\nWhy should you include this fields ? Auditability Incorporating these fields into every table significantly simplifies the auditing process. They enable you to track who created or modified an entry and when these actions occurred. It\u0026rsquo;s important to note that while this doesn\u0026rsquo;t provide a complete audit trail, not all tables require exhaustive audit trails. These fields deliver sufficient oversight for many applications.\nSoft Delete Capability Utilizing the deleted_at field for soft deletions boosts data recovery and error correction capabilities, enabling businesses to effortlessly restore mistakenly deleted data or perform historical data analysis without relying on intricate backup systems. Additionally, you can set up a cron job to transfer data to an archive table periodically. For instance, you might move all data marked as deleted over three months ago to cold storage. This strategy helps maintain manageable table sizes by systematically archiving older records.\nRow Level Security/Permissions (RLS) These fields might seem superfluous at first, but they are incredibly useful for controlling user access to specific rows within a table. For instance, you may want to prevent a user from updating a row that was created by someone else. By using these fields, you can define such permissions clearly and effectively. Furthermore, they enable more nuanced scenarios—for example, allowing a user to restore a deleted row only if they were the original creator, while still permitting any user to delete a row. This level of detailed control ensures both data integrity and adherence to specified access protocols.\nAvoiding Nightmares: A Cautionary Tale Imagine you\u0026rsquo;ve deployed a cron job in the background designed to update certain attributes in your table based on specific business logic. It ran flawlessly during the staging tests, so you pushed it to production without further validation. But then, disaster strikes: the script modifies incorrect data. Fortunately, the updated_at and updated_by fields can come to your rescue (though not always). To identify the affected data, you can execute a query like:\nSELECT * FROM items WHERE updated_at BETWEEN {script_begin} AND {script_end} AND updated_by = {script_user}; This allows you to pinpoint the exact entries altered during the time the script ran, providing a straightforward way to assess and rectify the unintended changes. This is a prime example of how such fields can help mitigate potential disasters, helping you manage crises more effectively.\nORM: Django if you\u0026rsquo;re using some framework for accessing db like ORM in your codebase, it becomes easier to add these fields to your tables and helper queries. For example, I am showcasing you how to add these fields in django (python).\n1. Create mixin class from django.db import models from django.utils import timezone from django.conf import settings class AuditFieldsMixin(models.Model): created_at = models.DateTimeField(auto_now_add=True) updated_at = models.DateTimeField(auto_now=True) deleted_at = models.DateTimeField(null=True, blank=True) created_by = models.ForeignKey(settings.AUTH_USER_MODEL, related_name=\u0026#34;%(class)s_created_by\u0026#34;, on_delete=models.SET_NULL, null=True) updated_by = models.ForeignKey(settings.AUTH_USER_MODEL, related_name=\u0026#34;%(class)s_updated_by\u0026#34;, on_delete=models.SET_NULL, null=True) class Meta: abstract = True def soft_delete(self): self.deleted_at = timezone.now() self.save() What’s going on here? We’re defining fields that automatically capture when and by whom a record was created or updated. Plus, we threw in a soft_delete method for good measure, so you can \u0026ldquo;delete\u0026rdquo; records without actually losing them.\nSlap the Mixin on a Model Using this mixin is as easy as pie. Just inherit from AuditFieldsMixin in your model:\nclass Item(AuditFieldsMixin): name = models.CharField(max_length=255) description = models.TextField() price = models.DecimalField(max_digits=5, decimal_places=2) # Imagine there are other fields here too! 2. QuerySets That Ignore Deleted Stuff You don\u0026rsquo;t want your default queries pulling up deleted records, right? Let’s fix that by tweaking the model’s manager to ignore anything that’s been soft-deleted:\nclass AuditQuerySet(models.QuerySet): def active(self): return self.filter(deleted_at__isnull=True) def deleted(self): return self.filter(deleted_at__isnull=False) class AuditManager(models.Manager): def get_queryset(self): return AuditQuerySet(self.model, using=self._db).active() class Item(AuditFieldsMixin): objects = AuditManager() all_objects = models.Manager() # This lets you access ALL records, even the \u0026#34;deleted\u0026#34; ones name = models.CharField(max_length=255) description = models.TextField() price = models.DecimalField(max_digits=5, decimal_places=2) # More fields, potentially Conclusion Why do you need conclusion ? This is ain\u0026rsquo;t generated by GPT. I am just a human being trying to help you.\nIf you have any past expirences of getting saved by some random fields, please let me know. I would be happy to learn.\nSend me an email at pate@ + dipkumar.dev\n","permalink":"https://dipkumar.dev/posts/essential-db-design-1/","summary":"Essential Fields Be it relational or not, every table should have these 5 fields:\ncreated_at (default now()) updated_at (default now()) deleted_at (default null) created_by (not null) updated_by (not null) Just to be clear, every table should have these 5 fields and not must. Adding these fields have other side-effects such as bloat, performance and disk size. But, if you\u0026rsquo;re having these problems, i hope you\u0026rsquo;re profitable.\nWhy should you include this fields ?","title":"Essential Database Design: Five Fields Every Table Must Have"},{"content":"Work-in Progress ","permalink":"https://dipkumar.dev/about/","summary":"Work-in Progress ","title":"About Me"},{"content":"The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it. Before you read further, the blog assumes you have background on transformers; if you don\u0026rsquo;t, then read this blog post. It’s awesome, and you will learn a lot from it.\nFirst, let’s understand a few things about GPT code.\ndef gpt(inputs: list[int]) -\u0026gt; list[list[float]]: # inputs has shape [n_seq] # output has shape [n_seq, n_vocab] output = # beep boop neural network magic return output We can deduce from the input-output signature that we can provide arbitrary long input and receive output of the same length, with each element of the output indicating the probability of the next token. So, I can just give a single token as input and get the probability of next token. It should just work, right ?\nModifying the code of picoGPT to just give the input of the last single token and get the probability of the next token.\nfor _ in tqdm(range(n_tokens_to_generate), \u0026#34;generating\u0026#34;): # auto-regressive decode loop logits = gpt2(inputs[-1:], **params, n_head=n_head) # model forward pass next_id = np.argmax(logits[-1]) # greedy sampling inputs = np.append(inputs, [next_id]) # append prediction to input We are providing inputs[-1:] as input (single token) to the model. So, we are just passing a single token as input. Let\u0026rsquo;s see what happens.\nthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the I didn’t work. Because the main magic is in the attention, in order to have good prediction of next tokens we need to provide all previous tokens. Although in practice, we do have limited memory and compute which forces us to provide context upto last N tokens. for example, chagpt has context upto 4096. In summary, We can’t just pass a single token and get very good prediction of next token. This makes attention have quadratic complexity.\nBut, if we look at the architecture of GPT, we can see that we only interact with previous tokens in the attention block, all other layers, such as the embedding layer, the feed forward layer, the layer norm, etc., don’t care about previous tokens. So, what if we can cache the input of the attention block for all previous tokens and pass it during inference? We don’t have to pass all these tokens again and again. We can just pass the last token and get the probability of the next token.\nThe input of the attention block is q, k, v and mask. We can try to cache q, k, and v for all previous tokens. But, let’s think about what really matters for us. We only need k and v of the previous tokens to perform attention on the current input token because we are only passing one token as input. See the image below for a visual representation of what I mean.\ndef attention(q, k, v, mask): # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -\u0026gt; [n_q, d_v] return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v So, we need to calculate new_k and new_v for current input token. Append it to the existing cache and pass it to attention block for further processing.\ndef mha(x, c_attn, c_proj, n_head, kvcache=None): # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] # qkv projection # when we pass kvcache, n_seq = 1. so we will compute new_q, new_k and new_v x = linear(x, **c_attn) # [n_seq, n_embd] -\u0026gt; [n_seq, 3*n_embd] # split into qkv qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -\u0026gt; [3, n_seq, n_embd] if kvcache: # qkv new_q, new_k, new_v = qkv # new_q, new_k, new_v = [1, n_embd] old_k, old_v = kvcache k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1 v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1 qkv = [new_q, k, v] There is one more thing we need to take care of is causal mask. When we pass single token we would like it to attend to all previous tokens.\n# causal mask to hide future inputs from being attended to if kvcache: # when we pass kvcache, we are passing single token as input which need to attend to all previous tokens, so we create mask with all 0s causal_mask = np.zeros((1, k.shape[0])) else: # create triangular causal mask causal_mask = (1 - np.tri(x.shape[0])) * -1e10 # [n_seq, n_seq] Combining all the things together, we get the following code.\ndef mha(x, c_attn, c_proj, n_head, kvcache=None): # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] # qkv projection # n_seq = 1 when we pass kvcache, so we will compute new_q, new_k and new_v x = linear(x, **c_attn) # [n_seq, n_embd] -\u0026gt; [n_seq, 3*n_embd] # split into qkv qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -\u0026gt; [3, n_seq, n_embd] if kvcache: # qkv new_q, new_k, new_v = qkv # new_q, new_k, new_v = [1, n_embd] old_k, old_v = kvcache k = np.vstack([old_k, new_k]) # k = [n_seq, n_embd], where n_seq = prev_n_seq + 1 v = np.vstack([old_v, new_v]) # v = [n_seq, n_embd], where n_seq = prev_n_seq + 1 qkv = [new_q, k, v] current_cache = [qkv[1], qkv[2]] # split into heads qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv)) # [3, n_seq, n_embd] -\u0026gt; [n_head, 3, n_seq, n_embd/n_head] # causal mask to hide future inputs from being attended to if kvcache: causal_mask = np.zeros((1, k.shape[0])) else: causal_mask = (1 - np.tri(x.shape[0])) * -1e10 # [n_seq, n_seq] # perform attention over each head out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)] # [n_head, 3, n_seq, n_embd/n_head] -\u0026gt; [n_head, n_seq, n_embd/n_head] # merge heads x = np.hstack(out_heads) # [n_head, n_seq, n_embd/n_head] -\u0026gt; [n_seq, n_embd] # out projection x = linear(x, **c_proj) # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] return x, current_cache We introduced minor breaking changes in output as well. We are introducing current_cache alongside our normal output. This is because we can use an updated cache for the next run.\nWe also need to change a few functions to make it work.\ndef transformer_block(x, mlp, attn, ln_1, ln_2, n_head, kvcache=None): # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] # multi-head causal self attention attn_out, kvcache_updated = mha(layer_norm(x, **ln_1), **attn, n_head=n_head, kvcache=kvcache) x = x + attn_out # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] # position-wise feed forward network x = x + ffn(layer_norm(x, **ln_2), **mlp) # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] return x, kvcache_updated We added kvcache as an input to the function and returned kvcache_updated as an output for each transformer block. We also need to change transformer function.\ndef gpt2(inputs, wte, wpe, blocks, ln_f, n_head, kvcache = None): # [n_seq] -\u0026gt; [n_seq, n_vocab] if not kvcache: kvcache = [None]*len(blocks) wpe_out = wpe[range(len(inputs))] else: # cache already available, only send last token as input for predicting next token wpe_out = wpe[[len(inputs)-1]] inputs = [inputs[-1]] # token + positional embeddings x = wte[inputs] + wpe_out # [n_seq] -\u0026gt; [n_seq, n_embd] # forward pass through n_layer transformer blocks new_kvcache = [] for block, kvcache_block in zip(blocks, kvcache): x, updated_cache = transformer_block(x, **block, n_head=n_head, kvcache=kvcache_block) # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] new_kvcache.append(updated_cache) # TODO: inplace extend new cache instead of re-saving whole # projection to vocab x = layer_norm(x, **ln_f) # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] return x @ wte.T, new_kvcache # [n_seq, n_embd] -\u0026gt; [n_seq, n_vocab] Notice, When we have already compute kvcache, we only return input last token to GPT2 alongside with kvcache. You can also see len(kvcache) == # number of transformer blocks. This is because we need to update kvcache for attention and we have single attention in each transformer block.\nAnd, finally, it\u0026rsquo;s time to change our generate function to use cache. In the first iteration, we will not have kvcache and we will pass kvcache=None to gpt2 function. In subsequent iterations, we will utilise the previously generated kvcache.\nkvcache = None for _ in tqdm(range(n_tokens_to_generate), \u0026#34;generating\u0026#34;): # auto-regressive decode loop logits, kvcache = gpt2(inputs, **params, n_head=n_head, kvcache=kvcache) # model forward pass next_id = np.argmax(logits[-1]) # greedy sampling inputs = np.append(inputs, [next_id]) # append prediction to input This cache helps us to reduce computation for each iteration. We can see that, in first iteration, we are computing attention for all tokens in input. But, in subsequent iterations, we are only computing attention for last token. Reducing time complexity from O(n^2) to O(n).\nFinally, we can verify generate text with our previous code which didn\u0026rsquo;t have caching and compare two output. Both output should be same.\nIn terminal\n\u0026gt;\u0026gt;\u0026gt; python gpt2_kvcache.py \u0026#34;Alan Turing theorized that computers would one day become\u0026#34; Output: the most powerful machines on the planet. The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain. You can see the all the code in this pull request. You can also see the code in this repository.\nYou can see more details of calculation related to kv cache memory footprint calculation and computation time in this blog post.\nReferences:\nhttps://kipp.ly/blog/transformer-inference-arithmetic/ https://lilianweng.github.io/posts/2023-01-10-inference-optimization/ https://jaykmody.com/blog/gpt-from-scratch/ ","permalink":"https://dipkumar.dev/posts/gpt-kvcache/","summary":"The common optimization trick for speeding up transformer inference is KV caching 1 2. This technique is so prominent that huggingface library has use_cache flag is enabled by default 6. A few days ago, I read an awesome blog post on GPT in 60 Lines of NumPy. So, i thought, why not extend it to use the KV cache technique? So, let’s roll up our sleeves and start working on it.","title":"Speeding up the GPT - KV cache"},{"content":"Lec-2 spatial demension\n","permalink":"https://dipkumar.dev/notes/efficient-ml/","summary":"Lec-2 spatial demension","title":"Efficient ML"},{"content":"Biweekly-66 (27th Nov, 2021) 2085. Count Common Words With One Occurrence Hint 1\rUse hashmap (Counter)\r2086. Minimum Number of Buckets Required to Collect Rainwater from Houses\u0026quot; Hint 1\rFirst put the bucket at best place and the remove those covering home.\rHint 2\rAnswer is (best bucket cnt + remaining house).\rHint 3\rCorner case: check for each house is coverable\r2087. Minimum Cost Homecoming of a Robot in a Grid Hint\rdjikstra will fail. why ?\rHint\rToo many cells to cover (10**10). Think of something else\rHint\rTo reach home, which path you need to take ? (cost is non-negative)\rHint\rTo reach home, number of rows and number of cols changes are fixed.\r2088. Count Fertile Pyramids in a Land Hint\rDeconstruct pyramid into smaller part and then think to calculate how many pyramids are there\rHint\rwe can calculate left and right perpendiculars and then construct pyramids from them\rHint\rcalculate for normal and flipped version of grid\rWeekly-269 (28th Nov, 2021) 2089. Find Target Indices After Sorting Array Hint\rImplementation\r2090. K Radius Subarray Averages Hint\rPrefix sum\r2091. Removing Minimum and Maximum From Array Hint\rGreedy cases to minimize number of remove\rHint\rmin(r+1, n-l, l+1+(n-r)). here l and r are index of max and min elements (l \u0026lt; r).\r2092. Find All People With Secret Hint\rsort by time and try to share secret\rHint\rat current timestamp, find connected components and color all nodes if one of them have seen secret\r","permalink":"https://dipkumar.dev/posts/leetcode-contest/","summary":"Biweekly-66 (27th Nov, 2021) 2085. Count Common Words With One Occurrence Hint 1\rUse hashmap (Counter)\r2086. Minimum Number of Buckets Required to Collect Rainwater from Houses\u0026quot; Hint 1\rFirst put the bucket at best place and the remove those covering home.\rHint 2\rAnswer is (best bucket cnt + remaining house).\rHint 3\rCorner case: check for each house is coverable\r2087. Minimum Cost Homecoming of a Robot in a Grid Hint\rdjikstra will fail.","title":"LC contest problems summary"},{"content":"run local server hugo server -D Create New Post hugo new content/posts/{post-name}.md Hugo build/export the site hugo -d ../becoming-the-unbeatable relative imports example: static\\icons\\favicon.png relative imports: icons\\favicon.png\nfix for label image icon: small_icon.jpg instead of icon: small_icon.png\ngithub issue: https://github.com/adityatelange/hugo-PaperMod/issues/622\n","permalink":"https://dipkumar.dev/posts/hugo-cmds/","summary":"run local server hugo server -D Create New Post hugo new content/posts/{post-name}.md Hugo build/export the site hugo -d ../becoming-the-unbeatable relative imports example: static\\icons\\favicon.png relative imports: icons\\favicon.png\nfix for label image icon: small_icon.jpg instead of icon: small_icon.png\ngithub issue: https://github.com/adityatelange/hugo-PaperMod/issues/622","title":"Hugo commands"}]